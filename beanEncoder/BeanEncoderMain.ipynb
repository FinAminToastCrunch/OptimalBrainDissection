{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import synapseclient \n",
    "#  import synapseutils \n",
    " \n",
    "#  syn = synapseclient.Synapse() \n",
    "#  syn.login('finamintoastcrunch','1Hjldria!') \n",
    "#  files = synapseutils.syncFromSynapse(syn, ' syn2825306 ') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "qTo_HuQkGgAq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_visualizer import visualizer \n",
    "\n",
    "from tensorflow.keras.layers import*\n",
    "import shap\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Conv1D\n",
    "# from keras.layers import Conv1DTranspose\n",
    "# from keras.layers import Flatten, Reshape\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS = 7\n",
    "NUM_TARGETS = 372\n",
    "NUM_TIME_STEPS = 11\n",
    "NUM_REPLICATES = 3 #used to be 4, but replicate 4 is trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "gXpISOijEYqQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# matrix_path = \"regulator-gene-matrix.csv\"\n",
    "# data_path_syn = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\synData\"\n",
    "# data_path_inter =  r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\interpolatedOnly\"\n",
    "# data_path_og_exp1 = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\datasets\\exp1\"\n",
    "# data_path_testSet = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\testSetFixed\"\n",
    "# data_path_petal = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\petal_len.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_RGM = r'Regulations_Control_Altona.csv'\n",
    "dirty_regulations = r'FullTable_Control.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372, 44)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtyRGM = pd.read_csv(dirty_RGM,  index_col = 0, )#on_bad_lines='skip')\n",
    "dirtyReg = pd.read_csv(dirty_regulations,  index_col = 0,)# on_bad_lines='skip')\n",
    "dirtyReg = dirtyReg.select_dtypes(include=np.number)\n",
    "dirtyReg = dirtyReg.to_numpy()\n",
    "dirtyReg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 11, 372)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Fix this. it puts nans where it should not. \n",
    "\n",
    "def fix_dataset(dirtyR):\n",
    "    dataset = np.zeros(shape=(NUM_REPLICATES,NUM_TARGETS, NUM_TIME_STEPS))\n",
    "    ret = np.zeros(shape=(NUM_REPLICATES,NUM_TIME_STEPS, NUM_TARGETS))\n",
    "\n",
    "    for i in range(0,NUM_REPLICATES*NUM_TIME_STEPS, 4):\n",
    "        for j in range(0, NUM_REPLICATES):\n",
    "            dataset[j][:,i//NUM_REPLICATES] = dirtyR[:,(i+j)]\n",
    "    \n",
    "    dataset[dataset==0] = np.nan\n",
    "\n",
    "\n",
    "    for i in range(NUM_REPLICATES):\n",
    "        regScaled = MinMaxScaler().fit_transform(dataset[i].flatten().reshape((-1,1)))\n",
    "        regScaled = regScaled.reshape((NUM_TARGETS, NUM_TIME_STEPS))\n",
    "        regScaled = np.nan_to_num(regScaled, nan= -1.0)\n",
    "        ret[i] = regScaled.T\n",
    "    return ret\n",
    "\n",
    "dataset = fix_dataset(dirtyR=dirtyReg)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11, 372) (1, 11, 372)\n"
     ]
    }
   ],
   "source": [
    "beanIntensities = dataset[0:2]\n",
    "validation = np.array([dataset[2]])\n",
    "print(beanIntensities.shape, validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the third replicate is trash. we will not use it. \n",
    "# df(dataset[3]).head(11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372, 372)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regulator_gene_matrix = np.load(\"soyBeanRGM.npy\")\n",
    "regulator_gene_matrix = regulator_gene_matrix.astype('float32')\n",
    "regulator_gene_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent Matrix + Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of parent index (7,)\n"
     ]
    }
   ],
   "source": [
    "superParent = regulator_gene_matrix.copy() #init the super parent with the ordinary RGM, and do forward passes with super parent\n",
    "#print(superParent.shape)\n",
    "\n",
    "ones = np.ones((NUM_TARGETS))\n",
    "parentIndex = []\n",
    "not_parentIndex = []\n",
    "for i in range(len(regulator_gene_matrix)):\n",
    "    if (np.isin(regulator_gene_matrix[i], [1])).any():\n",
    "        #print(i)\n",
    "        superParent[i] = ones \n",
    "        parentIndex.append(i)\n",
    "    else:\n",
    "        not_parentIndex.append(i)\n",
    "\n",
    "parentIndex = np.array(parentIndex)\n",
    "parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "parent_idx = parentIndex.numpy()\n",
    "not_parentIndex = np.array(not_parentIndex)\n",
    "not_parentIndex = tf.convert_to_tensor(not_parentIndex)\n",
    "print(\"shape of parent index\", parentIndex.shape)\n",
    "\n",
    "def ignore_noParent_MSE_old(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "    y_true_pruned = tf.gather(y_true,parentIndex, axis =2) \n",
    "    #print(y_true_pruned.shape\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis =2)   \n",
    "    return l(y_true_pruned, y_pred_pruned)\n",
    "\n",
    "#this will not work if the entire dataset is -1 (degenerate), or has only one actual value (also degen)\n",
    "def ignore_noParent_MSE(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "   # print(y_true.shape) #(None, 44, 372)\n",
    "\n",
    "    #get the parents and flatten them\n",
    "    y_true_pruned = tf.gather(y_true, parentIndex, axis = 2) #axis 2 because batch, time, gene\n",
    "    y_true_pruned = tf.reshape(y_true_pruned, shape=([tf.size(y_true_pruned)] ) )\n",
    "\n",
    "   # print(y_true_pruned.shape)\n",
    "   # print(\"tf size\", tf.size(y_true_pruned))\n",
    "\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis = 2) \n",
    "    y_pred_pruned = tf.reshape(y_pred_pruned, shape=([tf.size(y_pred_pruned)]) )\n",
    "\n",
    "    #get the index of the parents which are not -1\n",
    "    y_true_posID = tf.where(y_true_pruned >= 0) #gets args\n",
    "    y_true_posID = tf.squeeze(y_true_posID)\n",
    "    #get the idx of all the -1s \n",
    "    y_true_negID = tf.where(y_true_pruned < 0) \n",
    "    y_true_negID = tf.squeeze(y_true_negID)\n",
    "\n",
    "    #get all the -1s in the parents \n",
    "    y_true_neg = tf.gather(y_true_pruned, y_true_negID) #get all the -1s in y_true\n",
    "    y_pred_neg = tf.gather(y_pred_pruned, y_true_negID) #get the corresponding values for y_pred\n",
    "\n",
    "    #get the indexes where pred should be -1 but is not. get the corresponding index for ytrue\n",
    "    y_shouldBeNegButIsntID = tf.where(y_pred_neg >= 0)  \n",
    "    y_shouldBeNegButIsntID = tf.squeeze(y_shouldBeNegButIsntID) #get the idx which should be -1 for prediction but are not\n",
    "    y_true_wrong = tf.gather(y_true_pruned, y_shouldBeNegButIsntID) #get the same corresponding values from ytrue\n",
    "    y_shouldBeNegButIsnt = tf.gather(y_pred_pruned, y_shouldBeNegButIsntID) #this has all the wrongly predicted values which should be -1 but are not\n",
    "\n",
    "    y_true_pos = tf.gather(y_true_pruned, y_true_posID)\n",
    "    y_pred_pos = tf.gather(y_pred_pruned, y_true_posID)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0. \n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 1: #dim goes away if size = 1. \n",
    "        y_shouldBeNegButIsnt = tf.expand_dims(y_shouldBeNegButIsnt, axis = 0) #should all be flattened\n",
    "        y_true_wrong = tf.expand_dims(y_true_wrong, axis=0)\n",
    "\n",
    "    #print(\"y_pred\", (y_pred_pos), \"y_true\", (y_shouldBeNegButIsnt))\n",
    "    try:\n",
    "        y_pred_total = tf.concat([y_pred_pos, y_shouldBeNegButIsnt], axis = 0) #concatenate for total mse\n",
    "        y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\n",
    "    except Exception as e:\n",
    "        print(y_pred_pos.shape, y_shouldBeNegButIsnt.shape, tf.size(y_shouldBeNegButIsnt))\n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "    return l(y_true_total, y_pred_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117, 131, 164, 225, 259, 334, 350])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(superParent, cmap='hot');\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(regulator_gene_matrix, cmap='hot');\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "1yhzyikoFncq"
   },
   "outputs": [],
   "source": [
    "class EncoderLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, input_dim=32, units=32):\n",
    "        super(EncoderLinear, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.rgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "4uchxU-bmBDe"
   },
   "outputs": [],
   "source": [
    "class DecoderLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, input_dim=32, units=32):\n",
    "        super(DecoderLinear, self).__init__()\n",
    "        self.rgm = rgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        #return tf.matmul(inputs, self.w)\n",
    "        # v = tf.zeros_like(X)\n",
    "        # u = tf.ones_like(X)\n",
    "        # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "        return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "PQVz4BCpmNMd"
   },
   "outputs": [],
   "source": [
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(490, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association)\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(1024)(X)#Dense(128)(X)\n",
    "    de_dense = Reshape((1, 1024))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def model(rgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 32):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinaryAE = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS)\n",
    "ordinaryAE.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2169 - val_loss: 0.2494\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.2152 - val_loss: 0.2335\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2078 - val_loss: 0.2017\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1826 - val_loss: 0.1552\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1467 - val_loss: 0.1210\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1240 - val_loss: 0.1147\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1211 - val_loss: 0.0770\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0865 - val_loss: 0.0515\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0588 - val_loss: 0.0410\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0523 - val_loss: 0.0329\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0422 - val_loss: 0.0295\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0378 - val_loss: 0.0369\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0440 - val_loss: 0.0401\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0490 - val_loss: 0.0297\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0375 - val_loss: 0.0223\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0283 - val_loss: 0.0214\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0262 - val_loss: 0.0203\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0251 - val_loss: 0.0156\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0240 - val_loss: 0.0099\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0178 - val_loss: 0.0082\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0175 - val_loss: 0.0101\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0212 - val_loss: 0.0103\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0205 - val_loss: 0.0085\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0174 - val_loss: 0.0085\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0133 - val_loss: 0.0104\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0158 - val_loss: 0.0113\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0153 - val_loss: 0.0101\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0111 - val_loss: 0.0085\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0098 - val_loss: 0.0079\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0100 - val_loss: 0.0080\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0103 - val_loss: 0.0074\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0088 - val_loss: 0.0070\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0080 - val_loss: 0.0071\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0076 - val_loss: 0.0081\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0085 - val_loss: 0.0071\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0071 - val_loss: 0.0060\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0067 - val_loss: 0.0052\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0066 - val_loss: 0.0048\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0064 - val_loss: 0.0047\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0062 - val_loss: 0.0046\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0059 - val_loss: 0.0045\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0058 - val_loss: 0.0050\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0055 - val_loss: 0.0056\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0057 - val_loss: 0.0054\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0049 - val_loss: 0.0050\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0049 - val_loss: 0.0045\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0043 - val_loss: 0.0051\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0048 - val_loss: 0.0050\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0039 - val_loss: 0.0047\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0037 - val_loss: 0.0043\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0036 - val_loss: 0.0041\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0047 - val_loss: 0.0039\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0047 - val_loss: 0.0039\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0036 - val_loss: 0.0042\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0037 - val_loss: 0.0042\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0036 - val_loss: 0.0045\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0035 - val_loss: 0.0048\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0049 - val_loss: 0.0042\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0045 - val_loss: 0.0037\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0042 - val_loss: 0.0039\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0048 - val_loss: 0.0038\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0041 - val_loss: 0.0046\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0035 - val_loss: 0.0064\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0041 - val_loss: 0.0067\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0059 - val_loss: 0.0047\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0033 - val_loss: 0.0040\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0052 - val_loss: 0.0041\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0034 - val_loss: 0.0063\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0039 - val_loss: 0.0079\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0048 - val_loss: 0.0064\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0039 - val_loss: 0.0041\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0045 - val_loss: 0.0041\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0045 - val_loss: 0.0035\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0029 - val_loss: 0.0050\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0031 - val_loss: 0.0058\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0035 - val_loss: 0.0050\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0030 - val_loss: 0.0036\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0026 - val_loss: 0.0032\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0033 - val_loss: 0.0033\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0027 - val_loss: 0.0040\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0032 - val_loss: 0.0045\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0033 - val_loss: 0.0037\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0015 - val_loss: 0.0030\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0021 - val_loss: 0.0038\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0019 - val_loss: 0.0044\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0014 - val_loss: 0.0028\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0010 - val_loss: 0.0026\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.4031e-04 - val_loss: 0.0025\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.1484e-04 - val_loss: 0.0026\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.8498e-04 - val_loss: 0.0029\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.9587e-04 - val_loss: 0.0031\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.5098e-04 - val_loss: 0.0031\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.8063e-04 - val_loss: 0.0028\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.7255e-04 - val_loss: 0.0026\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.0612e-04 - val_loss: 0.0025\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.8356e-04 - val_loss: 0.0025\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.7398e-04 - val_loss: 0.0024\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.1680e-04 - val_loss: 0.0025\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.5637e-04 - val_loss: 0.0025\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.9000e-04 - val_loss: 0.0026\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.8537e-04 - val_loss: 0.0026\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.7184e-04 - val_loss: 0.0023\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8849e-04 - val_loss: 0.0022\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.0385e-04 - val_loss: 0.0022\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.1069e-04 - val_loss: 0.0022\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.8928e-04 - val_loss: 0.0024\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7428e-04 - val_loss: 0.0024\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.9987e-04 - val_loss: 0.0023\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.3675e-04 - val_loss: 0.0022\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.1270e-04 - val_loss: 0.0021\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.0206e-04 - val_loss: 0.0020\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8481e-04 - val_loss: 0.0021\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6749e-04 - val_loss: 0.0021\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6003e-04 - val_loss: 0.0022\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3821e-04 - val_loss: 0.0021\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3856e-04 - val_loss: 0.0021\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1361e-04 - val_loss: 0.0020\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1340e-04 - val_loss: 0.0020\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1925e-04 - val_loss: 0.0020\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.4126e-05 - val_loss: 0.0020\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.0335e-05 - val_loss: 0.0020\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.3861e-05 - val_loss: 0.0020\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.3886e-05 - val_loss: 0.0019\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.3046e-05 - val_loss: 0.0019\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.6168e-05 - val_loss: 0.0019\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.8337e-05 - val_loss: 0.0019\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.8025e-05 - val_loss: 0.0019\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.3265e-05 - val_loss: 0.0019\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.8843e-05 - val_loss: 0.0019\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.5234e-05 - val_loss: 0.0019\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.7827e-05 - val_loss: 0.0019\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.0937e-05 - val_loss: 0.0018\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.2277e-05 - val_loss: 0.0019\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.2409e-05 - val_loss: 0.0019\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.1128e-05 - val_loss: 0.0019\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.1870e-05 - val_loss: 0.0019\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9619e-05 - val_loss: 0.0019\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.1355e-05 - val_loss: 0.0019\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.2549e-05 - val_loss: 0.0019\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8834e-05 - val_loss: 0.0019\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.5158e-05 - val_loss: 0.0019\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6187e-05 - val_loss: 0.0019\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.4853e-05 - val_loss: 0.0019\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2216e-05 - val_loss: 0.0019\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1984e-05 - val_loss: 0.0019\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1256e-05 - val_loss: 0.0019\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.7783e-06 - val_loss: 0.0019\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.7320e-05 - val_loss: 0.0019\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.5535e-06 - val_loss: 0.0019\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5713e-05 - val_loss: 0.0019\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.8103e-06 - val_loss: 0.0019\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.3523e-05 - val_loss: 0.0019\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.1067e-06 - val_loss: 0.0019\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0391e-05 - val_loss: 0.0019\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.2362e-06 - val_loss: 0.0019\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.5194e-06 - val_loss: 0.0019\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.6703e-06 - val_loss: 0.0019\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.2055e-06 - val_loss: 0.0019\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.7033e-06 - val_loss: 0.0019\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.7533e-06 - val_loss: 0.0019\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.4261e-06 - val_loss: 0.0019\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.6108e-06 - val_loss: 0.0019\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.1450e-06 - val_loss: 0.0019\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.6651e-06 - val_loss: 0.0019\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7419e-06 - val_loss: 0.0019\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.4514e-06 - val_loss: 0.0019\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.8838e-06 - val_loss: 0.0019\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.4187e-06 - val_loss: 0.0019\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.2394e-06 - val_loss: 0.0019\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9530e-06 - val_loss: 0.0019\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4972e-06 - val_loss: 0.0019\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.9977e-06 - val_loss: 0.0019\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1040e-06 - val_loss: 0.0019\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.9293e-06 - val_loss: 0.0019\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7830e-06 - val_loss: 0.0019\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.6810e-06 - val_loss: 0.0019\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.1924e-07 - val_loss: 0.0019\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.7494e-07 - val_loss: 0.0019\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.8717e-07 - val_loss: 0.0019\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.3753e-07 - val_loss: 0.0019\n"
     ]
    }
   ],
   "source": [
    "o = ordinaryAE.fit(beanIntensities, beanIntensities, epochs=200, verbose = True, validation_data=(validation, validation))\n",
    "#print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(EncoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.OGrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "class DecoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DecoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        #return tf.matmul(inputs, self.w)\n",
    "        # v = tf.zeros_like(X)\n",
    "        # u = tf.ones_like(X)\n",
    "        # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "        return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        \n",
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(128)(X)\n",
    "    de_dense = Reshape((1, 128))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelSuperParent(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSuperParentSequential(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    m = tf.keras.Sequential()\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2108 - val_loss: 0.2467\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2181 - val_loss: 0.2314\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2112 - val_loss: 0.2045\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1955 - val_loss: 0.1648\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1609 - val_loss: 0.1099\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1178 - val_loss: 0.0488\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0698 - val_loss: 0.0124\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0414 - val_loss: 0.0483\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0764 - val_loss: 0.0477\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0718 - val_loss: 0.0182\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0413 - val_loss: 0.0040\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0232 - val_loss: 0.0096\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0187 - val_loss: 0.0214\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0275 - val_loss: 0.0298\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0272 - val_loss: 0.0316\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0256 - val_loss: 0.0279\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0229 - val_loss: 0.0212\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0154 - val_loss: 0.0146\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0095 - val_loss: 0.0106\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0104 - val_loss: 0.0096\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0100 - val_loss: 0.0092\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0183 - val_loss: 0.0073\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0099 - val_loss: 0.0055\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0068 - val_loss: 0.0056\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0084 - val_loss: 0.0078\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0067 - val_loss: 0.0111\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0065 - val_loss: 0.0134\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0082 - val_loss: 0.0135\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0088 - val_loss: 0.0114\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0081 - val_loss: 0.0083\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0049 - val_loss: 0.0038\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0051 - val_loss: 0.0034\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0055 - val_loss: 0.0037\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0061 - val_loss: 0.0040\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0060 - val_loss: 0.0040\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0056 - val_loss: 0.0042\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0042 - val_loss: 0.0065\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0039 - val_loss: 0.0079\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0043 - val_loss: 0.0086\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0045 - val_loss: 0.0085\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0044 - val_loss: 0.0077\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0041 - val_loss: 0.0065\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0037 - val_loss: 0.0054\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0034 - val_loss: 0.0045\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0034 - val_loss: 0.0040\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0029 - val_loss: 0.0037\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0027 - val_loss: 0.0040\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0027 - val_loss: 0.0043\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0027 - val_loss: 0.0044\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0038 - val_loss: 0.0040\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0025 - val_loss: 0.0035\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0023 - val_loss: 0.0031\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0023 - val_loss: 0.0028\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0025 - val_loss: 0.0030\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0020 - val_loss: 0.0032\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0016 - val_loss: 0.0035\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0014 - val_loss: 0.0031\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0013 - val_loss: 0.0028\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0011 - val_loss: 0.0025\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0023 - val_loss: 0.0024\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.7521e-04 - val_loss: 0.0026\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0012 - val_loss: 0.0028\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.0548e-04 - val_loss: 0.0023\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.2742e-04 - val_loss: 0.0024\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.7082e-04 - val_loss: 0.0026\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.0907e-04 - val_loss: 0.0027\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.4632e-04 - val_loss: 0.0025\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.3012e-04 - val_loss: 0.0023\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.8926e-04 - val_loss: 0.0021\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9746e-04 - val_loss: 0.0021\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8356e-04 - val_loss: 0.0021\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.3324e-04 - val_loss: 0.0023\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.6465e-04 - val_loss: 0.0023\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.0223e-04 - val_loss: 0.0022\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.9442e-04 - val_loss: 0.0023\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.4870e-04 - val_loss: 0.0025\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.0730e-04 - val_loss: 0.0027\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.6398e-04 - val_loss: 0.0025\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6582e-04 - val_loss: 0.0022\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.9061e-04 - val_loss: 0.0019\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.0360e-05 - val_loss: 0.0019\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6340e-04 - val_loss: 0.0019\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.4884e-04 - val_loss: 0.0018\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.8297e-04 - val_loss: 0.0018\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.1403e-04 - val_loss: 0.0018\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.4626e-05 - val_loss: 0.0019\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.5986e-05 - val_loss: 0.0020\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.1233e-05 - val_loss: 0.0020\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.7427e-04 - val_loss: 0.0019\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.6484e-05 - val_loss: 0.0018\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.0896e-05 - val_loss: 0.0018\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.2298e-05 - val_loss: 0.0018\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.8822e-04 - val_loss: 0.0018\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4615e-04 - val_loss: 0.0019\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.6297e-05 - val_loss: 0.0020\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.2327e-05 - val_loss: 0.0021\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.3444e-05 - val_loss: 0.0020\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.5654e-05 - val_loss: 0.0019\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.2941e-05 - val_loss: 0.0018\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8875e-05 - val_loss: 0.0018\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.9822e-05 - val_loss: 0.0018\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.6908e-05 - val_loss: 0.0017\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.5807e-05 - val_loss: 0.0017\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3026e-05 - val_loss: 0.0017\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3429e-05 - val_loss: 0.0018\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7984e-05 - val_loss: 0.0018\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.7230e-05 - val_loss: 0.0018\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.0648e-05 - val_loss: 0.0018\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.3740e-05 - val_loss: 0.0018\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.8655e-06 - val_loss: 0.0018\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4876e-05 - val_loss: 0.0018\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2939e-05 - val_loss: 0.0018\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4109e-05 - val_loss: 0.0018\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.1686e-06 - val_loss: 0.0018\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.1002e-06 - val_loss: 0.0019\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4622e-05 - val_loss: 0.0019\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.5524e-05 - val_loss: 0.0018\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.1419e-06 - val_loss: 0.0018\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.7598e-06 - val_loss: 0.0018\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.9765e-06 - val_loss: 0.0018\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0643e-05 - val_loss: 0.0018\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.8419e-06 - val_loss: 0.0018\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.3517e-06 - val_loss: 0.0018\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.4196e-06 - val_loss: 0.0018\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.1030e-06 - val_loss: 0.0018\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.3380e-06 - val_loss: 0.0018\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.8338e-06 - val_loss: 0.0018\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.7409e-06 - val_loss: 0.0018\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.5524e-06 - val_loss: 0.0018\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.0859e-06 - val_loss: 0.0018\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.0103e-06 - val_loss: 0.0018\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.7987e-07 - val_loss: 0.0018\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4887e-06 - val_loss: 0.0018\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8826e-06 - val_loss: 0.0018\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7399e-06 - val_loss: 0.0018\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3360e-06 - val_loss: 0.0018\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.3335e-07 - val_loss: 0.0018\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4372e-06 - val_loss: 0.0018\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.9632e-06 - val_loss: 0.0018\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0167e-06 - val_loss: 0.0018\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.0786e-07 - val_loss: 0.0018\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.2361e-07 - val_loss: 0.0018\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.6293e-07 - val_loss: 0.0018\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0008e-06 - val_loss: 0.0018\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.0583e-07 - val_loss: 0.0018\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.0779e-07 - val_loss: 0.0018\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.2261e-07 - val_loss: 0.0018\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.8994e-07 - val_loss: 0.0018\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.1011e-07 - val_loss: 0.0018\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.3708e-07 - val_loss: 0.0018\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.8974e-07 - val_loss: 0.0018\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.4160e-07 - val_loss: 0.0018\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.8453e-07 - val_loss: 0.0018\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.0070e-07 - val_loss: 0.0018\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6033e-07 - val_loss: 0.0018\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.4900e-07 - val_loss: 0.0018\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.6363e-07 - val_loss: 0.0018\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1545e-07 - val_loss: 0.0018\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.0266e-07 - val_loss: 0.0018\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7820e-07 - val_loss: 0.0018\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.2142e-07 - val_loss: 0.0018\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.2905e-07 - val_loss: 0.0018\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.3551e-08 - val_loss: 0.0018\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1424e-07 - val_loss: 0.0018\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.0273e-07 - val_loss: 0.0018\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2666e-07 - val_loss: 0.0018\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.4865e-08 - val_loss: 0.0018\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1557e-07 - val_loss: 0.0018\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2475e-07 - val_loss: 0.0018\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3638e-07 - val_loss: 0.0018\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.2722e-08 - val_loss: 0.0018\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.7283e-08 - val_loss: 0.0018\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1479e-07 - val_loss: 0.0018\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1872e-07 - val_loss: 0.0018\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.6976e-08 - val_loss: 0.0018\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.8262e-08 - val_loss: 0.0018\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.2106e-08 - val_loss: 0.0018\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.6864e-08 - val_loss: 0.0018\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1219e-07 - val_loss: 0.0018\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.0832e-08 - val_loss: 0.0018\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.2986e-08 - val_loss: 0.0018\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.7754e-08 - val_loss: 0.0018\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.4034e-08 - val_loss: 0.0018\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.0732e-08 - val_loss: 0.0018\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7363e-08 - val_loss: 0.0018\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.4668e-08 - val_loss: 0.0018\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.4498e-08 - val_loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "looseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "looseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "o = looseParent.fit(beanIntensities, beanIntensities, epochs=200, verbose = True,  validation_data=(validation, validation))\n",
    "#print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNetAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a second copy of the layers which will be modified to be a denseNET auto encoder\n",
    "'''\n",
    "#TODO: fix call to map to -1?\n",
    "class DenseEncoderLinear2(tf.keras.layers.Layer): #TODO: Fix the decoder to -1\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DenseEncoderLinear2, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.oldrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "\n",
    "class DenseDecoderLinear2(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DenseDecoderLinear2, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.oldrgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        return tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) #used to have a transpose\n",
    "        #return tf.matmul(inputs, self.w)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "        \n",
    "\n",
    "def denseencoder2(parent_child_biological_association, inp, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(490, 3, activation = \"tanh\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    inp = Flatten()(inp)\n",
    "    #print(en_dense.shape, inp.shape)\n",
    "    d = Concatenate()([en_dense, inp]) #dense layer\n",
    "    o_d = Dense(1024, activation = 'tanh')(d) #added a layer\n",
    "    c = Concatenate()([o_d, d]) #TOTALY NEW LAYER\n",
    "    en_dense = Dense(128, activation = 'tanh')(c) #TOTALY NEW LAYER\n",
    "    \n",
    "    phenotype = Dense(num_hidden_units, activation=\"tanh\")(d)\n",
    "    return phenotype\n",
    "\n",
    "def densedecoder2(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(784, activation = 'tanh')(X)\n",
    "    de_dense = Dense(512, activation = 'tanh')(de_dense) #TOTALY NEW LAYER\n",
    "    de_dense = Dense(256, activation = 'tanh')(de_dense) #added a layer\n",
    "    de_dense = Reshape((1, 256))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"tanh\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelDense2(rgm, oldrgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = DenseEncoderLinear2(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    #x = EncoderLinear2(x)\n",
    "    enc = denseencoder2(x, inp, num_hidden_units)\n",
    "    dec = densedecoder2(enc, num_protein_gene, time_steps)\n",
    "    out = DenseDecoderLinear2(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2003 - val_loss: 0.2348\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2112 - val_loss: 0.0961\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0855 - val_loss: 0.0284\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0424 - val_loss: 0.0107\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0287 - val_loss: 0.0286\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0271 - val_loss: 0.0395\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0241 - val_loss: 0.0293\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0157 - val_loss: 0.0146\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0137 - val_loss: 0.0125\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0196 - val_loss: 0.0113\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0152 - val_loss: 0.0146\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0105 - val_loss: 0.0183\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0108 - val_loss: 0.0173\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0128 - val_loss: 0.0152\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0133 - val_loss: 0.0133\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0133 - val_loss: 0.0159\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0115 - val_loss: 0.0142\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0114 - val_loss: 0.0101\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0110 - val_loss: 0.0087\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0093 - val_loss: 0.0121\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0089 - val_loss: 0.0118\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0087 - val_loss: 0.0105\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0091 - val_loss: 0.0108\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0094 - val_loss: 0.0123\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0087 - val_loss: 0.0134\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0083 - val_loss: 0.0113\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0077 - val_loss: 0.0082\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0078 - val_loss: 0.0081\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0082 - val_loss: 0.0104\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0080 - val_loss: 0.0126\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0079 - val_loss: 0.0118\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0076 - val_loss: 0.0098\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0072 - val_loss: 0.0098\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0072 - val_loss: 0.0112\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0072 - val_loss: 0.0120\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0075 - val_loss: 0.0106\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0075 - val_loss: 0.0088\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0074 - val_loss: 0.0082\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0073 - val_loss: 0.0091\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0071 - val_loss: 0.0094\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0071 - val_loss: 0.0087\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0071 - val_loss: 0.0090\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0070 - val_loss: 0.0100\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0070 - val_loss: 0.0105\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0069 - val_loss: 0.0102\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0069 - val_loss: 0.0097\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0070 - val_loss: 0.0098\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0070 - val_loss: 0.0102\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0070 - val_loss: 0.0101\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0069 - val_loss: 0.0093\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0069 - val_loss: 0.0087\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0069 - val_loss: 0.0089\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0069 - val_loss: 0.0097\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0069 - val_loss: 0.0096\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0069 - val_loss: 0.0091\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0069 - val_loss: 0.0097\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0069 - val_loss: 0.0098\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0090\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0090\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0090\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0099\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0068 - val_loss: 0.0090\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0068 - val_loss: 0.0099\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0068 - val_loss: 0.0089\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0088\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0103\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0090\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0103\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0068 - val_loss: 0.0087\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0088\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0069 - val_loss: 0.0096\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0101\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0088\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0094\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0099\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0089\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0090\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0101\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0068 - val_loss: 0.0097\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0091\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0096\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0090\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0092\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0068 - val_loss: 0.0093\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0068 - val_loss: 0.0099\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0068 - val_loss: 0.0093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eaf05b3b20>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "dense.fit(beanIntensities, beanIntensities, epochs=200,  verbose=True, validation_data=(validation, validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 11, 372)]    0           []                               \n",
      "                                                                                                  \n",
      " dense_encoder_linear2_1 (Dense  (None, 11, 372)     138384      ['input_6[0][0]']                \n",
      " EncoderLinear2)                                                                                  \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 9, 490)       547330      ['dense_encoder_linear2_1[0][0]']\n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 4410)         0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 4092)         0           ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 8502)         0           ['flatten_6[0][0]',              \n",
      "                                                                  'flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 32)           272096      ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 784)          25872       ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 512)          401920      ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 256)          131328      ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)            (None, 1, 256)       0           ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_transpose_5 (Conv1DTran  (None, 11, 372)     1047924     ['reshape_5[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " dense_decoder_linear2_1 (Dense  (None, 11, 372)     138384      ['conv1d_transpose_5[0][0]']     \n",
      " DecoderLinear2)                                                                                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,703,238\n",
      "Trainable params: 2,703,238\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dense.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 134ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 11, 372)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = dense.predict(validation)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(o.reshape(11,372))\n",
    "v = pd.DataFrame(validation.reshape(11,372))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>117</th>\n",
       "      <th>131</th>\n",
       "      <th>164</th>\n",
       "      <th>225</th>\n",
       "      <th>259</th>\n",
       "      <th>334</th>\n",
       "      <th>350</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.362516</td>\n",
       "      <td>0.496475</td>\n",
       "      <td>0.372111</td>\n",
       "      <td>0.430475</td>\n",
       "      <td>0.520153</td>\n",
       "      <td>0.435315</td>\n",
       "      <td>0.486603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350683</td>\n",
       "      <td>0.473706</td>\n",
       "      <td>0.429351</td>\n",
       "      <td>0.432575</td>\n",
       "      <td>0.567391</td>\n",
       "      <td>0.483652</td>\n",
       "      <td>0.491651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.366587</td>\n",
       "      <td>0.430712</td>\n",
       "      <td>0.447965</td>\n",
       "      <td>0.403672</td>\n",
       "      <td>0.589513</td>\n",
       "      <td>0.510056</td>\n",
       "      <td>0.471765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.006958</td>\n",
       "      <td>-0.526775</td>\n",
       "      <td>-1.007270</td>\n",
       "      <td>-1.005157</td>\n",
       "      <td>-0.926593</td>\n",
       "      <td>-1.005033</td>\n",
       "      <td>-1.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.392949</td>\n",
       "      <td>0.382632</td>\n",
       "      <td>0.417534</td>\n",
       "      <td>0.446376</td>\n",
       "      <td>0.505069</td>\n",
       "      <td>0.485860</td>\n",
       "      <td>0.498799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.341756</td>\n",
       "      <td>0.418540</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.325601</td>\n",
       "      <td>0.545365</td>\n",
       "      <td>0.420619</td>\n",
       "      <td>0.499098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.419024</td>\n",
       "      <td>0.467960</td>\n",
       "      <td>0.434355</td>\n",
       "      <td>0.452357</td>\n",
       "      <td>0.598895</td>\n",
       "      <td>0.505582</td>\n",
       "      <td>0.502952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.014701</td>\n",
       "      <td>-0.019644</td>\n",
       "      <td>0.057635</td>\n",
       "      <td>0.038698</td>\n",
       "      <td>-0.008534</td>\n",
       "      <td>0.006706</td>\n",
       "      <td>-0.018166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.351214</td>\n",
       "      <td>0.456517</td>\n",
       "      <td>0.417395</td>\n",
       "      <td>0.406108</td>\n",
       "      <td>0.548310</td>\n",
       "      <td>0.482105</td>\n",
       "      <td>0.494334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.362195</td>\n",
       "      <td>0.478551</td>\n",
       "      <td>0.344282</td>\n",
       "      <td>0.312177</td>\n",
       "      <td>0.563712</td>\n",
       "      <td>0.488460</td>\n",
       "      <td>0.482327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.382386</td>\n",
       "      <td>0.488227</td>\n",
       "      <td>0.412796</td>\n",
       "      <td>0.416879</td>\n",
       "      <td>0.587944</td>\n",
       "      <td>0.509722</td>\n",
       "      <td>0.504098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         117       131       164       225       259       334       350\n",
       "0   0.362516  0.496475  0.372111  0.430475  0.520153  0.435315  0.486603\n",
       "1   0.350683  0.473706  0.429351  0.432575  0.567391  0.483652  0.491651\n",
       "2   0.366587  0.430712  0.447965  0.403672  0.589513  0.510056  0.471765\n",
       "3  -1.006958 -0.526775 -1.007270 -1.005157 -0.926593 -1.005033 -1.001392\n",
       "4   0.392949  0.382632  0.417534  0.446376  0.505069  0.485860  0.498799\n",
       "5   0.341756  0.418540  0.347165  0.325601  0.545365  0.420619  0.499098\n",
       "6   0.419024  0.467960  0.434355  0.452357  0.598895  0.505582  0.502952\n",
       "7   0.014701 -0.019644  0.057635  0.038698 -0.008534  0.006706 -0.018166\n",
       "8   0.351214  0.456517  0.417395  0.406108  0.548310  0.482105  0.494334\n",
       "9   0.362195  0.478551  0.344282  0.312177  0.563712  0.488460  0.482327\n",
       "10  0.382386  0.488227  0.412796  0.416879  0.587944  0.509722  0.504098"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>117</th>\n",
       "      <th>131</th>\n",
       "      <th>164</th>\n",
       "      <th>225</th>\n",
       "      <th>259</th>\n",
       "      <th>334</th>\n",
       "      <th>350</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.464054</td>\n",
       "      <td>0.445813</td>\n",
       "      <td>0.475648</td>\n",
       "      <td>0.475451</td>\n",
       "      <td>0.601666</td>\n",
       "      <td>0.538352</td>\n",
       "      <td>0.517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464782</td>\n",
       "      <td>0.410751</td>\n",
       "      <td>0.479145</td>\n",
       "      <td>0.469503</td>\n",
       "      <td>0.615856</td>\n",
       "      <td>0.581054</td>\n",
       "      <td>0.521357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.448974</td>\n",
       "      <td>0.424922</td>\n",
       "      <td>0.462434</td>\n",
       "      <td>0.489874</td>\n",
       "      <td>0.618050</td>\n",
       "      <td>0.541640</td>\n",
       "      <td>0.511626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.448781</td>\n",
       "      <td>0.442738</td>\n",
       "      <td>0.473856</td>\n",
       "      <td>0.468686</td>\n",
       "      <td>0.588641</td>\n",
       "      <td>0.488964</td>\n",
       "      <td>0.528962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.375646</td>\n",
       "      <td>0.487818</td>\n",
       "      <td>0.385170</td>\n",
       "      <td>0.420683</td>\n",
       "      <td>0.515270</td>\n",
       "      <td>0.469018</td>\n",
       "      <td>0.508054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.506284</td>\n",
       "      <td>0.412253</td>\n",
       "      <td>0.486031</td>\n",
       "      <td>0.449196</td>\n",
       "      <td>0.638067</td>\n",
       "      <td>0.580319</td>\n",
       "      <td>0.503012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.469681</td>\n",
       "      <td>0.415919</td>\n",
       "      <td>0.528647</td>\n",
       "      <td>0.509842</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.563962</td>\n",
       "      <td>0.527670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.462067</td>\n",
       "      <td>0.470147</td>\n",
       "      <td>0.484273</td>\n",
       "      <td>0.481506</td>\n",
       "      <td>0.631007</td>\n",
       "      <td>0.569937</td>\n",
       "      <td>0.540709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.525481</td>\n",
       "      <td>0.414705</td>\n",
       "      <td>0.491357</td>\n",
       "      <td>0.473794</td>\n",
       "      <td>0.641192</td>\n",
       "      <td>0.572485</td>\n",
       "      <td>0.556883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         117       131       164       225       259       334       350\n",
       "0   0.464054  0.445813  0.475648  0.475451  0.601666  0.538352  0.517300\n",
       "1   0.464782  0.410751  0.479145  0.469503  0.615856  0.581054  0.521357\n",
       "2   0.448974  0.424922  0.462434  0.489874  0.618050  0.541640  0.511626\n",
       "3  -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000\n",
       "4   0.448781  0.442738  0.473856  0.468686  0.588641  0.488964  0.528962\n",
       "5   0.375646  0.487818  0.385170  0.420683  0.515270  0.469018  0.508054\n",
       "6   0.506284  0.412253  0.486031  0.449196  0.638067  0.580319  0.503012\n",
       "7  -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000\n",
       "8   0.469681  0.415919  0.528647  0.509842  0.595536  0.563962  0.527670\n",
       "9   0.462067  0.470147  0.484273  0.481506  0.631007  0.569937  0.540709\n",
       "10  0.525481  0.414705  0.491357  0.473794  0.641192  0.572485  0.556883"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0051756306>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_noParent_MSE(validation, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Size Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 40/40 [38:28<00:00, 57.71s/it]\n"
     ]
    }
   ],
   "source": [
    "N = 40\n",
    "hidden = np.arange(2,24, 1) #range(1,32)\n",
    "lossMatrix = []\n",
    "for i in tqdm(range(N)):\n",
    "    \n",
    "    losses = []\n",
    "    for value in (hidden):\n",
    "        dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, value)\n",
    "        dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "        dense.fit(beanIntensities, beanIntensities, epochs=40,  verbose=0)\n",
    "        test_hat = dense(validation) #, verbose = 0)\n",
    "        loss = ignore_noParent_MSE(validation, test_hat)\n",
    "        losses.append(loss)\n",
    "        tf.keras.backend.clear_session()\n",
    "    lossMatrix.append(losses)\n",
    "    \n",
    "lossMatrix = np.array(lossMatrix)\n",
    "#run 100 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "avgMSE = np.average(lossMatrix, axis = 0)\n",
    "plt.plot(hidden, avgMSE);\n",
    "plt.xlabel(\"Latent Space Size\");\n",
    "plt.ylabel(\"MSE\");\n",
    "plt.title(\"MSE of the AutoEncoder with Respect to the Latent Space Size\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = pd.DataFrame(lossMatrix)\n",
    "lm.to_csv(\"lossmatrix4lisa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 23)\n"
     ]
    }
   ],
   "source": [
    "temp = pd.read_csv(\"lossmatrix4lisa.csv\").to_numpy()\n",
    "print(temp.shape)\n",
    "avgMSE = np.average(lossMatrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ec16d3a370>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hidden, avgMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lazy_train(epochs=60, hidden = 32):\n",
    "    ep = epochs\n",
    "    hidden = [hidden,] #range(1,32)\n",
    "    lossMatrix = []\n",
    "    lazy_weights = []\n",
    "    dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, num_hidden_units=32)\n",
    "    dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "\n",
    "    #for i in tqdm(range(ep)):\n",
    "    for i in range(ep):\n",
    "        dense.fit(beanIntensities, beanIntensities, validation_data=(validation, validation), epochs=1,  verbose=0)\n",
    "        lazy_weights.append(dense.get_weights())\n",
    "        test = dense(validation) #, verbose = 0)\n",
    "        loss = ignore_noParent_MSE(validation, test)\n",
    "        lossMatrix.append(loss)\n",
    "        \n",
    "    lossMatrix = np.array(lossMatrix)\n",
    "    lazy_weights = np.array(lazy_weights)\n",
    "\n",
    "    return lossMatrix, lazy_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://proceedings.mlr.press/v162/rachwan22a/rachwan22a.pdf Winning the Lottery Ticket Ahead of Time:\n",
    "def lazyKernelRegime(w, parent_idx=parent_idx):\n",
    "\n",
    "    firstLayer = []\n",
    "    for i in range(len(w)):\n",
    "        firstLayer.append(w[i][0][parent_idx])\n",
    "\n",
    "    fL = np.array(firstLayer)    \n",
    "    d0 = np.square(fL[1] - fL[0])\n",
    "    kernelChange = []\n",
    "    for i in range(1,len(fL)):\n",
    "        dt = np.square(fL[i] - fL[0])\n",
    "        dt_minus1 = np.square(fL[i-1] - fL[0])   \n",
    "        d = np.abs(dt - dt_minus1)/d0                        #eq 11 from the paper\n",
    "        kernelChange.append(d)\n",
    "    \n",
    "    kernelChange = np.moveaxis(kernelChange, 0, 2)\n",
    "    # plt.plot(kernelChange[0][0]);\n",
    "    # plt.title(\"$|\\Delta W|$ vs Epoch\")\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"$|\\Delta W|$\")\n",
    "\n",
    "    return np.array(  kernelChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distrib(change, t = .05, raw = False): #raw means return the unshaped indicies. \n",
    "    stop = []\n",
    "\n",
    "    for parent in range(len(change)):\n",
    "        for child in range(len(change[0])):\n",
    "            try:\n",
    "                stop.append(np.min(np.argwhere(change[parent][child] < t)))\n",
    "            except ValueError:\n",
    "                stop.append(len(change))\n",
    "                # print(parent, child)\n",
    "                # plt.plot(change[parent][child])\n",
    "                # assert(False)\n",
    "    # print(change.shape)\n",
    "    # assert(False)\n",
    "    \n",
    "    stop = np.array(stop).flatten()\n",
    "    # print(stop.shape)\n",
    "    var = np.std(stop)\n",
    "    # print(var)\n",
    "    # assert(False)\n",
    "    mean = np.average(stop)\n",
    "    top_parents = np.argwhere(stop > mean + 2*var) #get the parents which take more than 2 stds to stop training\n",
    "    top_parent_child = []\n",
    "    for tp in top_parents:\n",
    "        top_parent_child.append(np.unravel_index(tp, shape = (NUM_PARENTS, NUM_TARGETS)))\n",
    "\n",
    "    if raw == False:\n",
    "        plt.hist(stop)\n",
    "        plt.xlabel(\"Epoch where regulator-target-weight began changing by at most \"+ str(t));\n",
    "        plt.ylabel(\"Number of parent-child-weights\");\n",
    "        plt.title(\"Histogram of weight stops\");\n",
    "        print(\"average stop: \", mean);\n",
    "\n",
    "    if raw == True:\n",
    "        return np.array(top_parents)\n",
    "\n",
    "    return np.array(top_parent_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazyKernels(N=100):\n",
    "\n",
    "    candidates = []\n",
    "    final_w = []\n",
    "    for i in tqdm(range(N)):\n",
    "        lm, lazy_weights = do_lazy_train(epochs=80)\n",
    "        change = lazyKernelRegime(lazy_weights)\n",
    "        top_pr = np.squeeze(compute_distrib(change, t = 0.05, raw=True))\n",
    "        candidates.append(top_pr)\n",
    "        final_w.append(lazy_weights[-1])\n",
    "    \n",
    "    final_w = np.array(final_w)\n",
    "    firstLayer = []\n",
    "    for i in range(len(final_w)):\n",
    "        firstLayer.append(np.abs(final_w[i][0][parent_idx]))\n",
    "    fw = np.array(firstLayer)\n",
    "    #print(fw.shape)\n",
    "    fw_avg = np.average(fw, axis = 0)\n",
    "    #print(fw_avg.shape)\n",
    "    \n",
    "    candidates = np.hstack(candidates)\n",
    "    candidates = candidates.reshape(candidates.size)\n",
    "    #print(candidates.shape)\n",
    "\n",
    "    plt.hist(candidates, bins=np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    plt.title(\"Parent-Child Regulator Histogram\")\n",
    "    plt.xlabel(\"Parent-Child Weight\")\n",
    "    plt.ylabel(\"Num-Times parent-child relationship trained for top 5% of time\")\n",
    "    return candidates, fw_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can, mag = lazyKernels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf, bins = np.histogram(can, np.arange(NUM_PARENTS*NUM_TARGETS))\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitudes After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS*NUM_TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(NUM_PARENTS*NUM_TARGETS, shape = (NUM_PARENTS, NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_reg_targets(can, mag):\n",
    "    \n",
    "    pdf, bins = np.histogram(can, np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    pdf2d = np.zeros(shape = (NUM_PARENTS,NUM_TARGETS))\n",
    "\n",
    "    for i in bins:\n",
    "      idx2d = np.unravel_index(i, shape = (NUM_PARENTS, NUM_TARGETS))\n",
    "      try:\n",
    "        pdf2d[idx2d] = pdf[i]\n",
    "      except IndexError:\n",
    "        print(i, idx2d, len(pdf), len(bins))\n",
    "    \n",
    "\n",
    "    importance = np.multiply(pdf2d, mag)\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = get_top_reg_targets(can, mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top[6][122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(top, cmap = 'inferno', vmin = 0, vmax = np.max(top));\n",
    "plt.colorbar(u ,fraction=0.0086, pad=0.02);\n",
    "plt.title(\"The Most Important Regulator-Target Combinations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topID = np.array(np.unravel_index(np.argsort(top, axis=None), top.shape))\n",
    "topID = np.flip(topID, axis=1)\n",
    "topID[0] = parent_idx[topID[0]]\n",
    "topID = topID.T\n",
    "topID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topR_T = pd.DataFrame(topID)\n",
    "# topR_T.to_csv(\"Top_reg_target_decendingOrder_firstColIsRegulator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.zeros(NUM_PARENTS)\n",
    "for i in range(NUM_PARENTS):\n",
    "    best[i] = np.sum(top[i])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,NUM_PARENTS, dtype=int)\n",
    "plt.plot(x, best); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.flip(np.argsort(best))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(can).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Petal Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal = pd.read_excel(data_path_petal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train = petal[petal[\"Line\"] == \"WT\"]\n",
    "petal_train = petal_train.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_train.head(12)\n",
    "petal_train = petal_train.groupby(['Plate']).mean()\n",
    "petal_train.head()\n",
    "petal_train = petal_train.to_numpy()\n",
    "print(petal_train.shape)\n",
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(petal_train)\n",
    "petal_train = scaler1.transform(petal_train)\n",
    "mm = MinMaxScaler()\n",
    "mm.fit(petal_train)\n",
    "petal_train = mm.transform(petal_train)\n",
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test = petal[petal[\"Line\"] != \"WT\"]\n",
    "petal_test = petal_test.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_test = petal_test.groupby(['Plate']).mean()\n",
    "petal_test.head()\n",
    "petal_test = petal_test.to_numpy()\n",
    "print(petal_test.shape)\n",
    "petal_test = scaler1.transform(petal_test)\n",
    "petal_test = mm.transform(petal_test)\n",
    "petal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1.shape\n",
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densePredictor = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS, 22)\n",
    "densePredictor.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "densePredictor.fit(beanIntensities, beanIntensities,validation_data=(experiment1, experiment1),  epochs=100,  verbose=1)\n",
    "test = densePredictor(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgm = superParent\n",
    "time_steps = 6\n",
    "num_kinase_regulators = NUM_TARGETS\n",
    "num_hidden_units = 22\n",
    "\n",
    "inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "x = DenseEncoderLinear2(rgm, regulator_gene_matrix, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "enc = denseencoder2(x, inp, num_hidden_units)\n",
    "denseP = tf.keras.Model(inputs=inp, outputs=enc)\n",
    "#set the weights of the encoder to the weights of auto encoder\n",
    "dw = densePredictor.get_weights()\n",
    "enc_w = dw[0:5]\n",
    "denseP.set_weights(enc_w)\n",
    "#add a dense layer  because we are ouputing 1 number\n",
    "l = Dense(32, activation = 'swish', use_bias=True, kernel_regularizer='l1_l2')(denseP.layers[-1].output)\n",
    "l = Dense(1, activation = 'linear', use_bias = True)(l)\n",
    "denseP = tf.keras.Model(denseP.inputs, l)\n",
    "#denseP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = np.concatenate([experiment1, experiment1, experiment1, experiment1])\n",
    "bp.shape\n",
    "#bigexperiment1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = petal_train[0]\n",
    "b = petal_train[1]\n",
    "c = petal_train[2]\n",
    "d = petal_train[3]\n",
    "\n",
    "petal_train1 = np.array([a,a,a,a, b,b,b,b, c,c,c,c, d,d,d,d]) #does this make sense? we are training network to predict .5\n",
    "petal_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "denseP.fit(experiment1, petal_train, epochs=500, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP(experiment1) #experiment1 is part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(denseP(testCandidate)) #model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test #true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.evaluate(testCandidate, petal_test) #eval gave 1.3999 before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test set and the synthetic dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestSet(test_path):\n",
    "    testFiles = []\n",
    "    for np_name in glob(os.path.join(data_path_testSet,'*.np[yz]')):\n",
    "        k = np.load(os.path.join(data_path_testSet,np_name))\n",
    "        testFiles.append(k)\n",
    "#         print(np_name)\n",
    "#         print(k.shape)\n",
    "    return np.array(testFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PKjzdFCMwFg"
   },
   "outputs": [],
   "source": [
    "def read_files(data_path):\n",
    "\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        try:\n",
    "            #print('file name:',file)\n",
    "            #print('value of i:',i)\n",
    "            genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "            #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "            replicates.append(np.array(genes_intensities_data_matrix.values, dtype = float))\n",
    "            # i+=1\n",
    "        except PermissionError:\n",
    "            print(\"Not a CSV: \", os.path.join(data_path , file))\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    rep = np.array(replicates).astype(np.float32)\n",
    "    \n",
    "    return rep, rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCwo4LwlO_FF"
   },
   "outputs": [],
   "source": [
    "beanIntensities, regulator_gene_matrix= read_files(data_path_syn)\n",
    "matrix = regulator_gene_matrix\n",
    "replicates = beanIntensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros(shape = (3,6,8))\n",
    "id = np.unravel_index(3*6*8 - 1, shape = d.shape)\n",
    "d[id] = 1\n",
    "plt.imshow(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0][ : , parentIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)\n",
    "print(testCandidate.shape)\n",
    "syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(change[0][22])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"change in weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossMatrix)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change.shape, lossMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diff(change[0][22])\n",
    "plt.plot(d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change[0][0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_tresh(change, stop = 0.05):\n",
    "#     diffs = []\n",
    "#     for parent in range(len(change)):\n",
    "#         for child in range(len(change[0])):\n",
    "#             diffs.append(np.diff(change[parent][child]))\n",
    "#     inflection = []\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         for d in diffs:\n",
    "#             print(np.argwhere(np.abs(d) < stop))\n",
    "#             inflection.append(np.min(np.argwhere(np.abs(d) < stop))) #return where the second derivative is first 0. \n",
    "\n",
    "#     except ValueError:\n",
    "#         print(\"Stop value \", stop, \" is too high, trying stop = \", stop + 0.05)\n",
    "#         # s = stop + 0.05\n",
    "#         # return compute_tresh(change, stop = s)\n",
    "        \n",
    "\n",
    "#     return np.average(inflection)\n",
    "        \n",
    "# d = compute_tresh(change)\n",
    "# d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"mse3.npy\", avgMSE) #mse2/3 is with -1 fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sciencedirect.com/science/article/pii/S0925231220314570?casa_token=lcEJANqO0JwAAAAA:uL3DGUZctPUZz_sPz1K1i2klMtb83TyKnc9CI3_N-uSOaM7VHL8GhM0jCGYfo25NmpDQQ9Cvlw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rshp = Flatten()(looseParent.layers[-1].output)\n",
    "\n",
    "modelTemp = tf.keras.Model(inputs=looseParent.input, outputs = [rshp])\n",
    "modelTemp.summary()\n",
    "type(modelTemp)\n",
    "explainer = shap.DeepExplainer(modelTemp, syntheticDataTrain)\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough #this solves the \"shap_ADDV2\" problem but another one will appear\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"FusedBatchNormV3\"] = shap.explainers._deep.deep_tf.passthrough #this solves the next problem which allows you to run the DeepExplainer.\n",
    "\n",
    "shap_values = explainer.shap_values(testCandidate[0:1])\n",
    "def f(x):\n",
    "    return modelTemp.predict(x)\n",
    "\n",
    "print(f(testCandidate))\n",
    "explainer = shap.KernelExplainer(f , testCandidate[0:1], link=\"logit\") #svm.predict_proba, X_train, link=\"logit\")\n",
    "shap_values = explainer.shap_values(testCandidate[0:1], nsamples=100)\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import json\n",
    "import shap\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "X,y = shap.datasets.imagenet50()\n",
    "to_explain = X[[39,41]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(enc_dec_Synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [5,6]\n",
    "u = tf.concat([a,b], axis = 0)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newConnections = superParent - regulator_gene_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(newConnections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nC = []\n",
    "# for i in range(len(newConnections[0])):\n",
    "#     for j in range(len(newConnections[1])):\n",
    "#         if newConnections[i][j] > 0:\n",
    "#             nC.append([i,j])\n",
    "# nC = np.array(nC)\n",
    "# nC = pd.DataFrame(nC)\n",
    "# nC.to_csv(\"new_connections_in_superParents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code for testing loss function\n",
    "# print(outSyn.shape)\n",
    "# print(testCandidate.shape)\n",
    "# syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "# syntheticLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Auto Encoder\n",
    "Autoencoder has not been trained on synthetic version of experiement 1. We test on the original experiment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrRuJ_bsrHll"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAOZEEvRRVU4"
   },
   "outputs": [],
   "source": [
    "# enc_dec_Synthetic.compile(optimizer='adam',loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticDataTrain = beanIntensities[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1649273035573,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "EuNDZx5Ov34I",
    "outputId": "74597aa1-c71c-4491-b8c9-4fefed309325"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.fit(syntheticDataTrain,syntheticDataTrain,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = enc_dec_Synthetic(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = enc_dec_Synthetic.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w[0], cmap = \"hot\", vmin=0,vmax=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBzDmjqJViEw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we do not need to use this function for the testset\n",
    "def getCSVs(data_path_head):\n",
    "    PATH = data_path_head\n",
    "    EXT = \"*.csv\"\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "    actual = []\n",
    "    for p in all_csv_files:\n",
    "        actual.append(pd.read_csv(p, index_col = 0).to_numpy())\n",
    "    return np.array(actual)\n",
    "    \n",
    "experiment1 = getCSVs(data_path_og_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate = test.numpy().astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outSyn = enc_dec_Synthetic.predict(testCandidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymagn(A, B):\n",
    "    mse = (np.square(A - B)).mean(axis=None)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticLoss = ignore_noParent_MSE(testCandidate, outSyn )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(outSyn-testCandidate).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install keras-visualizer\n",
    "#!pip install pydot\n",
    "#data_path_og_exp1 = data_path_testSet \n",
    "# !pip install pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = loadTestSet(data_path_testSet)\n",
    "# testCandidate = test.astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPRV0OAMpKPH"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS) #we can just change the time steps to something higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1649273028683,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "D6aPT5_cpKK0",
    "outputId": "c261d824-0f38-4eee-b139-f03a80f093e1"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated dataset Auto Encoder\n",
    "Once again, we do not train on any version of exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filesV2(data_path):\n",
    "    '''\n",
    "    *Changed*\n",
    "    currently hardcoded for only one file. \n",
    "    change code a bit for reading multiple files.\n",
    "    '''\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        #print('file name:',file)\n",
    "        #print('value of i:',i)\n",
    "        genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "        #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "        replicates.append(genes_intensities_data_matrix.values)\n",
    "        # i+=1\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    \n",
    "    return np.asarray(replicates), rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes, _ = read_filesV2(data_path_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(interpolated_genes[2]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = []\n",
    "for k in range(len(interpolated_genes)):\n",
    "    #print(k)\n",
    "    if k == 2 or k == 3 or k == 4:\n",
    "        inter.append(np.reshape(interpolated_genes[k], (4,6,NUM_TARGETS)))\n",
    "    else: \n",
    "        inter.append(np.reshape(interpolated_genes[k], (5,6,NUM_TARGETS)))\n",
    "inter = np.vstack(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beanIntensities[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter = model(regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS) \n",
    "enc_dec_inter.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter.fit(inter, inter,epochs=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter = enc_dec_inter.predict(testCandidate)\n",
    "interpolationLoss = ignore_noParent_MSE(testCandidate, outInter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolationLoss #used to be 3.84 on broke ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = enc_dec_inter.history\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons between various outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(np.reshape((np.abs(outSyn)), (24,NUM_TARGETS)), cmap = \"hot\", vmin=0,vmax=1.0 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(outSyn[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(testCandidate[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape((np.abs(outSyn-outInter)), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 );\n",
    "plt.title(\"Difference Between the Outputs of Both Autoencoders\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"DiffBetweenOut.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outInter-experiment1), (24,NUM_TARGETS)) , cmap = \"hot\") #, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Interpolated Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30)\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"InterDiffImage.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outSyn-experiment1), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"SynDiffImage.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx = parentIndex.numpy()\n",
    "#print(parent_idx)\n",
    "oSyn = (np.reshape((outSyn), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "oSyn = oSyn.T\n",
    "oSyn.shape\n",
    "\n",
    "exp1_col = (np.reshape((experiment1), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "exp1_col = exp1_col.T\n",
    "print(exp1_col.shape)\n",
    "\n",
    "u = plt.imshow(np.abs(oSyn - exp1_col), cmap = 'hot') #TODO use TF loss function instead of difference.\n",
    "ddff = oSyn-exp1_col\n",
    "plt.colorbar(u)\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data. Only parents.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(oSyn).head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(exp1_col).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ddff).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"interpolated_v2.npy\", inter) #the interpolated dataset\n",
    "# np.save(\"synthetic_v2.npy\", beanIntensities[1:]) # the synthetic dataset\n",
    "# np.save(\"synOut_v2.npy\", outSyn) #the output of the encoder trained on synthetic data with the input being exp1\n",
    "# np.save(\"interOut_v2.npy\", outInter) #the output of the encoder trained on interpolated data with the input being exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6J5QSynTTgZ+lFOhgOtAG",
   "collapsed_sections": [],
   "name": "cnn-third.ipynb",
   "provenance": [
    {
     "file_id": "1mmRxO5jnBc5CdzxXj8sMtPpG0BQ0ulSs",
     "timestamp": 1648921397876
    },
    {
     "file_id": "10758zFj2UnTnwpQaSFIr5r9MqZWdiMsf",
     "timestamp": 1648674775255
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
