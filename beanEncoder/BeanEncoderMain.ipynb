{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qTo_HuQkGgAq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_visualizer import visualizer \n",
    "\n",
    "from tensorflow.keras.layers import*\n",
    "import shap\n",
    "from keras.utils.vis_utils import plot_model\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Conv1D\n",
    "# from keras.layers import Conv1DTranspose\n",
    "# from keras.layers import Flatten, Reshape\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS = 7\n",
    "NUM_TARGETS = 372\n",
    "NUM_TIME_STEPS = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gXpISOijEYqQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# matrix_path = \"regulator-gene-matrix.csv\"\n",
    "# data_path_syn = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\synData\"\n",
    "# data_path_inter =  r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\interpolatedOnly\"\n",
    "# data_path_og_exp1 = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\datasets\\exp1\"\n",
    "# data_path_testSet = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\testSetFixed\"\n",
    "# data_path_petal = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\petal_len.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_RGM = r'Regulations_Control_Altona.csv'\n",
    "dirty_regulations = r'FullTable_Control.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirtyRGM = pd.read_csv(dirty_RGM,  index_col = 0, )#on_bad_lines='skip')\n",
    "dirtyReg = pd.read_csv(dirty_regulations,  index_col = 0,)# on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372, 44)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtyReg = dirtyReg.select_dtypes(include=np.number)\n",
    "dirtyReg = dirtyReg.to_numpy()\n",
    "dirtyReg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(dirtyReg).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 372)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we want to scale the regulators but some of the data is 0 which means it was not computed. so we remove the 0s and then scale and then replace the 0s with -1 for the autoencoder \n",
    "\n",
    "dirtyReg[dirtyReg==0] = np.nan\n",
    "regScaled = MinMaxScaler().fit_transform(dirtyReg.flatten().reshape((-1,1)))\n",
    "regScaled = regScaled.reshape((NUM_TARGETS, NUM_TIME_STEPS))\n",
    "regScaled = np.nan_to_num(regScaled, nan= -1)\n",
    "#pd.DataFrame(regScaled).head(10)\n",
    "regScaled = regScaled.T\n",
    "regScaled.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 44, 372)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beanIntensities = regScaled.copy()\n",
    "#expand dims for AE\n",
    "beanIntensities = np.expand_dims(beanIntensities, axis=0)\n",
    "beanIntensities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulator_gene_matrix = np.load(\"soyBeanRGM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulator_gene_matrix = regulator_gene_matrix.astype('float32')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent Matrix + Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of parent index (7,)\n"
     ]
    }
   ],
   "source": [
    "superParent = regulator_gene_matrix.copy() #init the super parent with the ordinary RGM, and do forward passes with super parent\n",
    "#print(superParent.shape)\n",
    "\n",
    "ones = np.ones((NUM_TARGETS))\n",
    "parentIndex = []\n",
    "not_parentIndex = []\n",
    "for i in range(len(regulator_gene_matrix)):\n",
    "    if (np.isin(regulator_gene_matrix[i], [1])).any():\n",
    "        #print(i)\n",
    "        superParent[i] = ones \n",
    "        parentIndex.append(i)\n",
    "    else:\n",
    "        not_parentIndex.append(i)\n",
    "\n",
    "parentIndex = np.array(parentIndex)\n",
    "parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "not_parentIndex = np.array(not_parentIndex)\n",
    "not_parentIndex = tf.convert_to_tensor(not_parentIndex)\n",
    "print(\"shape of parent index\", parentIndex.shape)\n",
    "\n",
    "def ignore_noParent_MSE_old(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "    y_true_pruned = tf.gather(y_true,parentIndex, axis =2) \n",
    "    #print(y_true_pruned.shape\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis =2)   \n",
    "    return l(y_true_pruned, y_pred_pruned)\n",
    "\n",
    "#this will not work if the entire dataset is -1 (degenerate)\n",
    "def ignore_noParent_MSE(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "    #print(y_true.shape) #(None, 44, 372)\n",
    "\n",
    "    #get the parents and flatten them\n",
    "    y_true_pruned = tf.gather(y_true, parentIndex, axis = 2) #axis 2 because batch, time, gene\n",
    "    y_true_pruned = tf.reshape(y_true_pruned, shape=([tf.size(y_true_pruned)] ) )\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis = 2) \n",
    "    y_pred_pruned = tf.reshape(y_pred_pruned, shape=([tf.size(y_pred_pruned)]) )\n",
    "\n",
    "    #get the index of the parents which are not -1\n",
    "    y_true_posID = tf.where(y_true_pruned >= 0) #gets args\n",
    "    y_true_posID = tf.squeeze(y_true_posID)\n",
    "    #get the idx of all the -1s \n",
    "    y_true_negID = tf.where(y_true_pruned < 0) \n",
    "    y_true_negID = tf.squeeze(y_true_negID)\n",
    "\n",
    "    #get all the -1s in the parents \n",
    "    y_true_neg = tf.gather(y_true_pruned, y_true_negID) #get all the -1s in y_true\n",
    "    y_pred_neg = tf.gather(y_pred_pruned, y_true_negID) #get the corresponding values for y_pred\n",
    "\n",
    "    #get the indexes where pred should be -1 but is not. get the corresponding index for ytrue\n",
    "    y_shouldBeNegButIsntID = tf.where(y_pred_neg >= 0)  \n",
    "    y_shouldBeNegButIsntID = tf.squeeze(y_shouldBeNegButIsntID) #get the idx which should be -1 for prediction but are not\n",
    "    y_true_wrong = tf.gather(y_true_pruned, y_shouldBeNegButIsntID) #get the same corresponding values from ypred\n",
    "    y_shouldBeNegButIsnt = tf.gather(y_pred_pruned, y_shouldBeNegButIsntID) #this has all the wrongly predicted values which should be -1 but are not\n",
    "\n",
    "    y_true_pos = tf.gather(y_true_pruned, y_true_posID)\n",
    "    y_pred_pos = tf.gather(y_pred_pruned, y_true_posID)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0. \n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "    y_pred_total = tf.concat([y_pred_pos, y_shouldBeNegButIsnt], axis = 0) #concatenate for total mse\n",
    "    y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\n",
    "\n",
    "    return l(y_true_total, y_pred_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1338231e370>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(superParent, cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x13385179bb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(regulator_gene_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1yhzyikoFncq"
   },
   "outputs": [],
   "source": [
    "class EncoderLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, input_dim=32, units=32):\n",
    "        super(EncoderLinear, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.rgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4uchxU-bmBDe"
   },
   "outputs": [],
   "source": [
    "class DecoderLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, input_dim=32, units=32):\n",
    "        super(DecoderLinear, self).__init__()\n",
    "        self.rgm = rgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        #return tf.matmul(inputs, self.w)\n",
    "        # v = tf.zeros_like(X)\n",
    "        # u = tf.ones_like(X)\n",
    "        # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "        return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PQVz4BCpmNMd"
   },
   "outputs": [],
   "source": [
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(490, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association)\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(1024)(X)#Dense(128)(X)\n",
    "    de_dense = Reshape((1, 1024))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def model(rgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 32):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinaryAE = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS)\n",
    "ordinaryAE.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.847166668042746e-09\n"
     ]
    }
   ],
   "source": [
    "o = ordinaryAE.fit(beanIntensities, beanIntensities, epochs=200, verbose = False)\n",
    "print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(EncoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.OGrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "class DecoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DecoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.OGrgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        #return tf.matmul(inputs, self.w)\n",
    "        # v = tf.zeros_like(X)\n",
    "        # u = tf.ones_like(X)\n",
    "        # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "        return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        \n",
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(128)(X)\n",
    "    de_dense = Reshape((1, 128))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelSuperParent(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSuperParentSequential(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    m = tf.keras.Sequential()\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.212118709379098e-10\n"
     ]
    }
   ],
   "source": [
    "looseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 21)\n",
    "looseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "o = looseParent.fit(beanIntensities, beanIntensities, epochs=240, verbose = 0)\n",
    "print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNetAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a second copy of the layers which will be modified to be a denseNET auto encoder\n",
    "'''\n",
    "#TODO: fix call to map to -1?\n",
    "class DenseEncoderLinear2(tf.keras.layers.Layer): #TODO: Fix the decoder to -1\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DenseEncoderLinear2, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.oldrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "\n",
    "class DenseDecoderLinear2(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DenseDecoderLinear2, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.oldrgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        return tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) #used to have a transpose\n",
    "        #return tf.matmul(inputs, self.w)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "        \n",
    "\n",
    "def denseencoder2(parent_child_biological_association, inp, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(490, 3, activation = \"tanh\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    inp = Flatten()(inp)\n",
    "    #print(en_dense.shape, inp.shape)\n",
    "    d = Concatenate()([en_dense, inp]) #dense layer\n",
    "    o_d = Dense(1024, activation = 'tanh')(d) #added a layer\n",
    "    c = Concatenate()([o_d, d]) #TOTALY NEW LAYER\n",
    "    en_dense = Dense(128, activation = 'tanh')(c) #TOTALY NEW LAYER\n",
    "    \n",
    "    phenotype = Dense(num_hidden_units, activation=\"tanh\")(d)\n",
    "    return phenotype\n",
    "\n",
    "def densedecoder2(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(784, activation = 'tanh')(X)\n",
    "    de_dense = Dense(512, activation = 'tanh')(de_dense) #TOTALY NEW LAYER\n",
    "    de_dense = Dense(256, activation = 'tanh')(de_dense) #added a layer\n",
    "    de_dense = Reshape((1, 256))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"tanh\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelDense2(rgm, oldrgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = DenseEncoderLinear2(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    #x = EncoderLinear2(x)\n",
    "    enc = denseencoder2(x, inp, num_hidden_units)\n",
    "    dec = densedecoder2(enc, num_protein_gene, time_steps)\n",
    "    out = DenseDecoderLinear2(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1376e8c3fa0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "dense.fit(beanIntensities, beanIntensities, epochs=250,  verbose=False) #validation_data=(experiment1, experiment1),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Size Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "hidden = np.arange(2,33, 1) #range(1,32)\n",
    "lossMatrix = []\n",
    "for i in tqdm(range(N)):\n",
    "    \n",
    "    \n",
    "    losses = []\n",
    "    for value in (hidden):\n",
    "        dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS, value)\n",
    "        dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "        dense.fit(beanIntensities, beanIntensities, epochs=120,  verbose=0)\n",
    "\n",
    "        test = dense(testCandidate) #, verbose = 0)\n",
    "        loss = ignore_noParent_MSE(testCandidate, test)\n",
    "        losses.append(loss)\n",
    "        tf.keras.backend.clear_session()\n",
    "    lossMatrix.append(losses)\n",
    "    \n",
    "lossMatrix = np.array(lossMatrix)\n",
    "#run 100 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgMSE = np.average(lossMatrix, axis = 0)\n",
    "plt.plot(hidden, avgMSE);\n",
    "plt.xlabel(\"Latent Space Size\");\n",
    "plt.ylabel(\"MSE\");\n",
    "plt.title(\"MSE of the AutoEncoder with Respect to the Latent Space Size\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = pd.DataFrame(lossMatrix)\n",
    "lm.to_csv(\"lossmatrix4lisaN400.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"lossmatrix4lisa.csv\").to_numpy()\n",
    "print(temp.shape)\n",
    "avgMSE = np.average(lossMatrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hidden, avgMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lazy_train(epochs=60, hidden = 32):\n",
    "    ep = epochs\n",
    "    hidden = [hidden,] #range(1,32)\n",
    "    lossMatrix = []\n",
    "    lazy_weights = []\n",
    "    dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS, num_hidden_units=32)\n",
    "    dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "\n",
    "    #for i in tqdm(range(ep)):\n",
    "    for i in range(ep):\n",
    "        dense.fit(beanIntensities, beanIntensities, validation_data=(experiment1, experiment1), epochs=1,  verbose=0)\n",
    "        lazy_weights.append(dense.get_weights())\n",
    "        test = dense(testCandidate) #, verbose = 0)\n",
    "        loss = ignore_noParent_MSE(testCandidate, test)\n",
    "        lossMatrix.append(loss)\n",
    "        \n",
    "    lossMatrix = np.array(lossMatrix)\n",
    "    lazy_weights = np.array(lazy_weights)\n",
    "\n",
    "    return lossMatrix, lazy_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://proceedings.mlr.press/v162/rachwan22a/rachwan22a.pdf Winning the Lottery Ticket Ahead of Time:\n",
    "def lazyKernelRegime(w, parent_idx=parent_idx):\n",
    "\n",
    "    firstLayer = []\n",
    "    for i in range(len(w)):\n",
    "        firstLayer.append(w[i][0][parent_idx])\n",
    "\n",
    "    fL = np.array(firstLayer)    \n",
    "    d0 = np.square(fL[1] - fL[0])\n",
    "    kernelChange = []\n",
    "    for i in range(1,len(fL)):\n",
    "        dt = np.square(fL[i] - fL[0])\n",
    "        dt_minus1 = np.square(fL[i-1] - fL[0])   \n",
    "        d = np.abs(dt - dt_minus1)/d0                        #eq 11 from the paper\n",
    "        kernelChange.append(d)\n",
    "    \n",
    "    kernelChange = np.moveaxis(kernelChange, 0, 2)\n",
    "    # plt.plot(kernelChange[0][0]);\n",
    "    # plt.title(\"$|\\Delta W|$ vs Epoch\")\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"$|\\Delta W|$\")\n",
    "\n",
    "    return np.array(  kernelChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distrib(change, t = .05, raw = False): #raw means return the unshaped indicies. \n",
    "    stop = []\n",
    "\n",
    "    for parent in range(len(change)):\n",
    "        for child in range(len(change[0])):\n",
    "            try:\n",
    "                stop.append(np.min(np.argwhere(change[parent][child] < t)))\n",
    "            except ValueError:\n",
    "                stop.append(len(change))\n",
    "                # print(parent, child)\n",
    "                # plt.plot(change[parent][child])\n",
    "                # assert(False)\n",
    "    # print(change.shape)\n",
    "    # assert(False)\n",
    "    \n",
    "    stop = np.array(stop).flatten()\n",
    "    # print(stop.shape)\n",
    "    var = np.std(stop)\n",
    "    # print(var)\n",
    "    # assert(False)\n",
    "    mean = np.average(stop)\n",
    "    top_parents = np.argwhere(stop > mean + 2*var) #get the parents which take more than 2 stds to stop training\n",
    "    top_parent_child = []\n",
    "    for tp in top_parents:\n",
    "        top_parent_child.append(np.unravel_index(tp, shape = (NUM_PARENTS, NUM_TARGETS)))\n",
    "\n",
    "    if raw == False:\n",
    "        plt.hist(stop)\n",
    "        plt.xlabel(\"Epoch where regulator-target-weight began changing by at most \"+ str(t));\n",
    "        plt.ylabel(\"Number of parent-child-weights\");\n",
    "        plt.title(\"Histogram of weight stops\");\n",
    "        print(\"average stop: \", mean);\n",
    "\n",
    "    if raw == True:\n",
    "        return np.array(top_parents)\n",
    "\n",
    "    return np.array(top_parent_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazyKernels(N=100):\n",
    "\n",
    "    candidates = []\n",
    "    final_w = []\n",
    "    for i in tqdm(range(N)):\n",
    "        lm, lazy_weights = do_lazy_train(epochs=80)\n",
    "        change = lazyKernelRegime(lazy_weights)\n",
    "        top_pr = np.squeeze(compute_distrib(change, t = 0.05, raw=True))\n",
    "        candidates.append(top_pr)\n",
    "        final_w.append(lazy_weights[-1])\n",
    "    \n",
    "    final_w = np.array(final_w)\n",
    "    firstLayer = []\n",
    "    for i in range(len(final_w)):\n",
    "        firstLayer.append(np.abs(final_w[i][0][parent_idx]))\n",
    "    fw = np.array(firstLayer)\n",
    "    #print(fw.shape)\n",
    "    fw_avg = np.average(fw, axis = 0)\n",
    "    #print(fw_avg.shape)\n",
    "    \n",
    "    candidates = np.hstack(candidates)\n",
    "    candidates = candidates.reshape(candidates.size)\n",
    "    #print(candidates.shape)\n",
    "\n",
    "    plt.hist(candidates, bins=np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    plt.title(\"Parent-Child Regulator Histogram\")\n",
    "    plt.xlabel(\"Parent-Child Weight\")\n",
    "    plt.ylabel(\"Num-Times parent-child relationship trained for top 5% of time\")\n",
    "    return candidates, fw_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can, mag = lazyKernels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf, bins = np.histogram(can, np.arange(NUM_PARENTS*NUM_TARGETS))\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitudes After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS*NUM_TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(5144, shape = (NUM_PARENTS, NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_reg_targets(can, mag):\n",
    "    \n",
    "    pdf, bins = np.histogram(can, np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    pdf2d = np.zeros(shape = (NUM_PARENTS,NUM_TARGETS))\n",
    "\n",
    "    for i in bins:\n",
    "      idx2d = np.unravel_index(i, shape = (NUM_PARENTS, NUM_TARGETS))\n",
    "      try:\n",
    "        pdf2d[idx2d] = pdf[i]\n",
    "      except IndexError:\n",
    "        print(i, idx2d, len(pdf), len(bins))\n",
    "    \n",
    "\n",
    "    importance = np.multiply(pdf2d, mag)\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = get_top_reg_targets(can, mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top[12][122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(top, cmap = 'inferno', vmin = 0, vmax = np.max(top));\n",
    "plt.colorbar(u ,fraction=0.0086, pad=0.02);\n",
    "plt.title(\"The Most Important Regulator-Target Combinations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topID = np.array(np.unravel_index(np.argsort(top, axis=None), top.shape))\n",
    "topID = np.flip(topID, axis=1)\n",
    "topID[0] = parent_idx[topID[0]]\n",
    "topID = topID.T\n",
    "topID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topR_T = pd.DataFrame(topID)\n",
    "# topR_T.to_csv(\"Top_reg_target_decendingOrder_firstColIsRegulator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.zeros(21)\n",
    "for i in range(21):\n",
    "    best[i] = np.sum(top[i])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,22, dtype=int)\n",
    "plt.plot(x, best); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.flip(np.argsort(best))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(can).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Petal Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal = pd.read_excel(data_path_petal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train = petal[petal[\"Line\"] == \"WT\"]\n",
    "petal_train = petal_train.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_train.head(12)\n",
    "petal_train = petal_train.groupby(['Plate']).mean()\n",
    "petal_train.head()\n",
    "petal_train = petal_train.to_numpy()\n",
    "print(petal_train.shape)\n",
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(petal_train)\n",
    "petal_train = scaler1.transform(petal_train)\n",
    "mm = MinMaxScaler()\n",
    "mm.fit(petal_train)\n",
    "petal_train = mm.transform(petal_train)\n",
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test = petal[petal[\"Line\"] != \"WT\"]\n",
    "petal_test = petal_test.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_test = petal_test.groupby(['Plate']).mean()\n",
    "petal_test.head()\n",
    "petal_test = petal_test.to_numpy()\n",
    "print(petal_test.shape)\n",
    "petal_test = scaler1.transform(petal_test)\n",
    "petal_test = mm.transform(petal_test)\n",
    "petal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1.shape\n",
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densePredictor = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS, 22)\n",
    "densePredictor.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "densePredictor.fit(beanIntensities, beanIntensities,validation_data=(experiment1, experiment1),  epochs=100,  verbose=1)\n",
    "test = densePredictor(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgm = superParent\n",
    "time_steps = 6\n",
    "num_kinase_regulators = NUM_TARGETS\n",
    "num_hidden_units = 22\n",
    "\n",
    "inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "x = DenseEncoderLinear2(rgm, regulator_gene_matrix, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "enc = denseencoder2(x, inp, num_hidden_units)\n",
    "denseP = tf.keras.Model(inputs=inp, outputs=enc)\n",
    "#set the weights of the encoder to the weights of auto encoder\n",
    "dw = densePredictor.get_weights()\n",
    "enc_w = dw[0:5]\n",
    "denseP.set_weights(enc_w)\n",
    "#add a dense layer  because we are ouputing 1 number\n",
    "l = Dense(32, activation = 'swish', use_bias=True, kernel_regularizer='l1_l2')(denseP.layers[-1].output)\n",
    "l = Dense(1, activation = 'linear', use_bias = True)(l)\n",
    "denseP = tf.keras.Model(denseP.inputs, l)\n",
    "#denseP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = np.concatenate([experiment1, experiment1, experiment1, experiment1])\n",
    "bp.shape\n",
    "#bigexperiment1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = petal_train[0]\n",
    "b = petal_train[1]\n",
    "c = petal_train[2]\n",
    "d = petal_train[3]\n",
    "\n",
    "petal_train1 = np.array([a,a,a,a, b,b,b,b, c,c,c,c, d,d,d,d]) #does this make sense? we are training network to predict .5\n",
    "petal_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "denseP.fit(experiment1, petal_train, epochs=500, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP(experiment1) #experiment1 is part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(denseP(testCandidate)) #model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test #true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.evaluate(testCandidate, petal_test) #eval gave 1.3999 before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test set and the synthetic dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestSet(test_path):\n",
    "    testFiles = []\n",
    "    for np_name in glob(os.path.join(data_path_testSet,'*.np[yz]')):\n",
    "        k = np.load(os.path.join(data_path_testSet,np_name))\n",
    "        testFiles.append(k)\n",
    "#         print(np_name)\n",
    "#         print(k.shape)\n",
    "    return np.array(testFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PKjzdFCMwFg"
   },
   "outputs": [],
   "source": [
    "def read_files(data_path):\n",
    "\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        try:\n",
    "            #print('file name:',file)\n",
    "            #print('value of i:',i)\n",
    "            genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "            #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "            replicates.append(np.array(genes_intensities_data_matrix.values, dtype = float))\n",
    "            # i+=1\n",
    "        except PermissionError:\n",
    "            print(\"Not a CSV: \", os.path.join(data_path , file))\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    rep = np.array(replicates).astype(np.float32)\n",
    "    \n",
    "    return rep, rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCwo4LwlO_FF"
   },
   "outputs": [],
   "source": [
    "beanIntensities, regulator_gene_matrix= read_files(data_path_syn)\n",
    "matrix = regulator_gene_matrix\n",
    "replicates = beanIntensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros(shape = (3,6,8))\n",
    "id = np.unravel_index(3*6*8 - 1, shape = d.shape)\n",
    "d[id] = 1\n",
    "plt.imshow(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0][ : , parentIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)\n",
    "print(testCandidate.shape)\n",
    "syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(change[0][22])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"change in weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossMatrix)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change.shape, lossMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diff(change[0][22])\n",
    "plt.plot(d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change[0][0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_tresh(change, stop = 0.05):\n",
    "#     diffs = []\n",
    "#     for parent in range(len(change)):\n",
    "#         for child in range(len(change[0])):\n",
    "#             diffs.append(np.diff(change[parent][child]))\n",
    "#     inflection = []\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         for d in diffs:\n",
    "#             print(np.argwhere(np.abs(d) < stop))\n",
    "#             inflection.append(np.min(np.argwhere(np.abs(d) < stop))) #return where the second derivative is first 0. \n",
    "\n",
    "#     except ValueError:\n",
    "#         print(\"Stop value \", stop, \" is too high, trying stop = \", stop + 0.05)\n",
    "#         # s = stop + 0.05\n",
    "#         # return compute_tresh(change, stop = s)\n",
    "        \n",
    "\n",
    "#     return np.average(inflection)\n",
    "        \n",
    "# d = compute_tresh(change)\n",
    "# d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"mse3.npy\", avgMSE) #mse2/3 is with -1 fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sciencedirect.com/science/article/pii/S0925231220314570?casa_token=lcEJANqO0JwAAAAA:uL3DGUZctPUZz_sPz1K1i2klMtb83TyKnc9CI3_N-uSOaM7VHL8GhM0jCGYfo25NmpDQQ9Cvlw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rshp = Flatten()(looseParent.layers[-1].output)\n",
    "\n",
    "modelTemp = tf.keras.Model(inputs=looseParent.input, outputs = [rshp])\n",
    "modelTemp.summary()\n",
    "type(modelTemp)\n",
    "explainer = shap.DeepExplainer(modelTemp, syntheticDataTrain)\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough #this solves the \"shap_ADDV2\" problem but another one will appear\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"FusedBatchNormV3\"] = shap.explainers._deep.deep_tf.passthrough #this solves the next problem which allows you to run the DeepExplainer.\n",
    "\n",
    "shap_values = explainer.shap_values(testCandidate[0:1])\n",
    "def f(x):\n",
    "    return modelTemp.predict(x)\n",
    "\n",
    "print(f(testCandidate))\n",
    "explainer = shap.KernelExplainer(f , testCandidate[0:1], link=\"logit\") #svm.predict_proba, X_train, link=\"logit\")\n",
    "shap_values = explainer.shap_values(testCandidate[0:1], nsamples=100)\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import json\n",
    "import shap\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "X,y = shap.datasets.imagenet50()\n",
    "to_explain = X[[39,41]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(enc_dec_Synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [5,6]\n",
    "u = tf.concat([a,b], axis = 0)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newConnections = superParent - regulator_gene_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(newConnections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nC = []\n",
    "# for i in range(len(newConnections[0])):\n",
    "#     for j in range(len(newConnections[1])):\n",
    "#         if newConnections[i][j] > 0:\n",
    "#             nC.append([i,j])\n",
    "# nC = np.array(nC)\n",
    "# nC = pd.DataFrame(nC)\n",
    "# nC.to_csv(\"new_connections_in_superParents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code for testing loss function\n",
    "# print(outSyn.shape)\n",
    "# print(testCandidate.shape)\n",
    "# syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "# syntheticLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Auto Encoder\n",
    "Autoencoder has not been trained on synthetic version of experiement 1. We test on the original experiment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrRuJ_bsrHll"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAOZEEvRRVU4"
   },
   "outputs": [],
   "source": [
    "# enc_dec_Synthetic.compile(optimizer='adam',loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticDataTrain = beanIntensities[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1649273035573,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "EuNDZx5Ov34I",
    "outputId": "74597aa1-c71c-4491-b8c9-4fefed309325"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.fit(syntheticDataTrain,syntheticDataTrain,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = enc_dec_Synthetic(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = enc_dec_Synthetic.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w[0], cmap = \"hot\", vmin=0,vmax=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBzDmjqJViEw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we do not need to use this function for the testset\n",
    "def getCSVs(data_path_head):\n",
    "    PATH = data_path_head\n",
    "    EXT = \"*.csv\"\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "    actual = []\n",
    "    for p in all_csv_files:\n",
    "        actual.append(pd.read_csv(p, index_col = 0).to_numpy())\n",
    "    return np.array(actual)\n",
    "    \n",
    "experiment1 = getCSVs(data_path_og_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate = test.numpy().astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outSyn = enc_dec_Synthetic.predict(testCandidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymagn(A, B):\n",
    "    mse = (np.square(A - B)).mean(axis=None)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticLoss = ignore_noParent_MSE(testCandidate, outSyn )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(outSyn-testCandidate).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install keras-visualizer\n",
    "#!pip install pydot\n",
    "#data_path_og_exp1 = data_path_testSet \n",
    "# !pip install pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = loadTestSet(data_path_testSet)\n",
    "# testCandidate = test.astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPRV0OAMpKPH"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS) #we can just change the time steps to something higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1649273028683,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "D6aPT5_cpKK0",
    "outputId": "c261d824-0f38-4eee-b139-f03a80f093e1"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated dataset Auto Encoder\n",
    "Once again, we do not train on any version of exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filesV2(data_path):\n",
    "    '''\n",
    "    *Changed*\n",
    "    currently hardcoded for only one file. \n",
    "    change code a bit for reading multiple files.\n",
    "    '''\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        #print('file name:',file)\n",
    "        #print('value of i:',i)\n",
    "        genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "        #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "        replicates.append(genes_intensities_data_matrix.values)\n",
    "        # i+=1\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    \n",
    "    return np.asarray(replicates), rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes, _ = read_filesV2(data_path_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(interpolated_genes[2]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = []\n",
    "for k in range(len(interpolated_genes)):\n",
    "    #print(k)\n",
    "    if k == 2 or k == 3 or k == 4:\n",
    "        inter.append(np.reshape(interpolated_genes[k], (4,6,NUM_TARGETS)))\n",
    "    else: \n",
    "        inter.append(np.reshape(interpolated_genes[k], (5,6,NUM_TARGETS)))\n",
    "inter = np.vstack(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beanIntensities[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter = model(regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS) \n",
    "enc_dec_inter.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter.fit(inter, inter,epochs=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter = enc_dec_inter.predict(testCandidate)\n",
    "interpolationLoss = ignore_noParent_MSE(testCandidate, outInter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolationLoss #used to be 3.84 on broke ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = enc_dec_inter.history\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons between various outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(np.reshape((np.abs(outSyn)), (24,NUM_TARGETS)), cmap = \"hot\", vmin=0,vmax=1.0 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(outSyn[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(testCandidate[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape((np.abs(outSyn-outInter)), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 );\n",
    "plt.title(\"Difference Between the Outputs of Both Autoencoders\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"DiffBetweenOut.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outInter-experiment1), (24,NUM_TARGETS)) , cmap = \"hot\") #, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Interpolated Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30)\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"InterDiffImage.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outSyn-experiment1), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"SynDiffImage.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx = parentIndex.numpy()\n",
    "#print(parent_idx)\n",
    "oSyn = (np.reshape((outSyn), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "oSyn = oSyn.T\n",
    "oSyn.shape\n",
    "\n",
    "exp1_col = (np.reshape((experiment1), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "exp1_col = exp1_col.T\n",
    "print(exp1_col.shape)\n",
    "\n",
    "u = plt.imshow(np.abs(oSyn - exp1_col), cmap = 'hot') #TODO use TF loss function instead of difference.\n",
    "ddff = oSyn-exp1_col\n",
    "plt.colorbar(u)\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data. Only parents.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(oSyn).head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(exp1_col).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ddff).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"interpolated_v2.npy\", inter) #the interpolated dataset\n",
    "# np.save(\"synthetic_v2.npy\", beanIntensities[1:]) # the synthetic dataset\n",
    "# np.save(\"synOut_v2.npy\", outSyn) #the output of the encoder trained on synthetic data with the input being exp1\n",
    "# np.save(\"interOut_v2.npy\", outInter) #the output of the encoder trained on interpolated data with the input being exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6J5QSynTTgZ+lFOhgOtAG",
   "collapsed_sections": [],
   "name": "cnn-third.ipynb",
   "provenance": [
    {
     "file_id": "1mmRxO5jnBc5CdzxXj8sMtPpG0BQ0ulSs",
     "timestamp": 1648921397876
    },
    {
     "file_id": "10758zFj2UnTnwpQaSFIr5r9MqZWdiMsf",
     "timestamp": 1648674775255
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
