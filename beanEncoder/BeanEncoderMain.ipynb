{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import synapseclient \n",
    "#  import synapseutils \n",
    " \n",
    "#  syn = synapseclient.Synapse() \n",
    "#  syn.login('finamintoastcrunch','1Hjldria!') \n",
    "#  files = synapseutils.syncFromSynapse(syn, ' syn2825306 ') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "qTo_HuQkGgAq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_visualizer import visualizer \n",
    "\n",
    "from tensorflow.keras.layers import*\n",
    "import shap\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Conv1D\n",
    "# from keras.layers import Conv1DTranspose\n",
    "# from keras.layers import Flatten, Reshape\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS = 7\n",
    "NUM_TARGETS = 372\n",
    "NUM_TIME_STEPS = 11\n",
    "NUM_REPLICATES = 3 #used to be 4, but replicate 4 is trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "gXpISOijEYqQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# matrix_path = \"regulator-gene-matrix.csv\"\n",
    "# data_path_syn = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\synData\"\n",
    "# data_path_inter =  r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\interpolatedOnly\"\n",
    "# data_path_og_exp1 = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\datasets\\exp1\"\n",
    "# data_path_testSet = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\testSetFixed\"\n",
    "# data_path_petal = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\petal_len.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_RGM = r'Regulations_Control_Altona.csv'\n",
    "dirty_regulations = r'FullTable_Control.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372, 44)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtyRGM = pd.read_csv(dirty_RGM,  index_col = 0, )#on_bad_lines='skip')\n",
    "dirtyReg = pd.read_csv(dirty_regulations,  index_col = 0,)# on_bad_lines='skip')\n",
    "dirtyReg = dirtyReg.select_dtypes(include=np.number)\n",
    "dirtyReg = dirtyReg.to_numpy()\n",
    "dirtyReg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 11, 372)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Fix this. it puts nans where it should not. \n",
    "\n",
    "def fix_dataset(dirtyR):\n",
    "    dataset = np.zeros(shape=(NUM_REPLICATES,NUM_TARGETS, NUM_TIME_STEPS))\n",
    "    ret = np.zeros(shape=(NUM_REPLICATES,NUM_TIME_STEPS, NUM_TARGETS))\n",
    "\n",
    "    for i in range(0,NUM_REPLICATES*NUM_TIME_STEPS, 4):\n",
    "        for j in range(0, NUM_REPLICATES):\n",
    "            dataset[j][:,i//NUM_REPLICATES] = dirtyR[:,(i+j)]\n",
    "    \n",
    "    dataset[dataset==0] = np.nan\n",
    "\n",
    "\n",
    "    for i in range(NUM_REPLICATES):\n",
    "        regScaled = MinMaxScaler().fit_transform(dataset[i].flatten().reshape((-1,1)))\n",
    "        regScaled = regScaled.reshape((NUM_TARGETS, NUM_TIME_STEPS))\n",
    "        regScaled = np.nan_to_num(regScaled, nan= -1.0)\n",
    "        ret[i] = regScaled.T\n",
    "    return ret\n",
    "\n",
    "dataset = fix_dataset(dirtyR=dirtyReg)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11, 372) (1, 11, 372)\n"
     ]
    }
   ],
   "source": [
    "beanIntensities = dataset[0:2]\n",
    "validation = np.array([dataset[2]])\n",
    "print(beanIntensities.shape, validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the third replicate is trash. we will not use it. \n",
    "# df(dataset[3]).head(11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(372, 372)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regulator_gene_matrix = np.load(\"soyBeanRGM.npy\")\n",
    "regulator_gene_matrix = regulator_gene_matrix.astype('float32')\n",
    "regulator_gene_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent Matrix + Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of parent index (7,)\n"
     ]
    }
   ],
   "source": [
    "superParent = regulator_gene_matrix.copy() #init the super parent with the ordinary RGM, and do forward passes with super parent\n",
    "#print(superParent.shape)\n",
    "\n",
    "ones = np.ones((NUM_TARGETS))\n",
    "parentIndex = []\n",
    "not_parentIndex = []\n",
    "for i in range(len(regulator_gene_matrix)):\n",
    "    if (np.isin(regulator_gene_matrix[i], [1])).any():\n",
    "        #print(i)\n",
    "        superParent[i] = ones \n",
    "        parentIndex.append(i)\n",
    "    else:\n",
    "        not_parentIndex.append(i)\n",
    "\n",
    "parentIndex = np.array(parentIndex)\n",
    "parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "parent_idx = parentIndex.numpy()\n",
    "not_parentIndex = np.array(not_parentIndex)\n",
    "not_parentIndex = tf.convert_to_tensor(not_parentIndex)\n",
    "print(\"shape of parent index\", parentIndex.shape)\n",
    "\n",
    "def ignore_noParent_MSE_old(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "    y_true_pruned = tf.gather(y_true,parentIndex, axis =2) \n",
    "    #print(y_true_pruned.shape\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis =2)   \n",
    "    return l(y_true_pruned, y_pred_pruned)\n",
    "\n",
    "#this will not work if the entire dataset is -1 (degenerate)\n",
    "def ignore_noParent_MSE(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "   # print(y_true.shape) #(None, 44, 372)\n",
    "\n",
    "    #get the parents and flatten them\n",
    "    y_true_pruned = tf.gather(y_true, parentIndex, axis = 2) #axis 2 because batch, time, gene\n",
    "    y_true_pruned = tf.reshape(y_true_pruned, shape=([tf.size(y_true_pruned)] ) )\n",
    "\n",
    "   # print(y_true_pruned.shape)\n",
    "   # print(\"tf size\", tf.size(y_true_pruned))\n",
    "\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis = 2) \n",
    "    y_pred_pruned = tf.reshape(y_pred_pruned, shape=([tf.size(y_pred_pruned)]) )\n",
    "\n",
    "    #get the index of the parents which are not -1\n",
    "    y_true_posID = tf.where(y_true_pruned >= 0) #gets args\n",
    "    y_true_posID = tf.squeeze(y_true_posID)\n",
    "    #get the idx of all the -1s \n",
    "    y_true_negID = tf.where(y_true_pruned < 0) \n",
    "    y_true_negID = tf.squeeze(y_true_negID)\n",
    "\n",
    "    #get all the -1s in the parents \n",
    "    y_true_neg = tf.gather(y_true_pruned, y_true_negID) #get all the -1s in y_true\n",
    "    y_pred_neg = tf.gather(y_pred_pruned, y_true_negID) #get the corresponding values for y_pred\n",
    "\n",
    "    #get the indexes where pred should be -1 but is not. get the corresponding index for ytrue\n",
    "    y_shouldBeNegButIsntID = tf.where(y_pred_neg >= 0)  \n",
    "    y_shouldBeNegButIsntID = tf.squeeze(y_shouldBeNegButIsntID) #get the idx which should be -1 for prediction but are not\n",
    "    y_true_wrong = tf.gather(y_true_pruned, y_shouldBeNegButIsntID) #get the same corresponding values from ytrue\n",
    "    y_shouldBeNegButIsnt = tf.gather(y_pred_pruned, y_shouldBeNegButIsntID) #this has all the wrongly predicted values which should be -1 but are not\n",
    "\n",
    "    y_true_pos = tf.gather(y_true_pruned, y_true_posID)\n",
    "    y_pred_pos = tf.gather(y_pred_pruned, y_true_posID)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0. \n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "   # print(y_pred_pos.shape, y_shouldBeNegButIsnt.shape)\n",
    "    #print(\"y_pred\", (y_pred_pos), \"y_true\", (y_shouldBeNegButIsnt))\n",
    "    y_pred_total = tf.concat([y_pred_pos, y_shouldBeNegButIsnt], axis = 0) #concatenate for total mse\n",
    "    y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\n",
    "\n",
    "    return l(y_true_total, y_pred_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117, 131, 164, 225, 259, 334, 350])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(superParent);\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(regulator_gene_matrix);\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "1yhzyikoFncq"
   },
   "outputs": [],
   "source": [
    "class EncoderLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, input_dim=32, units=32):\n",
    "        super(EncoderLinear, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.rgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "4uchxU-bmBDe"
   },
   "outputs": [],
   "source": [
    "class DecoderLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, input_dim=32, units=32):\n",
    "        super(DecoderLinear, self).__init__()\n",
    "        self.rgm = rgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        #return tf.matmul(inputs, self.w)\n",
    "        # v = tf.zeros_like(X)\n",
    "        # u = tf.ones_like(X)\n",
    "        # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "        return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "PQVz4BCpmNMd"
   },
   "outputs": [],
   "source": [
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(490, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association)\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(1024)(X)#Dense(128)(X)\n",
    "    de_dense = Reshape((1, 1024))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def model(rgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 32):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinaryAE = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS)\n",
    "ordinaryAE.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.2277 - val_loss: 0.2534\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2229 - val_loss: 0.2402\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2101 - val_loss: 0.2127\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1882 - val_loss: 0.1676\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1530 - val_loss: 0.1164\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1157 - val_loss: 0.1024\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1118 - val_loss: 0.0814\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0926 - val_loss: 0.0502\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0594 - val_loss: 0.0389\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0441 - val_loss: 0.0397\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0417 - val_loss: 0.0399\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0399 - val_loss: 0.0407\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0388 - val_loss: 0.0431\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0411 - val_loss: 0.0408\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0391 - val_loss: 0.0326\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0325 - val_loss: 0.0259\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0279 - val_loss: 0.0222\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0281 - val_loss: 0.0176\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0246 - val_loss: 0.0121\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0216 - val_loss: 0.0086\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0204 - val_loss: 0.0075\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0193 - val_loss: 0.0066\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0177 - val_loss: 0.0058\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0175 - val_loss: 0.0062\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0139 - val_loss: 0.0069\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0132 - val_loss: 0.0065\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0120 - val_loss: 0.0054\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0139 - val_loss: 0.0049\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0095 - val_loss: 0.0054\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0093 - val_loss: 0.0052\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0079 - val_loss: 0.0053\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0100 - val_loss: 0.0059\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0092 - val_loss: 0.0058\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0079 - val_loss: 0.0060\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0063 - val_loss: 0.0057\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0065 - val_loss: 0.0053\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0059 - val_loss: 0.0050\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0055 - val_loss: 0.0055\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0047 - val_loss: 0.0044\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0049 - val_loss: 0.0037\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0047 - val_loss: 0.0037\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0046 - val_loss: 0.0038\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0046 - val_loss: 0.0049\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0040 - val_loss: 0.0066\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0042 - val_loss: 0.0075\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0044 - val_loss: 0.0067\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0040 - val_loss: 0.0054\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0037 - val_loss: 0.0049\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0043 - val_loss: 0.0049\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0048 - val_loss: 0.0050\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0043 - val_loss: 0.0057\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0035 - val_loss: 0.0064\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0035 - val_loss: 0.0045\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0052 - val_loss: 0.0043\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0037 - val_loss: 0.0053\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0042 - val_loss: 0.0051\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0039 - val_loss: 0.0042\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0037 - val_loss: 0.0055\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0042 - val_loss: 0.0048\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0039 - val_loss: 0.0034\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0034 - val_loss: 0.0032\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0028 - val_loss: 0.0052\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0044 - val_loss: 0.0048\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0027 - val_loss: 0.0038\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0024 - val_loss: 0.0033\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0024 - val_loss: 0.0032\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0023 - val_loss: 0.0033\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0029 - val_loss: 0.0032\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0016 - val_loss: 0.0030\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0021 - val_loss: 0.0026\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0019 - val_loss: 0.0026\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0011 - val_loss: 0.0037\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0010 - val_loss: 0.0025\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.5535e-04 - val_loss: 0.0041\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0012 - val_loss: 0.0039\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.5706e-04 - val_loss: 0.0026\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.8720e-04 - val_loss: 0.0023\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.3182e-04 - val_loss: 0.0025\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.7077e-04 - val_loss: 0.0030\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.4153e-04 - val_loss: 0.0028\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6340e-04 - val_loss: 0.0023\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.0195e-04 - val_loss: 0.0021\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.7295e-04 - val_loss: 0.0020\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.4539e-04 - val_loss: 0.0020\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.9156e-04 - val_loss: 0.0023\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.7040e-04 - val_loss: 0.0022\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.8674e-04 - val_loss: 0.0020\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1402e-04 - val_loss: 0.0020\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.2600e-04 - val_loss: 0.0020\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.2879e-04 - val_loss: 0.0021\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2669e-04 - val_loss: 0.0021\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3031e-04 - val_loss: 0.0022\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2929e-04 - val_loss: 0.0022\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1241e-04 - val_loss: 0.0022\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7776e-04 - val_loss: 0.0023\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.8895e-04 - val_loss: 0.0024\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2181e-04 - val_loss: 0.0024\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.8847e-04 - val_loss: 0.0023\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3575e-04 - val_loss: 0.0022\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2968e-04 - val_loss: 0.0021\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.7545e-04 - val_loss: 0.0021\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4519e-04 - val_loss: 0.0021\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0506e-04 - val_loss: 0.0022\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1757e-04 - val_loss: 0.0021\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.9695e-05 - val_loss: 0.0020\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.8521e-05 - val_loss: 0.0020\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.4136e-05 - val_loss: 0.0020\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1047e-04 - val_loss: 0.0021\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.3550e-05 - val_loss: 0.0021\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.8457e-05 - val_loss: 0.0021\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.4042e-05 - val_loss: 0.0020\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.4167e-05 - val_loss: 0.0020\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.0229e-05 - val_loss: 0.0020\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.2925e-05 - val_loss: 0.0020\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.2033e-05 - val_loss: 0.0020\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3000e-05 - val_loss: 0.0020\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.2816e-05 - val_loss: 0.0020\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.4240e-05 - val_loss: 0.0019\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4451e-05 - val_loss: 0.0020\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9791e-05 - val_loss: 0.0020\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4690e-05 - val_loss: 0.0020\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7387e-05 - val_loss: 0.0019\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2065e-05 - val_loss: 0.0019\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5752e-05 - val_loss: 0.0019\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1791e-05 - val_loss: 0.0019\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9907e-05 - val_loss: 0.0019\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9452e-05 - val_loss: 0.0019\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7073e-05 - val_loss: 0.0019\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4975e-05 - val_loss: 0.0019\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2646e-05 - val_loss: 0.0019\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2404e-05 - val_loss: 0.0019\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2045e-05 - val_loss: 0.0019\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1670e-05 - val_loss: 0.0019\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.2008e-06 - val_loss: 0.0019\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.8127e-06 - val_loss: 0.0019\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.0216e-06 - val_loss: 0.0019\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.5284e-06 - val_loss: 0.0019\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.6940e-06 - val_loss: 0.0019\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.7194e-06 - val_loss: 0.0019\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.0965e-06 - val_loss: 0.0019\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9344e-06 - val_loss: 0.0019\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.4589e-06 - val_loss: 0.0019\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.7849e-06 - val_loss: 0.0019\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2027e-06 - val_loss: 0.0019\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.6623e-06 - val_loss: 0.0019\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.2220e-06 - val_loss: 0.0019\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0233e-06 - val_loss: 0.0019\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8536e-06 - val_loss: 0.0019\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7791e-06 - val_loss: 0.0019\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.2902e-06 - val_loss: 0.0019\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3113e-06 - val_loss: 0.0019\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0922e-06 - val_loss: 0.0019\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.7222e-06 - val_loss: 0.0019\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6286e-06 - val_loss: 0.0019\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7093e-06 - val_loss: 0.0019\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.2990e-06 - val_loss: 0.0019\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3953e-06 - val_loss: 0.0019\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1052e-06 - val_loss: 0.0019\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2662e-06 - val_loss: 0.0019\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.3990e-07 - val_loss: 0.0019\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0322e-06 - val_loss: 0.0019\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.8652e-07 - val_loss: 0.0019\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.1162e-07 - val_loss: 0.0019\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.8824e-05 - val_loss: 0.0019\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.0570e-05 - val_loss: 0.0019\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4153e-05 - val_loss: 0.0019\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.0710e-06 - val_loss: 0.0019\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.9080e-06 - val_loss: 0.0019\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.6706e-06 - val_loss: 0.0019\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0710e-05 - val_loss: 0.0019\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3380e-05 - val_loss: 0.0019\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.6947e-07 - val_loss: 0.0019\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.7912e-05 - val_loss: 0.0019\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3062e-05 - val_loss: 0.0019\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.5149e-06 - val_loss: 0.0019\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.6901e-06 - val_loss: 0.0019\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2918e-05 - val_loss: 0.0019\n"
     ]
    }
   ],
   "source": [
    "o = ordinaryAE.fit(beanIntensities, beanIntensities, epochs=200, verbose = True, validation_data=(validation, validation))\n",
    "#print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(EncoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.OGrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "class DecoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DecoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        #return tf.matmul(inputs, self.w)\n",
    "        # v = tf.zeros_like(X)\n",
    "        # u = tf.ones_like(X)\n",
    "        # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "        return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        \n",
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(128)(X)\n",
    "    de_dense = Reshape((1, 128))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelSuperParent(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSuperParentSequential(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    m = tf.keras.Sequential()\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.2472 - val_loss: 0.2453\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2346 - val_loss: 0.2211\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2109 - val_loss: 0.1834\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1741 - val_loss: 0.1309\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1230 - val_loss: 0.0694\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0632 - val_loss: 0.0190\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0154 - val_loss: 0.0327\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0318 - val_loss: 0.0575\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0579 - val_loss: 0.0325\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0329 - val_loss: 0.0069\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0065 - val_loss: 0.0028\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0013 - val_loss: 0.0123\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0097 - val_loss: 0.0228\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0193 - val_loss: 0.0277\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0240 - val_loss: 0.0267\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0230 - val_loss: 0.0210\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0176 - val_loss: 0.0137\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0108 - val_loss: 0.0079\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0056 - val_loss: 0.0062\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0045 - val_loss: 0.0083\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0071 - val_loss: 0.0105\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0084 - val_loss: 0.0060\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0047 - val_loss: 0.0034\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0016 - val_loss: 0.0031\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.9371e-04 - val_loss: 0.0047\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0022 - val_loss: 0.0067\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0040 - val_loss: 0.0078\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0050 - val_loss: 0.0073\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0047 - val_loss: 0.0058\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0034 - val_loss: 0.0039\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0019 - val_loss: 0.0027\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0010 - val_loss: 0.0024\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0018 - val_loss: 0.0032\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.4791e-04 - val_loss: 0.0021\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.7058e-04 - val_loss: 0.0027\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.1267e-04 - val_loss: 0.0032\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0011 - val_loss: 0.0034\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0011 - val_loss: 0.0031\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.5875e-04 - val_loss: 0.0026\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.7009e-04 - val_loss: 0.0022\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.4067e-04 - val_loss: 0.0021\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.9606e-04 - val_loss: 0.0022\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.1217e-04 - val_loss: 0.0023\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.3746e-04 - val_loss: 0.0022\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.3049e-04 - val_loss: 0.0020\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.8965e-04 - val_loss: 0.0019\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1252e-04 - val_loss: 0.0020\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0647e-04 - val_loss: 0.0022\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.1512e-04 - val_loss: 0.0023\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.9972e-04 - val_loss: 0.0022\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.8592e-04 - val_loss: 0.0020\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8514e-04 - val_loss: 0.0018\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.9344e-05 - val_loss: 0.0017\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.5389e-05 - val_loss: 0.0017\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3027e-04 - val_loss: 0.0017\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7765e-04 - val_loss: 0.0017\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6115e-04 - val_loss: 0.0017\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.0271e-05 - val_loss: 0.0017\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.2443e-05 - val_loss: 0.0018\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.8175e-05 - val_loss: 0.0019\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.4038e-05 - val_loss: 0.0020\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.2341e-05 - val_loss: 0.0020\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.6618e-05 - val_loss: 0.0019\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.2897e-05 - val_loss: 0.0019\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.6401e-05 - val_loss: 0.0018\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.7672e-05 - val_loss: 0.0018\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.4933e-05 - val_loss: 0.0018\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.2440e-05 - val_loss: 0.0018\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.8435e-05 - val_loss: 0.0018\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6879e-05 - val_loss: 0.0018\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.8248e-06 - val_loss: 0.0018\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5672e-05 - val_loss: 0.0018\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.6873e-05 - val_loss: 0.0018\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.9277e-05 - val_loss: 0.0018\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.0390e-05 - val_loss: 0.0018\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.6637e-06 - val_loss: 0.0017\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.4463e-06 - val_loss: 0.0017\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2506e-05 - val_loss: 0.0017\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6677e-05 - val_loss: 0.0017\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3661e-05 - val_loss: 0.0017\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.4042e-06 - val_loss: 0.0018\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.9708e-06 - val_loss: 0.0018\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.4309e-06 - val_loss: 0.0018\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.3019e-06 - val_loss: 0.0018\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.2680e-06 - val_loss: 0.0018\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.8709e-06 - val_loss: 0.0018\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.4049e-06 - val_loss: 0.0018\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2136e-06 - val_loss: 0.0018\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.4892e-06 - val_loss: 0.0018\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.1814e-06 - val_loss: 0.0018\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.0099e-06 - val_loss: 0.0018\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9412e-06 - val_loss: 0.0018\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2704e-06 - val_loss: 0.0018\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.2390e-06 - val_loss: 0.0018\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.1206e-06 - val_loss: 0.0018\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.4289e-06 - val_loss: 0.0018\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1715e-06 - val_loss: 0.0017\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.5323e-07 - val_loss: 0.0017\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1172e-06 - val_loss: 0.0017\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8689e-06 - val_loss: 0.0017\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7299e-06 - val_loss: 0.0017\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.2480e-07 - val_loss: 0.0018\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.0901e-07 - val_loss: 0.0018\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.4989e-07 - val_loss: 0.0018\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.4262e-07 - val_loss: 0.0018\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.1293e-07 - val_loss: 0.0018\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.3423e-07 - val_loss: 0.0018\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4168e-07 - val_loss: 0.0018\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.4753e-07 - val_loss: 0.0018\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.2397e-07 - val_loss: 0.0018\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.5581e-07 - val_loss: 0.0018\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.6397e-07 - val_loss: 0.0018\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5797e-07 - val_loss: 0.0018\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6143e-07 - val_loss: 0.0018\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.0346e-07 - val_loss: 0.0018\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.5055e-07 - val_loss: 0.0018\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3659e-07 - val_loss: 0.0018\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.4218e-08 - val_loss: 0.0018\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1562e-07 - val_loss: 0.0018\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1071e-07 - val_loss: 0.0018\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9472e-07 - val_loss: 0.0018\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.8205e-08 - val_loss: 0.0018\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.4500e-08 - val_loss: 0.0018\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.8767e-08 - val_loss: 0.0018\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.7469e-08 - val_loss: 0.0018\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.3977e-08 - val_loss: 0.0018\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.9158e-08 - val_loss: 0.0018\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.4427e-08 - val_loss: 0.0018\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.2395e-08 - val_loss: 0.0018\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1171e-07 - val_loss: 0.0018\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.2177e-08 - val_loss: 0.0018\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.4485e-08 - val_loss: 0.0018\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7121e-08 - val_loss: 0.0018\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.4743e-08 - val_loss: 0.0018\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.1914e-08 - val_loss: 0.0018\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6758e-08 - val_loss: 0.0018\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.4562e-08 - val_loss: 0.0018\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4167e-08 - val_loss: 0.0018\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6548e-08 - val_loss: 0.0018\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.2728e-08 - val_loss: 0.0018\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1758e-08 - val_loss: 0.0018\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0244e-08 - val_loss: 0.0018\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0996e-08 - val_loss: 0.0018\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1033e-08 - val_loss: 0.0018\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.8443e-09 - val_loss: 0.0018\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.1676e-09 - val_loss: 0.0018\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.7041e-09 - val_loss: 0.0018\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.3075e-09 - val_loss: 0.0018\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.7854e-09 - val_loss: 0.0018\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.6274e-09 - val_loss: 0.0018\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.4835e-09 - val_loss: 0.0018\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.6268e-09 - val_loss: 0.0018\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.4548e-09 - val_loss: 0.0018\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.3437e-09 - val_loss: 0.0018\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.1913e-09 - val_loss: 0.0018\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1400e-09 - val_loss: 0.0018\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.0287e-09 - val_loss: 0.0018\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.8839e-09 - val_loss: 0.0018\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.7310e-09 - val_loss: 0.0018\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1406e-09 - val_loss: 0.0018\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9204e-09 - val_loss: 0.0018\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4215e-09 - val_loss: 0.0018\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.5620e-09 - val_loss: 0.0018\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0892e-09 - val_loss: 0.0018\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4120e-09 - val_loss: 0.0018\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.8115e-09 - val_loss: 0.0018\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2560e-09 - val_loss: 0.0018\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3554e-09 - val_loss: 0.0018\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7980e-09 - val_loss: 0.0018\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8238e-09 - val_loss: 0.0018\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0772e-09 - val_loss: 0.0018\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.9443e-10 - val_loss: 0.0018\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0574e-09 - val_loss: 0.0018\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.3606e-09 - val_loss: 0.0018\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.9118e-10 - val_loss: 0.0018\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.7553e-10 - val_loss: 0.0018\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.8577e-10 - val_loss: 0.0018\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.7376e-10 - val_loss: 0.0018\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.9179e-10 - val_loss: 0.0018\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.8930e-10 - val_loss: 0.0018\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.7347e-09 - val_loss: 0.0018\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7039e-09 - val_loss: 0.0018\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.4031e-10 - val_loss: 0.0018\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.3913e-10 - val_loss: 0.0018\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.0937e-10 - val_loss: 0.0018\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.4141e-10 - val_loss: 0.0018\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.7527e-09 - val_loss: 0.0018\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.2236e-10 - val_loss: 0.0018\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.3339e-10 - val_loss: 0.0018\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.0130e-10 - val_loss: 0.0018\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.3253e-10 - val_loss: 0.0018\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2656e-09 - val_loss: 0.0018\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1826e-09 - val_loss: 0.0018\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.4375e-10 - val_loss: 0.0018\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.7792e-10 - val_loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "looseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 21)\n",
    "looseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "o = looseParent.fit(beanIntensities[1:], beanIntensities[1:], epochs=200, verbose = True,  validation_data=(validation, validation))\n",
    "#print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNetAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a second copy of the layers which will be modified to be a denseNET auto encoder\n",
    "'''\n",
    "#TODO: fix call to map to -1?\n",
    "class DenseEncoderLinear2(tf.keras.layers.Layer): #TODO: Fix the decoder to -1\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DenseEncoderLinear2, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.oldrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "\n",
    "class DenseDecoderLinear2(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DenseDecoderLinear2, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.oldrgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        return tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) #used to have a transpose\n",
    "        #return tf.matmul(inputs, self.w)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "        \n",
    "\n",
    "def denseencoder2(parent_child_biological_association, inp, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(490, 3, activation = \"tanh\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    inp = Flatten()(inp)\n",
    "    #print(en_dense.shape, inp.shape)\n",
    "    d = Concatenate()([en_dense, inp]) #dense layer\n",
    "    o_d = Dense(1024, activation = 'tanh')(d) #added a layer\n",
    "    c = Concatenate()([o_d, d]) #TOTALY NEW LAYER\n",
    "    en_dense = Dense(128, activation = 'tanh')(c) #TOTALY NEW LAYER\n",
    "    \n",
    "    phenotype = Dense(num_hidden_units, activation=\"tanh\")(d)\n",
    "    return phenotype\n",
    "\n",
    "def densedecoder2(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(784, activation = 'tanh')(X)\n",
    "    de_dense = Dense(512, activation = 'tanh')(de_dense) #TOTALY NEW LAYER\n",
    "    de_dense = Dense(256, activation = 'tanh')(de_dense) #added a layer\n",
    "    de_dense = Reshape((1, 256))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"tanh\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelDense2(rgm, oldrgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = DenseEncoderLinear2(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    #x = EncoderLinear2(x)\n",
    "    enc = denseencoder2(x, inp, num_hidden_units)\n",
    "    dec = densedecoder2(enc, num_protein_gene, time_steps)\n",
    "    out = DenseDecoderLinear2(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "(None, 11, 372)\n",
      "(None,)\n",
      "tf size Tensor(\"ignore_noParent_MSE/Size_1:0\", shape=(), dtype=int32)\n",
      "<unknown> <unknown>\n",
      "(None, 11, 372)\n",
      "(None,)\n",
      "tf size Tensor(\"ignore_noParent_MSE/Size_1:0\", shape=(), dtype=int32)\n",
      "<unknown> <unknown>\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2058(None, 11, 372)\n",
      "(None,)\n",
      "tf size Tensor(\"ignore_noParent_MSE/Size_1:0\", shape=(), dtype=int32)\n",
      "<unknown> <unknown>\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2058 - val_loss: 0.2598\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.2166 - val_loss: 0.1240\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1073 - val_loss: 0.0397\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0487 - val_loss: 0.0104\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0295 - val_loss: 0.0206\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0227 - val_loss: 0.0468\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0292 - val_loss: 0.0342\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0178 - val_loss: 0.0114\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0097 - val_loss: 0.0079\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0167 - val_loss: 0.0118\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0180 - val_loss: 0.0170\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0150 - val_loss: 0.0139\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0095 - val_loss: 0.0112\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0092 - val_loss: 0.0155\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0127 - val_loss: 0.0200\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0120"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'ignore_noParent_MSE/cond/concat_1' defined at (most recent call last):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\1720463144.py\", line 3, in <cell line: 3>\n      dense.fit(beanIntensities, beanIntensities, epochs=200,  verbose=True, validation_data=(validation, validation))\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1445, in fit\n      val_logs = self.evaluate(\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1756, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1557, in test_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1546, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1535, in run_step\n      outputs = model.test_step(data)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1501, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 139, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 243, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\1752933174.py\", line 64, in ignore_noParent_MSE\n      if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0.\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\1752933174.py\", line 70, in ignore_noParent_MSE\n      y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\nNode: 'ignore_noParent_MSE/cond/concat_1'\nConcatOp : Ranks of all input tensors should match: shape[0] = [63] vs. shape[1] = []\n\t [[{{node ignore_noParent_MSE/cond/concat_1}}]] [Op:__inference_test_function_539719]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\beanEncoder\\BeanEncoderMain.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dense \u001b[39m=\u001b[39m modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, \u001b[39m32\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dense\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39mignore_noParent_MSE)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dense\u001b[39m.\u001b[39;49mfit(beanIntensities, beanIntensities, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,  verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, validation_data\u001b[39m=\u001b[39;49m(validation, validation))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'ignore_noParent_MSE/cond/concat_1' defined at (most recent call last):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\1720463144.py\", line 3, in <cell line: 3>\n      dense.fit(beanIntensities, beanIntensities, epochs=200,  verbose=True, validation_data=(validation, validation))\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1445, in fit\n      val_logs = self.evaluate(\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1756, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1557, in test_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1546, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1535, in run_step\n      outputs = model.test_step(data)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1501, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 139, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 243, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\1752933174.py\", line 64, in ignore_noParent_MSE\n      if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0.\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\1752933174.py\", line 70, in ignore_noParent_MSE\n      y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\nNode: 'ignore_noParent_MSE/cond/concat_1'\nConcatOp : Ranks of all input tensors should match: shape[0] = [63] vs. shape[1] = []\n\t [[{{node ignore_noParent_MSE/cond/concat_1}}]] [Op:__inference_test_function_539719]"
     ]
    }
   ],
   "source": [
    "dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "dense.fit(beanIntensities, beanIntensities, epochs=200,  verbose=True, validation_data=(validation, validation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recontruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 150ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 11, 372)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = dense.predict(validation)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(o.reshape(11,372))\n",
    "v = pd.DataFrame(validation.reshape(11,372))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>117</th>\n",
       "      <th>131</th>\n",
       "      <th>164</th>\n",
       "      <th>225</th>\n",
       "      <th>259</th>\n",
       "      <th>334</th>\n",
       "      <th>350</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.274163</td>\n",
       "      <td>0.487098</td>\n",
       "      <td>0.257865</td>\n",
       "      <td>0.371089</td>\n",
       "      <td>0.399192</td>\n",
       "      <td>0.277941</td>\n",
       "      <td>0.488437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278971</td>\n",
       "      <td>0.478599</td>\n",
       "      <td>0.372680</td>\n",
       "      <td>0.377176</td>\n",
       "      <td>0.530723</td>\n",
       "      <td>0.456966</td>\n",
       "      <td>0.466455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.306592</td>\n",
       "      <td>0.374852</td>\n",
       "      <td>0.407235</td>\n",
       "      <td>0.303886</td>\n",
       "      <td>0.566272</td>\n",
       "      <td>0.482502</td>\n",
       "      <td>0.441782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.020226</td>\n",
       "      <td>-0.999853</td>\n",
       "      <td>-0.352515</td>\n",
       "      <td>-0.998212</td>\n",
       "      <td>-1.060028</td>\n",
       "      <td>-1.000575</td>\n",
       "      <td>-1.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295452</td>\n",
       "      <td>0.431085</td>\n",
       "      <td>0.335573</td>\n",
       "      <td>0.383160</td>\n",
       "      <td>0.366015</td>\n",
       "      <td>0.391737</td>\n",
       "      <td>0.475569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.257569</td>\n",
       "      <td>0.432375</td>\n",
       "      <td>0.235357</td>\n",
       "      <td>0.150651</td>\n",
       "      <td>0.488824</td>\n",
       "      <td>0.319487</td>\n",
       "      <td>0.467923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.356134</td>\n",
       "      <td>0.445294</td>\n",
       "      <td>0.382733</td>\n",
       "      <td>0.387969</td>\n",
       "      <td>0.562093</td>\n",
       "      <td>0.457083</td>\n",
       "      <td>0.490710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.007986</td>\n",
       "      <td>0.061953</td>\n",
       "      <td>-0.104753</td>\n",
       "      <td>0.043568</td>\n",
       "      <td>-0.034042</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.060736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.266966</td>\n",
       "      <td>0.428140</td>\n",
       "      <td>0.320027</td>\n",
       "      <td>0.336281</td>\n",
       "      <td>0.488731</td>\n",
       "      <td>0.396681</td>\n",
       "      <td>0.478137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.290860</td>\n",
       "      <td>0.470449</td>\n",
       "      <td>0.212106</td>\n",
       "      <td>0.147425</td>\n",
       "      <td>0.506517</td>\n",
       "      <td>0.433794</td>\n",
       "      <td>0.479464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.310394</td>\n",
       "      <td>0.456960</td>\n",
       "      <td>0.334936</td>\n",
       "      <td>0.356321</td>\n",
       "      <td>0.554873</td>\n",
       "      <td>0.453193</td>\n",
       "      <td>0.504193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         117       131       164       225       259       334       350\n",
       "0   0.274163  0.487098  0.257865  0.371089  0.399192  0.277941  0.488437\n",
       "1   0.278971  0.478599  0.372680  0.377176  0.530723  0.456966  0.466455\n",
       "2   0.306592  0.374852  0.407235  0.303886  0.566272  0.482502  0.441782\n",
       "3  -1.020226 -0.999853 -0.352515 -0.998212 -1.060028 -1.000575 -1.000772\n",
       "4   0.295452  0.431085  0.335573  0.383160  0.366015  0.391737  0.475569\n",
       "5   0.257569  0.432375  0.235357  0.150651  0.488824  0.319487  0.467923\n",
       "6   0.356134  0.445294  0.382733  0.387969  0.562093  0.457083  0.490710\n",
       "7  -0.007986  0.061953 -0.104753  0.043568 -0.034042  0.013171  0.060736\n",
       "8   0.266966  0.428140  0.320027  0.336281  0.488731  0.396681  0.478137\n",
       "9   0.290860  0.470449  0.212106  0.147425  0.506517  0.433794  0.479464\n",
       "10  0.310394  0.456960  0.334936  0.356321  0.554873  0.453193  0.504193"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>117</th>\n",
       "      <th>131</th>\n",
       "      <th>164</th>\n",
       "      <th>225</th>\n",
       "      <th>259</th>\n",
       "      <th>334</th>\n",
       "      <th>350</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.464054</td>\n",
       "      <td>0.445813</td>\n",
       "      <td>0.475648</td>\n",
       "      <td>0.475451</td>\n",
       "      <td>0.601666</td>\n",
       "      <td>0.538352</td>\n",
       "      <td>0.517300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464782</td>\n",
       "      <td>0.410751</td>\n",
       "      <td>0.479145</td>\n",
       "      <td>0.469503</td>\n",
       "      <td>0.615856</td>\n",
       "      <td>0.581054</td>\n",
       "      <td>0.521357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.448974</td>\n",
       "      <td>0.424922</td>\n",
       "      <td>0.462434</td>\n",
       "      <td>0.489874</td>\n",
       "      <td>0.618050</td>\n",
       "      <td>0.541640</td>\n",
       "      <td>0.511626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.448781</td>\n",
       "      <td>0.442738</td>\n",
       "      <td>0.473856</td>\n",
       "      <td>0.468686</td>\n",
       "      <td>0.588641</td>\n",
       "      <td>0.488964</td>\n",
       "      <td>0.528962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.375646</td>\n",
       "      <td>0.487818</td>\n",
       "      <td>0.385170</td>\n",
       "      <td>0.420683</td>\n",
       "      <td>0.515270</td>\n",
       "      <td>0.469018</td>\n",
       "      <td>0.508054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.506284</td>\n",
       "      <td>0.412253</td>\n",
       "      <td>0.486031</td>\n",
       "      <td>0.449196</td>\n",
       "      <td>0.638067</td>\n",
       "      <td>0.580319</td>\n",
       "      <td>0.503012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.469681</td>\n",
       "      <td>0.415919</td>\n",
       "      <td>0.528647</td>\n",
       "      <td>0.509842</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.563962</td>\n",
       "      <td>0.527670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.462067</td>\n",
       "      <td>0.470147</td>\n",
       "      <td>0.484273</td>\n",
       "      <td>0.481506</td>\n",
       "      <td>0.631007</td>\n",
       "      <td>0.569937</td>\n",
       "      <td>0.540709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.525481</td>\n",
       "      <td>0.414705</td>\n",
       "      <td>0.491357</td>\n",
       "      <td>0.473794</td>\n",
       "      <td>0.641192</td>\n",
       "      <td>0.572485</td>\n",
       "      <td>0.556883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         117       131       164       225       259       334       350\n",
       "0   0.464054  0.445813  0.475648  0.475451  0.601666  0.538352  0.517300\n",
       "1   0.464782  0.410751  0.479145  0.469503  0.615856  0.581054  0.521357\n",
       "2   0.448974  0.424922  0.462434  0.489874  0.618050  0.541640  0.511626\n",
       "3  -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000\n",
       "4   0.448781  0.442738  0.473856  0.468686  0.588641  0.488964  0.528962\n",
       "5   0.375646  0.487818  0.385170  0.420683  0.515270  0.469018  0.508054\n",
       "6   0.506284  0.412253  0.486031  0.449196  0.638067  0.580319  0.503012\n",
       "7  -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000 -1.000000\n",
       "8   0.469681  0.415919  0.528647  0.509842  0.595536  0.563962  0.527670\n",
       "9   0.462067  0.470147  0.484273  0.481506  0.631007  0.569937  0.540709\n",
       "10  0.525481  0.414705  0.491357  0.473794  0.641192  0.572485  0.556883"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Size Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 11, 372)\n",
      "(None,)\n",
      "tf size Tensor(\"ignore_noParent_MSE/Size_1:0\", shape=(), dtype=int32)\n",
      "(None, 1) (None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\2463984898.py\", line 69, in ignore_noParent_MSE  *\n        y_pred_total = tf.concat([y_pred_pos, y_shouldBeNegButIsnt], axis = 0) #concatenate for total mse\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 1 and 2. Shapes are [1] and [2]. for '{{node ignore_noParent_MSE/cond/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](ignore_noParent_MSE/cond/concat/ignore_noParent_MSE/GatherV2_7, ignore_noParent_MSE/cond/concat/ignore_noParent_MSE/GatherV2_5, ignore_noParent_MSE/cond/concat/axis)' with input shapes: [?,1], [?,2], [] and with computed input tensors: input[2] = <0>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\beanEncoder\\BeanEncoderMain.ipynb Cell 44\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dense \u001b[39m=\u001b[39m modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, value)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dense\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39mignore_noParent_MSE)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dense\u001b[39m.\u001b[39;49mfit(beanIntensities, beanIntensities, epochs\u001b[39m=\u001b[39;49m\u001b[39m70\u001b[39;49m,  verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m test_hat \u001b[39m=\u001b[39m dense(validation) \u001b[39m#, verbose = 0)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m ignore_noParent_MSE(validation, test_hat)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\FINAMI~1\\AppData\\Local\\Temp\\__autograph_generated_filehc2cz775.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\FINAMI~1\\AppData\\Local\\Temp\\__autograph_generated_file89h6_za8.py:57\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__ignore_noParent_MSE\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     55\u001b[0m y_pred_total \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39my_pred_total\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     56\u001b[0m y_true_total \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39my_true_total\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39msize, (ag__\u001b[39m.\u001b[39mld(y_shouldBeNegButIsnt),), \u001b[39mNone\u001b[39;00m, fscope) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mdo_return\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mretval_\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m2\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[1;32mC:\\Users\\FINAMI~1\\AppData\\Local\\Temp\\__autograph_generated_file89h6_za8.py:47\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__ignore_noParent_MSE.<locals>.else_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mnonlocal\u001b[39;00m retval_, do_return\n\u001b[0;32m     46\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(ag__\u001b[39m.\u001b[39mld(y_pred_pos)\u001b[39m.\u001b[39mshape, ag__\u001b[39m.\u001b[39mld(y_shouldBeNegButIsnt)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 47\u001b[0m y_pred_total \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mconcat, ([ag__\u001b[39m.\u001b[39;49mld(y_pred_pos), ag__\u001b[39m.\u001b[39;49mld(y_shouldBeNegButIsnt)],), \u001b[39mdict\u001b[39;49m(axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), fscope)\n\u001b[0;32m     48\u001b[0m y_true_total \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mconcat, ([ag__\u001b[39m.\u001b[39mld(y_true_pos), ag__\u001b[39m.\u001b[39mld(y_true_wrong)],), \u001b[39mdict\u001b[39m(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), fscope)\n\u001b[0;32m     49\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\2463984898.py\", line 69, in ignore_noParent_MSE  *\n        y_pred_total = tf.concat([y_pred_pos, y_shouldBeNegButIsnt], axis = 0) #concatenate for total mse\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 1 and 2. Shapes are [1] and [2]. for '{{node ignore_noParent_MSE/cond/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](ignore_noParent_MSE/cond/concat/ignore_noParent_MSE/GatherV2_7, ignore_noParent_MSE/cond/concat/ignore_noParent_MSE/GatherV2_5, ignore_noParent_MSE/cond/concat/axis)' with input shapes: [?,1], [?,2], [] and with computed input tensors: input[2] = <0>.\n"
     ]
    }
   ],
   "source": [
    "N = 40\n",
    "hidden = np.arange(2,24, 1) #range(1,32)\n",
    "lossMatrix = []\n",
    "for i in tqdm(range(N)):\n",
    "    \n",
    "    losses = []\n",
    "    for value in (hidden):\n",
    "        dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, value)\n",
    "        dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "        dense.fit(beanIntensities, beanIntensities, epochs=70,  verbose=0)\n",
    "        test_hat = dense(validation) #, verbose = 0)\n",
    "        loss = ignore_noParent_MSE(validation, test_hat)\n",
    "        losses.append(loss)\n",
    "        tf.keras.backend.clear_session()\n",
    "    lossMatrix.append(losses)\n",
    "    \n",
    "lossMatrix = np.array(lossMatrix)\n",
    "#run 100 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgMSE = np.average(lossMatrix, axis = 0)\n",
    "plt.plot(hidden, avgMSE);\n",
    "plt.xlabel(\"Latent Space Size\");\n",
    "plt.ylabel(\"MSE\");\n",
    "plt.title(\"MSE of the AutoEncoder with Respect to the Latent Space Size\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = pd.DataFrame(lossMatrix)\n",
    "lm.to_csv(\"lossmatrix4lisa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(\"lossmatrix4lisa.csv\").to_numpy()\n",
    "print(temp.shape)\n",
    "avgMSE = np.average(lossMatrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hidden, avgMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lazy_train(epochs=60, hidden = 32):\n",
    "    ep = epochs\n",
    "    hidden = [hidden,] #range(1,32)\n",
    "    lossMatrix = []\n",
    "    lazy_weights = []\n",
    "    dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, num_hidden_units=32)\n",
    "    dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "\n",
    "    #for i in tqdm(range(ep)):\n",
    "    for i in range(ep):\n",
    "        dense.fit(beanIntensities, beanIntensities, validation_data=(validation, validation), epochs=1,  verbose=0)\n",
    "        lazy_weights.append(dense.get_weights())\n",
    "        test = dense(validation) #, verbose = 0)\n",
    "        loss = ignore_noParent_MSE(validation, test)\n",
    "        lossMatrix.append(loss)\n",
    "        \n",
    "    lossMatrix = np.array(lossMatrix)\n",
    "    lazy_weights = np.array(lazy_weights)\n",
    "\n",
    "    return lossMatrix, lazy_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://proceedings.mlr.press/v162/rachwan22a/rachwan22a.pdf Winning the Lottery Ticket Ahead of Time:\n",
    "def lazyKernelRegime(w, parent_idx=parent_idx):\n",
    "\n",
    "    firstLayer = []\n",
    "    for i in range(len(w)):\n",
    "        firstLayer.append(w[i][0][parent_idx])\n",
    "\n",
    "    fL = np.array(firstLayer)    \n",
    "    d0 = np.square(fL[1] - fL[0])\n",
    "    kernelChange = []\n",
    "    for i in range(1,len(fL)):\n",
    "        dt = np.square(fL[i] - fL[0])\n",
    "        dt_minus1 = np.square(fL[i-1] - fL[0])   \n",
    "        d = np.abs(dt - dt_minus1)/d0                        #eq 11 from the paper\n",
    "        kernelChange.append(d)\n",
    "    \n",
    "    kernelChange = np.moveaxis(kernelChange, 0, 2)\n",
    "    # plt.plot(kernelChange[0][0]);\n",
    "    # plt.title(\"$|\\Delta W|$ vs Epoch\")\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"$|\\Delta W|$\")\n",
    "\n",
    "    return np.array(  kernelChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distrib(change, t = .05, raw = False): #raw means return the unshaped indicies. \n",
    "    stop = []\n",
    "\n",
    "    for parent in range(len(change)):\n",
    "        for child in range(len(change[0])):\n",
    "            try:\n",
    "                stop.append(np.min(np.argwhere(change[parent][child] < t)))\n",
    "            except ValueError:\n",
    "                stop.append(len(change))\n",
    "                # print(parent, child)\n",
    "                # plt.plot(change[parent][child])\n",
    "                # assert(False)\n",
    "    # print(change.shape)\n",
    "    # assert(False)\n",
    "    \n",
    "    stop = np.array(stop).flatten()\n",
    "    # print(stop.shape)\n",
    "    var = np.std(stop)\n",
    "    # print(var)\n",
    "    # assert(False)\n",
    "    mean = np.average(stop)\n",
    "    top_parents = np.argwhere(stop > mean + 2*var) #get the parents which take more than 2 stds to stop training\n",
    "    top_parent_child = []\n",
    "    for tp in top_parents:\n",
    "        top_parent_child.append(np.unravel_index(tp, shape = (NUM_PARENTS, NUM_TARGETS)))\n",
    "\n",
    "    if raw == False:\n",
    "        plt.hist(stop)\n",
    "        plt.xlabel(\"Epoch where regulator-target-weight began changing by at most \"+ str(t));\n",
    "        plt.ylabel(\"Number of parent-child-weights\");\n",
    "        plt.title(\"Histogram of weight stops\");\n",
    "        print(\"average stop: \", mean);\n",
    "\n",
    "    if raw == True:\n",
    "        return np.array(top_parents)\n",
    "\n",
    "    return np.array(top_parent_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazyKernels(N=100):\n",
    "\n",
    "    candidates = []\n",
    "    final_w = []\n",
    "    for i in tqdm(range(N)):\n",
    "        lm, lazy_weights = do_lazy_train(epochs=80)\n",
    "        change = lazyKernelRegime(lazy_weights)\n",
    "        top_pr = np.squeeze(compute_distrib(change, t = 0.05, raw=True))\n",
    "        candidates.append(top_pr)\n",
    "        final_w.append(lazy_weights[-1])\n",
    "    \n",
    "    final_w = np.array(final_w)\n",
    "    firstLayer = []\n",
    "    for i in range(len(final_w)):\n",
    "        firstLayer.append(np.abs(final_w[i][0][parent_idx]))\n",
    "    fw = np.array(firstLayer)\n",
    "    #print(fw.shape)\n",
    "    fw_avg = np.average(fw, axis = 0)\n",
    "    #print(fw_avg.shape)\n",
    "    \n",
    "    candidates = np.hstack(candidates)\n",
    "    candidates = candidates.reshape(candidates.size)\n",
    "    #print(candidates.shape)\n",
    "\n",
    "    plt.hist(candidates, bins=np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    plt.title(\"Parent-Child Regulator Histogram\")\n",
    "    plt.xlabel(\"Parent-Child Weight\")\n",
    "    plt.ylabel(\"Num-Times parent-child relationship trained for top 5% of time\")\n",
    "    return candidates, fw_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [02:46<18:34, 12.81s/it]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'ignore_noParent_MSE/cond/concat_1' defined at (most recent call last):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\1478310156.py\", line 1, in <cell line: 1>\n      can, mag = lazyKernels()\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\233243190.py\", line 6, in lazyKernels\n      lm, lazy_weights = do_lazy_train(epochs=80)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\2645043549.py\", line 11, in do_lazy_train\n      dense.fit(beanIntensities, beanIntensities, validation_data=(validation, validation), epochs=1,  verbose=0)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1445, in fit\n      val_logs = self.evaluate(\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1756, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1557, in test_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1546, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1535, in run_step\n      outputs = model.test_step(data)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1501, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 139, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 243, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\938458728.py\", line 60, in ignore_noParent_MSE\n      if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0.\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\938458728.py\", line 64, in ignore_noParent_MSE\n      y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\nNode: 'ignore_noParent_MSE/cond/concat_1'\nConcatOp : Ranks of all input tensors should match: shape[0] = [63] vs. shape[1] = []\n\t [[{{node ignore_noParent_MSE/cond/concat_1}}]] [Op:__inference_test_function_523179]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\beanEncoder\\BeanEncoderMain.ipynb Cell 54\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m can, mag \u001b[39m=\u001b[39m lazyKernels()\n",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\beanEncoder\\BeanEncoderMain.ipynb Cell 54\u001b[0m in \u001b[0;36mlazyKernels\u001b[1;34m(N)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m final_w \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(N)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     lm, lazy_weights \u001b[39m=\u001b[39m do_lazy_train(epochs\u001b[39m=\u001b[39;49m\u001b[39m80\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     change \u001b[39m=\u001b[39m lazyKernelRegime(lazy_weights)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     top_pr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(compute_distrib(change, t \u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\beanEncoder\\BeanEncoderMain.ipynb Cell 54\u001b[0m in \u001b[0;36mdo_lazy_train\u001b[1;34m(epochs, hidden)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#for i in tqdm(range(ep)):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ep):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     dense\u001b[39m.\u001b[39;49mfit(beanIntensities, beanIntensities, validation_data\u001b[39m=\u001b[39;49m(validation, validation), epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,  verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     lazy_weights\u001b[39m.\u001b[39mappend(dense\u001b[39m.\u001b[39mget_weights())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/beanEncoder/BeanEncoderMain.ipynb#Y101sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     test \u001b[39m=\u001b[39m dense(validation) \u001b[39m#, verbose = 0)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'ignore_noParent_MSE/cond/concat_1' defined at (most recent call last):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\1478310156.py\", line 1, in <cell line: 1>\n      can, mag = lazyKernels()\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\233243190.py\", line 6, in lazyKernels\n      lm, lazy_weights = do_lazy_train(epochs=80)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\2645043549.py\", line 11, in do_lazy_train\n      dense.fit(beanIntensities, beanIntensities, validation_data=(validation, validation), epochs=1,  verbose=0)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1445, in fit\n      val_logs = self.evaluate(\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1756, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1557, in test_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1546, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1535, in run_step\n      outputs = model.test_step(data)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 1501, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 139, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Fin Amin\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\losses.py\", line 243, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\938458728.py\", line 60, in ignore_noParent_MSE\n      if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0.\n    File \"C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_7288\\938458728.py\", line 64, in ignore_noParent_MSE\n      y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\nNode: 'ignore_noParent_MSE/cond/concat_1'\nConcatOp : Ranks of all input tensors should match: shape[0] = [63] vs. shape[1] = []\n\t [[{{node ignore_noParent_MSE/cond/concat_1}}]] [Op:__inference_test_function_523179]"
     ]
    }
   ],
   "source": [
    "can, mag = lazyKernels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf, bins = np.histogram(can, np.arange(NUM_PARENTS*NUM_TARGETS))\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitudes After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS*NUM_TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(NUM_PARENTS*NUM_TARGETS, shape = (NUM_PARENTS, NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_reg_targets(can, mag):\n",
    "    \n",
    "    pdf, bins = np.histogram(can, np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    pdf2d = np.zeros(shape = (NUM_PARENTS,NUM_TARGETS))\n",
    "\n",
    "    for i in bins:\n",
    "      idx2d = np.unravel_index(i, shape = (NUM_PARENTS, NUM_TARGETS))\n",
    "      try:\n",
    "        pdf2d[idx2d] = pdf[i]\n",
    "      except IndexError:\n",
    "        print(i, idx2d, len(pdf), len(bins))\n",
    "    \n",
    "\n",
    "    importance = np.multiply(pdf2d, mag)\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = get_top_reg_targets(can, mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top[6][122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(top, cmap = 'inferno', vmin = 0, vmax = np.max(top));\n",
    "plt.colorbar(u ,fraction=0.0086, pad=0.02);\n",
    "plt.title(\"The Most Important Regulator-Target Combinations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topID = np.array(np.unravel_index(np.argsort(top, axis=None), top.shape))\n",
    "topID = np.flip(topID, axis=1)\n",
    "topID[0] = parent_idx[topID[0]]\n",
    "topID = topID.T\n",
    "topID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topR_T = pd.DataFrame(topID)\n",
    "# topR_T.to_csv(\"Top_reg_target_decendingOrder_firstColIsRegulator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.zeros(NUM_PARENTS)\n",
    "for i in range(NUM_PARENTS):\n",
    "    best[i] = np.sum(top[i])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,NUM_PARENTS, dtype=int)\n",
    "plt.plot(x, best); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.flip(np.argsort(best))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(can).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Petal Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal = pd.read_excel(data_path_petal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train = petal[petal[\"Line\"] == \"WT\"]\n",
    "petal_train = petal_train.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_train.head(12)\n",
    "petal_train = petal_train.groupby(['Plate']).mean()\n",
    "petal_train.head()\n",
    "petal_train = petal_train.to_numpy()\n",
    "print(petal_train.shape)\n",
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(petal_train)\n",
    "petal_train = scaler1.transform(petal_train)\n",
    "mm = MinMaxScaler()\n",
    "mm.fit(petal_train)\n",
    "petal_train = mm.transform(petal_train)\n",
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test = petal[petal[\"Line\"] != \"WT\"]\n",
    "petal_test = petal_test.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_test = petal_test.groupby(['Plate']).mean()\n",
    "petal_test.head()\n",
    "petal_test = petal_test.to_numpy()\n",
    "print(petal_test.shape)\n",
    "petal_test = scaler1.transform(petal_test)\n",
    "petal_test = mm.transform(petal_test)\n",
    "petal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1.shape\n",
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densePredictor = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS, 22)\n",
    "densePredictor.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "densePredictor.fit(beanIntensities, beanIntensities,validation_data=(experiment1, experiment1),  epochs=100,  verbose=1)\n",
    "test = densePredictor(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgm = superParent\n",
    "time_steps = 6\n",
    "num_kinase_regulators = NUM_TARGETS\n",
    "num_hidden_units = 22\n",
    "\n",
    "inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "x = DenseEncoderLinear2(rgm, regulator_gene_matrix, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "enc = denseencoder2(x, inp, num_hidden_units)\n",
    "denseP = tf.keras.Model(inputs=inp, outputs=enc)\n",
    "#set the weights of the encoder to the weights of auto encoder\n",
    "dw = densePredictor.get_weights()\n",
    "enc_w = dw[0:5]\n",
    "denseP.set_weights(enc_w)\n",
    "#add a dense layer  because we are ouputing 1 number\n",
    "l = Dense(32, activation = 'swish', use_bias=True, kernel_regularizer='l1_l2')(denseP.layers[-1].output)\n",
    "l = Dense(1, activation = 'linear', use_bias = True)(l)\n",
    "denseP = tf.keras.Model(denseP.inputs, l)\n",
    "#denseP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = np.concatenate([experiment1, experiment1, experiment1, experiment1])\n",
    "bp.shape\n",
    "#bigexperiment1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = petal_train[0]\n",
    "b = petal_train[1]\n",
    "c = petal_train[2]\n",
    "d = petal_train[3]\n",
    "\n",
    "petal_train1 = np.array([a,a,a,a, b,b,b,b, c,c,c,c, d,d,d,d]) #does this make sense? we are training network to predict .5\n",
    "petal_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "denseP.fit(experiment1, petal_train, epochs=500, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP(experiment1) #experiment1 is part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(denseP(testCandidate)) #model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test #true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.evaluate(testCandidate, petal_test) #eval gave 1.3999 before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test set and the synthetic dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestSet(test_path):\n",
    "    testFiles = []\n",
    "    for np_name in glob(os.path.join(data_path_testSet,'*.np[yz]')):\n",
    "        k = np.load(os.path.join(data_path_testSet,np_name))\n",
    "        testFiles.append(k)\n",
    "#         print(np_name)\n",
    "#         print(k.shape)\n",
    "    return np.array(testFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PKjzdFCMwFg"
   },
   "outputs": [],
   "source": [
    "def read_files(data_path):\n",
    "\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        try:\n",
    "            #print('file name:',file)\n",
    "            #print('value of i:',i)\n",
    "            genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "            #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "            replicates.append(np.array(genes_intensities_data_matrix.values, dtype = float))\n",
    "            # i+=1\n",
    "        except PermissionError:\n",
    "            print(\"Not a CSV: \", os.path.join(data_path , file))\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    rep = np.array(replicates).astype(np.float32)\n",
    "    \n",
    "    return rep, rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCwo4LwlO_FF"
   },
   "outputs": [],
   "source": [
    "beanIntensities, regulator_gene_matrix= read_files(data_path_syn)\n",
    "matrix = regulator_gene_matrix\n",
    "replicates = beanIntensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros(shape = (3,6,8))\n",
    "id = np.unravel_index(3*6*8 - 1, shape = d.shape)\n",
    "d[id] = 1\n",
    "plt.imshow(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0][ : , parentIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)\n",
    "print(testCandidate.shape)\n",
    "syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(change[0][22])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"change in weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossMatrix)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change.shape, lossMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diff(change[0][22])\n",
    "plt.plot(d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change[0][0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_tresh(change, stop = 0.05):\n",
    "#     diffs = []\n",
    "#     for parent in range(len(change)):\n",
    "#         for child in range(len(change[0])):\n",
    "#             diffs.append(np.diff(change[parent][child]))\n",
    "#     inflection = []\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         for d in diffs:\n",
    "#             print(np.argwhere(np.abs(d) < stop))\n",
    "#             inflection.append(np.min(np.argwhere(np.abs(d) < stop))) #return where the second derivative is first 0. \n",
    "\n",
    "#     except ValueError:\n",
    "#         print(\"Stop value \", stop, \" is too high, trying stop = \", stop + 0.05)\n",
    "#         # s = stop + 0.05\n",
    "#         # return compute_tresh(change, stop = s)\n",
    "        \n",
    "\n",
    "#     return np.average(inflection)\n",
    "        \n",
    "# d = compute_tresh(change)\n",
    "# d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"mse3.npy\", avgMSE) #mse2/3 is with -1 fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sciencedirect.com/science/article/pii/S0925231220314570?casa_token=lcEJANqO0JwAAAAA:uL3DGUZctPUZz_sPz1K1i2klMtb83TyKnc9CI3_N-uSOaM7VHL8GhM0jCGYfo25NmpDQQ9Cvlw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rshp = Flatten()(looseParent.layers[-1].output)\n",
    "\n",
    "modelTemp = tf.keras.Model(inputs=looseParent.input, outputs = [rshp])\n",
    "modelTemp.summary()\n",
    "type(modelTemp)\n",
    "explainer = shap.DeepExplainer(modelTemp, syntheticDataTrain)\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough #this solves the \"shap_ADDV2\" problem but another one will appear\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"FusedBatchNormV3\"] = shap.explainers._deep.deep_tf.passthrough #this solves the next problem which allows you to run the DeepExplainer.\n",
    "\n",
    "shap_values = explainer.shap_values(testCandidate[0:1])\n",
    "def f(x):\n",
    "    return modelTemp.predict(x)\n",
    "\n",
    "print(f(testCandidate))\n",
    "explainer = shap.KernelExplainer(f , testCandidate[0:1], link=\"logit\") #svm.predict_proba, X_train, link=\"logit\")\n",
    "shap_values = explainer.shap_values(testCandidate[0:1], nsamples=100)\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import json\n",
    "import shap\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "X,y = shap.datasets.imagenet50()\n",
    "to_explain = X[[39,41]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(enc_dec_Synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [5,6]\n",
    "u = tf.concat([a,b], axis = 0)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newConnections = superParent - regulator_gene_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(newConnections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nC = []\n",
    "# for i in range(len(newConnections[0])):\n",
    "#     for j in range(len(newConnections[1])):\n",
    "#         if newConnections[i][j] > 0:\n",
    "#             nC.append([i,j])\n",
    "# nC = np.array(nC)\n",
    "# nC = pd.DataFrame(nC)\n",
    "# nC.to_csv(\"new_connections_in_superParents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code for testing loss function\n",
    "# print(outSyn.shape)\n",
    "# print(testCandidate.shape)\n",
    "# syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "# syntheticLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Auto Encoder\n",
    "Autoencoder has not been trained on synthetic version of experiement 1. We test on the original experiment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrRuJ_bsrHll"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAOZEEvRRVU4"
   },
   "outputs": [],
   "source": [
    "# enc_dec_Synthetic.compile(optimizer='adam',loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticDataTrain = beanIntensities[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1649273035573,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "EuNDZx5Ov34I",
    "outputId": "74597aa1-c71c-4491-b8c9-4fefed309325"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.fit(syntheticDataTrain,syntheticDataTrain,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = enc_dec_Synthetic(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = enc_dec_Synthetic.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w[0], cmap = \"hot\", vmin=0,vmax=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBzDmjqJViEw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we do not need to use this function for the testset\n",
    "def getCSVs(data_path_head):\n",
    "    PATH = data_path_head\n",
    "    EXT = \"*.csv\"\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "    actual = []\n",
    "    for p in all_csv_files:\n",
    "        actual.append(pd.read_csv(p, index_col = 0).to_numpy())\n",
    "    return np.array(actual)\n",
    "    \n",
    "experiment1 = getCSVs(data_path_og_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate = test.numpy().astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outSyn = enc_dec_Synthetic.predict(testCandidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymagn(A, B):\n",
    "    mse = (np.square(A - B)).mean(axis=None)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticLoss = ignore_noParent_MSE(testCandidate, outSyn )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(outSyn-testCandidate).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install keras-visualizer\n",
    "#!pip install pydot\n",
    "#data_path_og_exp1 = data_path_testSet \n",
    "# !pip install pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = loadTestSet(data_path_testSet)\n",
    "# testCandidate = test.astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPRV0OAMpKPH"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS) #we can just change the time steps to something higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1649273028683,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "D6aPT5_cpKK0",
    "outputId": "c261d824-0f38-4eee-b139-f03a80f093e1"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated dataset Auto Encoder\n",
    "Once again, we do not train on any version of exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filesV2(data_path):\n",
    "    '''\n",
    "    *Changed*\n",
    "    currently hardcoded for only one file. \n",
    "    change code a bit for reading multiple files.\n",
    "    '''\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        #print('file name:',file)\n",
    "        #print('value of i:',i)\n",
    "        genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "        #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "        replicates.append(genes_intensities_data_matrix.values)\n",
    "        # i+=1\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    \n",
    "    return np.asarray(replicates), rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes, _ = read_filesV2(data_path_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(interpolated_genes[2]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = []\n",
    "for k in range(len(interpolated_genes)):\n",
    "    #print(k)\n",
    "    if k == 2 or k == 3 or k == 4:\n",
    "        inter.append(np.reshape(interpolated_genes[k], (4,6,NUM_TARGETS)))\n",
    "    else: \n",
    "        inter.append(np.reshape(interpolated_genes[k], (5,6,NUM_TARGETS)))\n",
    "inter = np.vstack(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beanIntensities[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter = model(regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS) \n",
    "enc_dec_inter.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter.fit(inter, inter,epochs=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter = enc_dec_inter.predict(testCandidate)\n",
    "interpolationLoss = ignore_noParent_MSE(testCandidate, outInter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolationLoss #used to be 3.84 on broke ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = enc_dec_inter.history\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons between various outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(np.reshape((np.abs(outSyn)), (24,NUM_TARGETS)), cmap = \"hot\", vmin=0,vmax=1.0 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(outSyn[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(testCandidate[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape((np.abs(outSyn-outInter)), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 );\n",
    "plt.title(\"Difference Between the Outputs of Both Autoencoders\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"DiffBetweenOut.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outInter-experiment1), (24,NUM_TARGETS)) , cmap = \"hot\") #, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Interpolated Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30)\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"InterDiffImage.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outSyn-experiment1), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"SynDiffImage.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx = parentIndex.numpy()\n",
    "#print(parent_idx)\n",
    "oSyn = (np.reshape((outSyn), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "oSyn = oSyn.T\n",
    "oSyn.shape\n",
    "\n",
    "exp1_col = (np.reshape((experiment1), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "exp1_col = exp1_col.T\n",
    "print(exp1_col.shape)\n",
    "\n",
    "u = plt.imshow(np.abs(oSyn - exp1_col), cmap = 'hot') #TODO use TF loss function instead of difference.\n",
    "ddff = oSyn-exp1_col\n",
    "plt.colorbar(u)\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data. Only parents.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(oSyn).head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(exp1_col).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ddff).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"interpolated_v2.npy\", inter) #the interpolated dataset\n",
    "# np.save(\"synthetic_v2.npy\", beanIntensities[1:]) # the synthetic dataset\n",
    "# np.save(\"synOut_v2.npy\", outSyn) #the output of the encoder trained on synthetic data with the input being exp1\n",
    "# np.save(\"interOut_v2.npy\", outInter) #the output of the encoder trained on interpolated data with the input being exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6J5QSynTTgZ+lFOhgOtAG",
   "collapsed_sections": [],
   "name": "cnn-third.ipynb",
   "provenance": [
    {
     "file_id": "1mmRxO5jnBc5CdzxXj8sMtPpG0BQ0ulSs",
     "timestamp": 1648921397876
    },
    {
     "file_id": "10758zFj2UnTnwpQaSFIr5r9MqZWdiMsf",
     "timestamp": 1648674775255
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
