{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import synapseclient \n",
    "#  import synapseutils \n",
    " \n",
    "#  syn = synapseclient.Synapse() \n",
    "#  syn.login('finamintoastcrunch','1Hjldria!') \n",
    "#  files = synapseutils.syncFromSynapse(syn, ' syn2825306 ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTo_HuQkGgAq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import*\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Conv1D\n",
    "# from keras.layers import Conv1DTranspose\n",
    "# from keras.layers import Flatten, Reshape\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS = 41\n",
    "NUM_TARGETS = 100\n",
    "NUM_TIME_STEPS = 21\n",
    "NUM_REPLICATES = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXpISOijEYqQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# matrix_path = \"regulator-gene-matrix.csv\"\n",
    "# data_path_syn = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\synData\"\n",
    "# data_path_inter =  r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\interpolatedOnly\"\n",
    "# data_path_og_exp1 = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\datasets\\exp1\"\n",
    "# data_path_testSet = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\testSetFixed\"\n",
    "# data_path_petal = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\petal_len.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirty_RGM = r'Regulations_Control_Altona.csv'\n",
    "dirty_regulations = r'insilico_size100_1_timeseries.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirtyReg = pd.read_csv(dirty_regulations,  index_col = 0,)# on_bad_lines='skip')\n",
    "dirtyReg = dirtyReg.dropna(axis=0)\n",
    "dirtyReg = dirtyReg.select_dtypes(include=np.number)\n",
    "dirtyReg = dirtyReg.to_numpy() #reshape would not work in this case. \n",
    "\n",
    "dRegs = np.zeros(shape=(NUM_REPLICATES, NUM_TIME_STEPS, NUM_TARGETS), dtype=np.float)\n",
    "for i in range(NUM_REPLICATES):\n",
    "    dRegs[i] = dirtyReg[NUM_TIME_STEPS*i : NUM_TIME_STEPS*(i+1)]\n",
    "dRegs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fix_dataset(dirtyR):\n",
    "    ds = dirtyR\n",
    "    ret = np.zeros(shape=(NUM_REPLICATES,NUM_TIME_STEPS, NUM_TARGETS))\n",
    "\n",
    "    # for i in range(0,NUM_REPLICATES*NUM_TIME_STEPS, 4):\n",
    "    #     for j in range(0, NUM_REPLICATES):\n",
    "    #         dataset[j][:,i//NUM_REPLICATES] = dirtyR[:,(i+j)]\n",
    "    \n",
    "    ds[ds==0.0] = np.nan #we do this so the scaling ignores 0.0 #CHECKED\n",
    "\n",
    "\n",
    "    for i in range(NUM_REPLICATES):\n",
    "        regScaled = StandardScaler().fit_transform(ds[i].flatten().reshape((-1,1)))\n",
    "        regScaled = MinMaxScaler().fit_transform(ds[i].flatten().reshape((-1,1))) #ignores np.nan\n",
    "        regScaled = regScaled.reshape((NUM_TARGETS, NUM_TIME_STEPS))\n",
    "        regScaled = np.nan_to_num(regScaled, nan= 0.0)\n",
    "        ret[i] = regScaled.T\n",
    "    return ret\n",
    "\n",
    "dataset = fix_dataset(dirtyR=dRegs)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beanIntensities = dataset[0:8]\n",
    "validation = dataset[8:]\n",
    "allData = dataset\n",
    "print(beanIntensities.shape, validation.shape, allData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the third replicate is trash. we will not use it. \n",
    "# df(dataset[3]).head(11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regulator_gene_matrix = np.load(\"soyBeanRGM.npy\")\n",
    "# regulator_gene_matrix = regulator_gene_matrix.astype('float32')\n",
    "# regulator_gene_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Compare Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv(\"DREAM4_GoldStandard_InSilico_Size100_1.csv\", header=None)\n",
    "gold.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_numeric(df):\n",
    "\n",
    "    return df.applymap(lambda x: ''.join(filter(str.isdigit, str(x))) if isinstance(x, (int,float)) else ''.join(filter(str.isdigit, x)) )\n",
    "gold = keep_numeric(gold)\n",
    "goldnp = np.array(gold, dtype = 'int')\n",
    "#subtract 1 from each index to match python index\n",
    "goldnp[:,0] = goldnp[:,0] - 1\n",
    "goldnp[:,1] = goldnp[:,1] - 1\n",
    "\n",
    "goldIm = np.zeros(shape=(NUM_TARGETS,NUM_TARGETS))\n",
    "\n",
    "for g in goldnp:\n",
    "    reg = g[0]\n",
    "    tar = g[1]\n",
    "    connection = g[2]\n",
    "    goldIm[reg][tar] = connection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(goldIm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero = np.nonzero(goldIm) #non-zero hold all the true connections\n",
    "#non_zero = np.array(np.unravel_index(non_zero, shape = (NUM_PARENTS, NUM_TARGETS)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGM and RGM+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulator_gene_matrix = goldIm.astype(np.float32) #I set rgm to gold because I didnt not want ot re-write everything\n",
    "superParent = regulator_gene_matrix.copy()\n",
    "ones = np.ones((NUM_TARGETS))\n",
    "parentIndex = []\n",
    "not_parentIndex = []\n",
    "for i in range(len(regulator_gene_matrix)):\n",
    "    if (np.isin(regulator_gene_matrix[i], [1])).any():\n",
    "        #print(i)\n",
    "        superParent[i] = ones \n",
    "        parentIndex.append(i)\n",
    "    else:\n",
    "        not_parentIndex.append(i)\n",
    "\n",
    "parentIndex = np.array(parentIndex)\n",
    "parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "parent_idx = parentIndex.numpy()\n",
    "not_parentIndex = np.array(not_parentIndex)\n",
    "not_parentIndex = tf.convert_to_tensor(not_parentIndex)\n",
    "print(\"shape of parent index\", parentIndex.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#the code below runs the experiment without any known regulators\n",
    "regulator_gene_matrix = np.ones((NUM_TARGETS,NUM_TARGETS), dtype='float32') #we keep this because we want to not mess with the weight init\n",
    "# superParent = np.ones((NUM_TARGETS, NUM_TARGETS), dtype = 'float32')\n",
    "# parentIndex = np.arange(stop=NUM_PARENTS)\n",
    "# parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "# parent_idx = parentIndex.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(superParent, cmap='hot');\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(regulator_gene_matrix, cmap='hot');\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superParent = regulator_gene_matrix.copy() #init the super parent with the ordinary RGM, and do forward passes with super parent\n",
    "# #print(superParent.shape)\n",
    "\n",
    "# ones = np.ones((NUM_TARGETS))\n",
    "# parentIndex = []\n",
    "# not_parentIndex = []\n",
    "# for i in range(len(regulator_gene_matrix)):\n",
    "#     if (np.isin(regulator_gene_matrix[i], [1])).any():\n",
    "#         #print(i)\n",
    "#         superParent[i] = ones \n",
    "#         parentIndex.append(i)\n",
    "#     else:\n",
    "#         not_parentIndex.append(i)\n",
    "\n",
    "# parentIndex = np.array(parentIndex)\n",
    "# parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "# parent_idx = parentIndex.numpy()\n",
    "# not_parentIndex = np.array(not_parentIndex)\n",
    "# not_parentIndex = tf.convert_to_tensor(not_parentIndex)\n",
    "# print(\"shape of parent index\", parentIndex.shape)\n",
    "\n",
    "def ignore_noParent_MSE_old(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "    y_true_pruned = tf.gather(y_true,parentIndex, axis =2) \n",
    "    #print(y_true_pruned.shape\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis =2)   \n",
    "    return l(y_true_pruned, y_pred_pruned)\n",
    "\n",
    "#this will not work if the entire dataset is -1 (degenerate), or has only one actual value (also degen)\n",
    "def ignore_noParent_MSE(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "   # print(y_true.shape) #(None, 44, 372)\n",
    "\n",
    "    #get the parents and flatten them\n",
    "    y_true_pruned = tf.gather(y_true, parentIndex, axis = 2) #axis 2 because batch, time, gene\n",
    "    y_true_pruned = tf.reshape(y_true_pruned, shape=([tf.size(y_true_pruned)] ) )\n",
    "\n",
    "   # print(y_true_pruned.shape)\n",
    "   # print(\"tf size\", tf.size(y_true_pruned))\n",
    "\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis = 2) \n",
    "    y_pred_pruned = tf.reshape(y_pred_pruned, shape=([tf.size(y_pred_pruned)]) )\n",
    "\n",
    "    #get the index of the parents which are not -1\n",
    "    y_true_posID = tf.where(y_true_pruned >= 0) #gets args\n",
    "    y_true_posID = tf.squeeze(y_true_posID)\n",
    "    #get the idx of all the -1s \n",
    "    y_true_negID = tf.where(y_true_pruned < 0) \n",
    "    y_true_negID = tf.squeeze(y_true_negID)\n",
    "\n",
    "    #get all the -1s in the parents \n",
    "    y_true_neg = tf.gather(y_true_pruned, y_true_negID) #get all the -1s in y_true\n",
    "    y_pred_neg = tf.gather(y_pred_pruned, y_true_negID) #get the corresponding values for y_pred\n",
    "\n",
    "    #get the indexes where pred should be -1 but is not. get the corresponding index for ytrue\n",
    "    y_shouldBeNegButIsntID = tf.where(y_pred_neg >= 0)  \n",
    "    y_shouldBeNegButIsntID = tf.squeeze(y_shouldBeNegButIsntID) #get the idx which should be -1 for prediction but are not\n",
    "    y_true_wrong = tf.gather(y_true_pruned, y_shouldBeNegButIsntID) #get the same corresponding values from ytrue\n",
    "    y_shouldBeNegButIsnt = tf.gather(y_pred_pruned, y_shouldBeNegButIsntID) #this has all the wrongly predicted values which should be -1 but are not\n",
    "\n",
    "    y_true_pos = tf.gather(y_true_pruned, y_true_posID)\n",
    "    y_pred_pos = tf.gather(y_pred_pruned, y_true_posID)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0. \n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 1: #dim goes away if size = 1. \n",
    "        y_shouldBeNegButIsnt = tf.expand_dims(y_shouldBeNegButIsnt, axis = 0) #should all be flattened\n",
    "        y_true_wrong = tf.expand_dims(y_true_wrong, axis=0)\n",
    "\n",
    "    #print(\"y_pred\", (y_pred_pos), \"y_true\", (y_shouldBeNegButIsnt))\n",
    "    try:\n",
    "        y_pred_total = tf.concat([y_pred_pos, y_shouldBeNegButIsnt], axis = 0) #concatenate for total mse\n",
    "        y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\n",
    "    except Exception as e:\n",
    "        print(y_pred_pos.shape, y_shouldBeNegButIsnt.shape, tf.size(y_shouldBeNegButIsnt))\n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "    return l(y_true_total, y_pred_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yhzyikoFncq"
   },
   "outputs": [],
   "source": [
    "# class EncoderLinear(tf.keras.layers.Layer):\n",
    "#     def __init__(self, rgm, input_dim=32, units=32):\n",
    "#         super(EncoderLinear, self).__init__()\n",
    "#         self.rgm = rgm\n",
    "        \n",
    "#         def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "#             w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.rgm, dtype=dtype)\n",
    "\n",
    "#             return w_init\n",
    "        \n",
    "\n",
    "#         self.w = tf.Variable(\n",
    "#             initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "#             trainable=True,\n",
    "#         )\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         X = inputs\n",
    "#         return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uchxU-bmBDe"
   },
   "outputs": [],
   "source": [
    "# class DecoderLinear(tf.keras.layers.Layer):\n",
    "#     def __init__(self, rgm, input_dim=32, units=32):\n",
    "#         super(DecoderLinear, self).__init__()\n",
    "#         self.rgm = rgm\n",
    "\n",
    "#         def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "#             w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "#             return w_init\n",
    "    \n",
    "        \n",
    "#         self.w = tf.Variable(\n",
    "#             initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "#             trainable=True,\n",
    "#         )\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         X = inputs\n",
    "#         #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "#         X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "#         #return tf.matmul(inputs, self.w)\n",
    "#         # v = tf.zeros_like(X)\n",
    "#         # u = tf.ones_like(X)\n",
    "#         # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "#         return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQVz4BCpmNMd"
   },
   "outputs": [],
   "source": [
    "# def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "#     '''\n",
    "#     Encoder structure\n",
    "#     '''\n",
    "#     '''\n",
    "#     The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "#     the intensities for each gene.\n",
    "#     '''\n",
    "#     en_conv = Conv1D(490, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association)\n",
    "#     en_dense = Flatten()(en_conv)\n",
    "#     phenotype = Dense(num_hidden_units)(en_dense)\n",
    "#     return phenotype\n",
    "\n",
    "# def decoder(X, num_protein_gene, time_steps):\n",
    "#     '''\n",
    "#     Decoder structure\n",
    "#     '''\n",
    "#     de_dense = Dense(1024)(X)#Dense(128)(X)\n",
    "#     de_dense = Reshape((1, 1024))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "#     de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "#     #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "#     # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "#     return de_deconv\n",
    "\n",
    "# def model(rgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 32):\n",
    "#     inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "#     x = EncoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "#     enc = encoder(x, num_hidden_units)\n",
    "#     dec = decoder(enc, num_protein_gene, time_steps)\n",
    "#     out = DecoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "#     _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "#     return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinaryAE = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS)\n",
    "# ordinaryAE.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = ordinaryAE.fit(beanIntensities, beanIntensities, epochs=200, verbose = True, validation_data=(validation, validation))\n",
    "#print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLinearSuperParent(tf.keras.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(EncoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "\n",
    "\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.OGrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def get_prunable_weights(self):\n",
    "        # Prune bias also, though that usually harms model accuracy too much.\n",
    "        return [self.w]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "class DecoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DecoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        #return tf.matmul(inputs, self.w)\n",
    "        # v = tf.zeros_like(X)\n",
    "        # u = tf.ones_like(X)\n",
    "        # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "        return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        \n",
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    rnn = LSTM(units = num_hidden_units)(parent_child_biological_association)\n",
    "    en_conv = Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(128)(X)\n",
    "    de_dense = Reshape((1, 128))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelSuperParent(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21, sparsity = 0.0): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = tfmot.sparsity.keras.prune_low_magnitude(EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS),\n",
    "                                                 pruning_schedule = tfmot.sparsity.keras.ConstantSparsity(sparsity, 0))(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSuperParentSequential(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    m = tf.keras.Sequential()\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "looseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "o = looseParent.fit(beanIntensities, beanIntensities, epochs=200, verbose = True,  \n",
    "                    validation_data=(validation, validation), callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()])\n",
    "print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looseParent.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNetAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a second copy of the layers which will be modified to be a denseNET auto encoder\n",
    "'''\n",
    "\n",
    "class DAE_Encoder_MASK(tf.keras.layers.Layer): \n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DAE_Encoder_MASK, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.oldrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "\n",
    "class DAE_Decoder_MASK(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DAE_Decoder_MASK, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.oldrgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) #used to have a transpose\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "        \n",
    "\n",
    "def denseencoder2(parent_child_biological_association, inp, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(NUM_TARGETS, 7, activation = \"tanh\")(parent_child_biological_association) # Conv1D(NUM_TARGETS, NUM_TIME_STEPS, activation = \"tanh\")(parent_child_biological_association)6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    inp = Flatten()(inp)\n",
    "    d = Concatenate()([en_dense, inp]) #dense layer\n",
    "    phenotype = Dense(num_hidden_units, activation=\"tanh\")(d)\n",
    "    return phenotype\n",
    "\n",
    "def densedecoder2(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(NUM_TARGETS, activation = 'tanh')(X)\n",
    "    de_dense = Reshape((1, NUM_TARGETS))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"tanh\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelDense2(rgm, oldrgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = DAE_Encoder_MASK(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    #x = EncoderLinear2(x)\n",
    "    enc = denseencoder2(x, inp, num_hidden_units)\n",
    "    dec = densedecoder2(enc, num_protein_gene, time_steps)\n",
    "    out = DAE_Decoder_MASK(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, num_hidden_units=NUM_PARENTS)\n",
    "dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "dense.fit(beanIntensities, beanIntensities, epochs=150,  verbose=True, validation_data=(validation, validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = dense.predict(validation)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(o[0].reshape(NUM_TIME_STEPS,NUM_TARGETS))\n",
    "v = pd.DataFrame(validation[0].reshape(NUM_TIME_STEPS,NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_noParent_MSE(validation, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Size Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(NUM_PARENTS, NUM_TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #to test on entire DS: \n",
    "# N = 50\n",
    "# hidden = np.arange(1,NUM_PARENTS+1, 1) #range(1,32)\n",
    "# lossMatrix = []\n",
    "# for i in tqdm(range(N)):\n",
    "    \n",
    "#     losses = []\n",
    "#     for value in (hidden):\n",
    "#         dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, value)\n",
    "#         dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "#         dense.fit(allData, allData, epochs=40,  verbose=0)\n",
    "#         test_hat = dense(allData) #, verbose = 0)\n",
    "#         loss = ignore_noParent_MSE(allData, test_hat)\n",
    "#         losses.append(loss)\n",
    "#         tf.keras.backend.clear_session()\n",
    "#     lossMatrix.append(losses)\n",
    "    \n",
    "# lossMatrix = np.array(lossMatrix)\n",
    "# #run 100 times \n",
    "\n",
    "\n",
    "#To test on Test set:\n",
    "# N = 40\n",
    "# hidden = np.arange(2,24, 1) #range(1,32)\n",
    "# lossMatrix = []\n",
    "# for i in tqdm(range(N)):\n",
    "    \n",
    "#     losses = []\n",
    "#     for value in (hidden):\n",
    "#         dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, value)\n",
    "#         dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "#         dense.fit(beanIntensities, beanIntensities, epochs=40,  verbose=0)\n",
    "#         test_hat = dense(validation) #, verbose = 0)\n",
    "#         loss = ignore_noParent_MSE(validation, test_hat)\n",
    "#         losses.append(loss)\n",
    "#         tf.keras.backend.clear_session()\n",
    "#     lossMatrix.append(losses)\n",
    "    \n",
    "# lossMatrix = np.array(lossMatrix)\n",
    "# #run 100 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avgMSE = np.average(lossMatrix, axis = 0)\n",
    "# plt.plot(hidden, avgMSE);\n",
    "# plt.xlabel(\"Latent Space Size\",fontsize=15);\n",
    "# plt.ylabel(\"MSE\",fontsize=15);\n",
    "# plt.title(\"Glycine Max: MSE with Respect to the Latent Space Size\", fontweight='bold', fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = pd.DataFrame(lossMatrix)\n",
    "# lm.to_csv(\"lossmatrix4lisa_dream4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.read_csv(\"lossmatrix4lisa_dream4.csv\").to_numpy()\n",
    "# print(temp.shape)\n",
    "# avgMSE = np.average(lossMatrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(hidden, avgMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, NUM_PARENTS)\n",
    "dense.compile(optimizer=tf.keras.optimizers.Adam(), loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array(dense.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def do_lazy_train(epochs, hidden = NUM_PARENTS):\n",
    "\n",
    "    '''trains the auto encoder epoch by epoch and returns the weights of the first layer'''\n",
    "    ep = epochs\n",
    "    #hidden = [hidden,] #range(1,32)\n",
    "    lossMatrix = []\n",
    "    lazy_weights = []\n",
    "\n",
    "    dense = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, hidden)\n",
    "    dense.compile(optimizer=tf.keras.optimizers.Adam(), loss=ignore_noParent_MSE)\n",
    "    \n",
    "    # dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, num_hidden_units=hidden)\n",
    "    # dense.compile(optimizer=tf.keras.optimizers.Adam(), loss=ignore_noParent_MSE)\n",
    "\n",
    "    #for i in tqdm(range(ep)):\n",
    "    for i in range(ep):\n",
    "        dense.fit(allData, allData, validation_data=(validation, validation), epochs=1,  verbose=0, callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()])\n",
    "        lazy_weights.append(dw := dense.get_weights()[0])\n",
    "        test = dense(validation) #, verbose = 0)\n",
    "        loss = ignore_noParent_MSE(validation, test)\n",
    "        lossMatrix.append(loss)\n",
    "        \n",
    "    lossMatrix = np.array(lossMatrix) #loss matrix no longer returns anything \n",
    "    lazy_weights = np.array(lazy_weights)\n",
    "    # print(\"lazy weights\", lazy_weights.shape)\n",
    "\n",
    "    return lossMatrix, lazy_weights #loss matrix no longer returns anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://proceedings.mlr.press/v162/rachwan22a/rachwan22a.pdf Winning the Lottery Ticket Ahead of Time:\n",
    "def lazyKernelRegime(w, parent_idx=parent_idx):\n",
    "    '''compute when each weight enters lazy kernel regime'''\n",
    "    \n",
    "    # print(\"wshape\", w.shape)\n",
    "\n",
    "    firstLayer = [] \n",
    "    for i in range(len(w)):\n",
    "         firstLayer.append(w[i]) #firstLayer.append(w[i][parent_idx])\n",
    "\n",
    "    fL = np.array(firstLayer)    \n",
    "    # print(\"print fL\", fL.shape, fL.dtype, fL[0], fL[1],)\n",
    "    d0 = np.square(fL[1] - fL[0])\n",
    "    # print(\"do\", d0.shape, d0)\n",
    "    \n",
    "    kernelChange = []\n",
    "    for i in range(1,len(fL)):\n",
    "        dt = np.square(fL[i] - fL[0])\n",
    "        dt_minus1 = np.square(fL[i-1] - fL[0])   \n",
    "        d = np.abs(dt - dt_minus1)/d0                        #eq 11 from the paper\n",
    "        kernelChange.append(d)\n",
    "    \n",
    "    kernelChange = np.moveaxis(kernelChange, 0, 2)\n",
    "    # plt.plot(kernelChange[0][0]);\n",
    "    # plt.title(\"$|\\Delta W|$ vs Epoch\")\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"$|\\Delta W|$\")\n",
    "\n",
    "    return np.array(kernelChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: try setting top parents to num epochs trained rather than culling process\n",
    "def compute_distrib(change, t, raw = False): #raw means return the unshaped indicies. \n",
    "    stop = []\n",
    "\n",
    "    z = np.zeros(shape=(NUM_TARGETS, NUM_TARGETS))\n",
    "\n",
    "\n",
    "    for parent in range(len(change)):\n",
    "        for child in range(len(change[0])):\n",
    "            try:\n",
    "                z[parent][child] = (np.min(np.argwhere(change[parent][child] < t)))\n",
    "            except ValueError:\n",
    "                stop.append(len(change))\n",
    "\n",
    "    return z \n",
    "    \n",
    "    stop = np.array(stop)\n",
    "    # print(\"stop shape\", stop.shape)\n",
    "    # print(stop.shape)\n",
    "    var = np.std(stop)\n",
    "    # print(var)\n",
    "    # assert(False)\n",
    "    mean = np.average(stop)\n",
    "   # print(\"mean, variance\", mean, var)\n",
    "    top_parents = np.argwhere(stop > (mean + 0.5*var)) #get the parents which take more than 2 stds to stop training\n",
    "    # print(\"top parents \", top_parents)\n",
    "    # assert(False)\n",
    "    top_parent_child = []\n",
    "    for tp in top_parents:\n",
    "        top_parent_child.append(np.unravel_index(tp, shape = (NUM_PARENTS, NUM_TARGETS)))\n",
    "\n",
    "    if raw == False:\n",
    "        plt.hist(stop)\n",
    "        plt.xlabel(\"Epoch where regulator-target-weight began changing by at most \"+ str(t));\n",
    "        plt.ylabel(\"Number of parent-child-weights\");\n",
    "        plt.title(\"Histogram of weight stops\");\n",
    "        print(\"average stop: \", mean);\n",
    "\n",
    "    if raw == True:\n",
    "        return np.array(top_parents)\n",
    "\n",
    "    return np.array(top_parent_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distrib_old(change, t, raw = False): #raw means return the unshaped indicies. \n",
    "    stop = []\n",
    "\n",
    "    for parent in range(len(change)):\n",
    "        for child in range(len(change[0])):\n",
    "            try:\n",
    "                stop.append(np.min(np.argwhere(change[parent][child] < t)))\n",
    "            except ValueError:\n",
    "                stop.append(len(change))\n",
    "                # print(parent, child)\n",
    "                # plt.plot(change[parent][child])\n",
    "                # assert(False)\n",
    "\n",
    "    \n",
    "    stop = np.array(stop).flatten()\n",
    "    # print(\"stop shape\", stop.shape)\n",
    "    # print(stop.shape)\n",
    "    var = np.std(stop)\n",
    "    # print(var)\n",
    "    # assert(False)\n",
    "    mean = np.average(stop)\n",
    "   # print(\"mean, variance\", mean, var)\n",
    "    top_parents = np.argwhere(stop > (mean + 0.5*var)) #get the parents which take more than 2 stds to stop training\n",
    "    # print(\"top parents \", top_parents)\n",
    "    # assert(False)\n",
    "    top_parent_child = []\n",
    "    for tp in top_parents:\n",
    "        top_parent_child.append(np.unravel_index(tp, shape = (NUM_PARENTS, NUM_TARGETS)))\n",
    "\n",
    "    if raw == False:\n",
    "        plt.hist(stop)\n",
    "        plt.xlabel(\"Epoch where regulator-target-weight began changing by at most \"+ str(t));\n",
    "        plt.ylabel(\"Number of parent-child-weights\");\n",
    "        plt.title(\"Histogram of weight stops\");\n",
    "        print(\"average stop: \", mean);\n",
    "\n",
    "    if raw == True:\n",
    "        return np.array(top_parents)\n",
    "\n",
    "    return np.array(top_parent_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazyKernels(N):\n",
    "\n",
    "    candidates = []\n",
    "    final_w = []\n",
    "    for i in tqdm(range(N)):\n",
    "        lm, lazy_weights = do_lazy_train(epochs=100)\n",
    "        # print(\"lm\", lm)\n",
    "        change = lazyKernelRegime(lazy_weights)\n",
    "        # print(\"change\",change.shape)\n",
    "        # assert(False)\n",
    "        top_pr = np.squeeze(compute_distrib(change, t = 0.2, raw=True))\n",
    "        candidates.append(top_pr)\n",
    "        final_w.append(lazy_weights[-1]) #final w is used for magntiude calculations at the end\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    final_w = np.array(final_w)\n",
    "    firstLayer = []\n",
    "    for i in range(len(final_w)):\n",
    "        firstLayer.append(np.abs(final_w[i])) #firstLayer.append(np.abs(final_w[i][parent_idx]))\n",
    "    fw = np.array(firstLayer)\n",
    "    candidates = np.array(candidates)\n",
    "\n",
    "    fw_avg = np.average(fw, axis = 0)\n",
    "    stop_avg = np.average(candidates, axis=0) \n",
    "\n",
    "    return stop_avg, fw_avg\n",
    "    #print(fw_avg.shape)\n",
    "    \n",
    "    candidates = np.hstack(candidates)\n",
    "    candidates = candidates.reshape(candidates.size)\n",
    "    #print(candidates.shape)\n",
    "\n",
    "    # plt.hist(candidates, bins=np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    # plt.title(\"Parent-Child Regulator Histogram\")\n",
    "    # plt.xlabel(\"Parent-Child Weight\")\n",
    "    # plt.ylabel(\"Num-Times parent-child relationship trained for top 5% of time\")\n",
    "    return candidates, fw_avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can, mag = lazyKernels(N = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv(\"DREAM4_GoldStandard_InSilico_Size100_1.csv\", header=None)\n",
    "\n",
    "def keep_numeric(df):\n",
    "\n",
    "    return df.applymap(lambda x: ''.join(filter(str.isdigit, str(x))) if isinstance(x, (int,float)) else ''.join(filter(str.isdigit, x)) )\n",
    "gold = keep_numeric(gold)\n",
    "goldnp = np.array(gold, dtype = 'int')\n",
    "#subtract 1 from each index to match python index\n",
    "goldnp[:,0] = goldnp[:,0] - 1\n",
    "goldnp[:,1] = goldnp[:,1] - 1\n",
    "\n",
    "goldIm = np.zeros(shape=(NUM_TARGETS,NUM_TARGETS))\n",
    "\n",
    "for g in goldnp:\n",
    "    reg = g[0]\n",
    "    tar = g[1]\n",
    "    connection = g[2]\n",
    "    goldIm[reg][tar] = connection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.multiply(can, mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(goldIm)\n",
    "plt.title('True RT Combos');\n",
    "print(sum(sum(goldIm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscore = np.nonzero(score > score.mean() + 0.8*score.std())\n",
    "tscoreIm = np.zeros(shape=goldIm.shape) \n",
    "tscoreIm[tscore] = 1\n",
    "plt.imshow(tscoreIm);\n",
    "plt.title('Score Out');\n",
    "print(sum(sum(tscoreIm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(goldIm-tscoreIm));\n",
    "plt.title(\"Difference between Gold and our score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(np.abs(goldIm-tscoreIm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmag = np.nonzero(mag > mag.mean() + 3.0*mag.std())\n",
    "tmagIm = np.zeros(shape=goldIm.shape) \n",
    "tmagIm[tmag] = 1\n",
    "plt.imshow(tmagIm);\n",
    "plt.title('Magnitude Out');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: do box plot of scores and compare with score of goldIM\n",
    "#TODO: Try shapley tf1 version\n",
    "plt.boxplot(score[np.nonzero(goldIm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load this with the gold IM mask after copying the weights of an old auto encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "looseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "o = looseParent.fit(beanIntensities, beanIntensities, epochs=200, verbose = True,  validation_data=(validation, validation),\n",
    "                    callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()])\n",
    "print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32, sparsity=0.98)\n",
    "sparseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "o = sparseParent.fit(beanIntensities, beanIntensities, epochs=200, verbose = False,  validation_data=(validation, validation),\n",
    "                    callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()])\n",
    "print(ignore_noParent_MSE(validation, sparseParent.predict(validation)))\n",
    "\n",
    "\n",
    "plt.imshow(tfMag := sparseParent.get_weights()[0])\n",
    "tfmag = np.nonzero(tfMag > tfMag.mean() + 0.0*tfMag.std())\n",
    "tfmagIm = np.zeros(shape=goldIm.shape) \n",
    "tfmagIm[tfmag] = 1\n",
    "plt.imshow(tfmagIm);\n",
    "plt.title('TF Magnitude Mask');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning_params = {\n",
    "#     'pruning_schedule': PolynomialDecay(initial_sparsity=0.5,\n",
    "#         final_sparsity=0.9824, begin_step=1000, end_step=2000),\n",
    "#     'block_size': (2, 3),\n",
    "#     'block_pooling_type': 'MAX'\n",
    "# }\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#     Dense(10, activation='relu', input_shape=(100,)),\n",
    "#     prune_low_magnitude(layers.Dense(2, activation='tanh'), **pruning_params)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldMask = 1-goldIm\n",
    "goldMask = goldMask.astype(np.float32)\n",
    "plt.imshow(goldMask);\n",
    "plt.title(\"Mask created by true R-T\");\n",
    "sum(sum(goldMask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscoreMask = -1.0*tscoreIm + 1.0 \n",
    "tscoreMask = tscoreMask.astype(np.float32)\n",
    "plt.imshow(tscoreMask);\n",
    "plt.title(\"Mask created by our score\");\n",
    "sum(sum(tscoreMask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmagMask = -1.0*tmagIm + 1.0 \n",
    "tmagMask = tmagMask.astype(np.float32)\n",
    "plt.imshow(tmagMask);\n",
    "plt.title(\"Mask created by AE magntidue\");\n",
    "print(sum(sum(tmagMask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmagAC = modelSuperParent(tmagMask, tmagMask, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "tmagAC.set_weights( looseParent.get_weights() )\n",
    "tmagACout = tmagAC(validation) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(validation, tmagACout)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreAC = modelSuperParent(tscoreMask, tscoreMask, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "scoreAC.set_weights(looseParent.get_weights())\n",
    "scoreACout = scoreAC(validation) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(validation, scoreACout)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldAC = modelSuperParent(goldMask, goldMask, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "goldAC.set_weights(looseParent.get_weights())\n",
    "goldout = goldAC(validation) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(validation, goldout)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ignore_noParent_MSE(validation, looseParent.predict(validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(goldIm[parent_idx].flatten(), tscoreIm[parent_idx].flatten(), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print('true negative, false positive')\n",
    "print('false negative, true positive') \n",
    "print(confusion_matrix(goldIm[parent_idx].flatten(), tscoreIm[parent_idx].flatten())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    " \n",
    "# set width of bar\n",
    "barWidth = 0.15\n",
    "#fig = plt.subplots(figsize =(12, 8))\n",
    " \n",
    "# set height of bar\n",
    "IT = [0.019060984] #base\n",
    "score = [0.023739332]\n",
    "mag = [0.021395445]\n",
    "\n",
    "ECE = [0.024257105 ]  #gold\n",
    " \n",
    " \n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(IT))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "br3 = [x + barWidth for x in br2]\n",
    "br4 = [x + barWidth for x in br3]\n",
    "\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(br1, IT, color ='cyan', width = barWidth,\n",
    "        edgecolor ='grey', label ='Control')\n",
    "plt.bar(br2, score, color ='blue', width = barWidth,\n",
    "        edgecolor ='grey', label ='OBD (us)')\n",
    "plt.bar(br3, mag, color ='red', width = barWidth,\n",
    "        edgecolor ='grey', label ='Magnitude Pruning')\n",
    "plt.bar(br4, ECE, color ='green', width = barWidth,\n",
    "        edgecolor ='grey', label ='Gold Standard')\n",
    "# plt.bar(br3, CSE, color ='b', width = barWidth,\n",
    "#         edgecolor ='grey', label ='CSE')\n",
    " \n",
    "# Adding Xticks\n",
    "plt.xlabel('Mask', fontweight ='bold', fontsize = 15)\n",
    "plt.ylabel('MSE', fontweight ='bold', fontsize = 15)\n",
    "plt.xticks([r + barWidth for r in range(len(IT))],\n",
    "        [''])\n",
    " \n",
    "plt.title(\"MSE on the Validation Set\", fontweight='bold', fontsize = 18);\n",
    "plt.legend(loc = 'upper left');\n",
    "plt.ylim(0.018, 0.028)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(tfMag := sparseParent.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfmag = np.nonzero(np.abs(tfMag) > np.abs(tfMag)+0.01)\n",
    "tfmagIm = np.zeros(shape=goldIm.shape) \n",
    "tfmagIm[tmag] = 1\n",
    "plt.imshow(tfmagIm);\n",
    "plt.title('Mask created by AE Magnitude. TF Implementation');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash Under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import scipy\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "X = scipy.signal.resample_poly(allData[0], up = 5, down = 1, axis = 0)\n",
    "print(X.shape)\n",
    "transformer = FactorAnalysis(n_components=41, random_state=0)\n",
    "X_transformed = transformer.fit_transform(X.T)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(allData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transformer.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  val_loss: 0.0221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldIm[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.cov(score, goldIm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.cov(goldIm, score)[100:, 100:])\n",
    "#[100:, 100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.vstack([goldIm.flatten(), score.flatten()])\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(score);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score[score > score.mean() + score.var()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf, bins = np.histogram(can, np.arange(NUM_PARENTS*NUM_TARGETS))\n",
    "# pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitudes After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS*NUM_TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unravel_index(NUM_PARENTS*NUM_TARGETS, shape = (NUM_PARENTS, NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf, bins = np.histogram(can, bins=NUM_PARENTS*NUM_TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf, bins = np.histogram(can, np.arange(0, NUM_PARENTS*NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(pdf, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: CHECK ME\n",
    "def get_top_reg_targets(can, mag):\n",
    "    \n",
    "    pdf, bins = np.histogram(can, np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    \n",
    "    pdf2d = np.zeros(shape = (NUM_PARENTS,NUM_TARGETS))\n",
    "\n",
    "    for i in bins:\n",
    "      idx2d = np.unravel_index(i, shape = (NUM_PARENTS, NUM_TARGETS))\n",
    "      try:\n",
    "        pdf2d[idx2d] = pdf[i]\n",
    "      except IndexError:\n",
    "        print(i, idx2d, len(pdf), len(bins), bins[-1])\n",
    "    \n",
    "\n",
    "    importance = np.multiply(pdf2d, mag)\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = get_top_reg_targets(can, mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(top)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: try factorial data, ordinary Auto encoder, treshold, tensorflow pruning, check average epoch of lazy kernel\n",
    "\n",
    "#TODO: Actually measure MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = pd.DataFrame(top)\n",
    "# lm.to_csv(\"top_r_t_HepG2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topdf = pd.read_csv('top_r_t_HepG2.csv')\n",
    "# topdf.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top = np.array(topdf)\n",
    "# plt.imshow(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top = top[:,1:]\n",
    "# plt.imshow(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(non_zero).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top[non_zero].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag[non_zero].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_connect = np.where(goldIm == 0)\n",
    "no_connect = np.array(no_connect)\n",
    "no_connect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top[no_connect].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag[no_connect].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag01 = MinMaxScaler().fit_transform(mag.reshape((-1,1)))\n",
    "tff = mag01.flatten()\n",
    "std = np.std(tff)\n",
    "\n",
    "mu = np.mean(tff)\n",
    "mu, std\n",
    "magt = np.where(top > mu + 2*std, 1, 0)\n",
    "magt.dtype = 'int'\n",
    "plt.imshow(magt.reshape(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(goldIm.flatten(), magt.flatten(), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top01 = MinMaxScaler().fit_transform(top.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(top01.reshape(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff = top01.flatten()\n",
    "std = np.std(tff)\n",
    "\n",
    "mu = np.mean(tff)\n",
    "print(mu, std)\n",
    "yout = np.where(top01.flatten() > mu + 4*std, 1, 0)\n",
    "yout.dtype = 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(yout.reshape(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(goldIm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(goldIm.flatten(), (yout).flatten(), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "PrecisionRecallDisplay.from_predictions(goldIm.flatten(), yout.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff = top.flatten()\n",
    "std = np.std(tff)\n",
    "\n",
    "mu = np.mean(tff)\n",
    "mu, std\n",
    "yout = np.where(top > mu + 3*std, 1, 0)\n",
    "yout.dtype = 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(goldIm.flatten(), yout.flatten(), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magX = StandardScaler().fit_transform(top.reshape((-1,1)))\n",
    "magX = MinMaxScaler().fit_transform(magX.reshape((-1,1)))\n",
    "you2 = np.where(magX > mu + 3*std, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(goldIm.flatten(), you2.flatten(), average='samples')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw old graphs; probably trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(top, cmap = 'inferno', vmin = 0, vmax = np.max(top));\n",
    "plt.colorbar(u ,fraction=0.026, pad=0.1, orientation='horizontal');\n",
    "plt.title(\"HepG2: Regulator-Target Combination Importance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topID = np.array(np.unravel_index(np.argsort(top, axis=None), top.shape))\n",
    "topID = np.flip(topID, axis=1)\n",
    "topID[0] = parent_idx[topID[0]]\n",
    "topID = topID.T\n",
    "topID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topR_T = pd.DataFrame(topID)\n",
    "topR_T.to_csv(\"DREAM4_Top_reg_target_decendingOrder_firstColIsRegulator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.zeros(NUM_PARENTS)\n",
    "for i in range(NUM_PARENTS):\n",
    "    best[i] = np.sum(top[i])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,NUM_PARENTS, dtype=int)\n",
    "plt.plot(x, best); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.flip(np.argsort(best))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(can).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Petal Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal = pd.read_excel(data_path_petal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train = petal[petal[\"Line\"] == \"WT\"]\n",
    "petal_train = petal_train.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_train.head(12)\n",
    "petal_train = petal_train.groupby(['Plate']).mean()\n",
    "petal_train.head()\n",
    "petal_train = petal_train.to_numpy()\n",
    "print(petal_train.shape)\n",
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(petal_train)\n",
    "petal_train = scaler1.transform(petal_train)\n",
    "mm = MinMaxScaler()\n",
    "mm.fit(petal_train)\n",
    "petal_train = mm.transform(petal_train)\n",
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test = petal[petal[\"Line\"] != \"WT\"]\n",
    "petal_test = petal_test.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_test = petal_test.groupby(['Plate']).mean()\n",
    "petal_test.head()\n",
    "petal_test = petal_test.to_numpy()\n",
    "print(petal_test.shape)\n",
    "petal_test = scaler1.transform(petal_test)\n",
    "petal_test = mm.transform(petal_test)\n",
    "petal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1.shape\n",
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densePredictor = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS, 22)\n",
    "densePredictor.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "densePredictor.fit(beanIntensities, beanIntensities,validation_data=(experiment1, experiment1),  epochs=100,  verbose=1)\n",
    "test = densePredictor(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgm = superParent\n",
    "time_steps = 6\n",
    "num_kinase_regulators = NUM_TARGETS\n",
    "num_hidden_units = 22\n",
    "\n",
    "inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "x = DenseEncoderLinear2(rgm, regulator_gene_matrix, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "enc = denseencoder2(x, inp, num_hidden_units)\n",
    "denseP = tf.keras.Model(inputs=inp, outputs=enc)\n",
    "#set the weights of the encoder to the weights of auto encoder\n",
    "dw = densePredictor.get_weights()\n",
    "enc_w = dw[0:5]\n",
    "denseP.set_weights(enc_w)\n",
    "#add a dense layer  because we are ouputing 1 number\n",
    "l = Dense(32, activation = 'swish', use_bias=True, kernel_regularizer='l1_l2')(denseP.layers[-1].output)\n",
    "l = Dense(1, activation = 'linear', use_bias = True)(l)\n",
    "denseP = tf.keras.Model(denseP.inputs, l)\n",
    "#denseP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = np.concatenate([experiment1, experiment1, experiment1, experiment1])\n",
    "bp.shape\n",
    "#bigexperiment1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = petal_train[0]\n",
    "b = petal_train[1]\n",
    "c = petal_train[2]\n",
    "d = petal_train[3]\n",
    "\n",
    "petal_train1 = np.array([a,a,a,a, b,b,b,b, c,c,c,c, d,d,d,d]) #does this make sense? we are training network to predict .5\n",
    "petal_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "denseP.fit(experiment1, petal_train, epochs=500, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP(experiment1) #experiment1 is part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(denseP(testCandidate)) #model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test #true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.evaluate(testCandidate, petal_test) #eval gave 1.3999 before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 16)               0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 6, 6, 16)         160       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 8, 8, 32)         4640      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 26, 26, 16)       4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 28, 28, 1)        145       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import json\n",
    "import shap\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "# encoder.summary()\n",
    "\n",
    "x = layers.Reshape((4, 4, 1))(encoder_output)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_16428\\1144032549.py:13: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (50, 224, 224, 3) for Tensor img_2:0, which has shape (None, 28, 28, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\dream4_100\\DreamEncoder4_100.ipynb Cell 207\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     feed_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m([model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minput], [preprocess_input(x\u001b[39m.\u001b[39mcopy())]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m K\u001b[39m.\u001b[39mget_session()\u001b[39m.\u001b[39mrun(model\u001b[39m.\u001b[39mlayers[layer]\u001b[39m.\u001b[39minput, feed_dict)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m e \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mGradientExplainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     (model\u001b[39m.\u001b[39mlayers[\u001b[39m7\u001b[39m]\u001b[39m.\u001b[39minput, model\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput),\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     map2layer(X, \u001b[39m7\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     local_smoothing\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m \u001b[39m# std dev of smoothing noise\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m shap_values,indexes \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mshap_values(map2layer(to_explain, \u001b[39m7\u001b[39m), ranked_outputs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# get the names for the classes\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\dream4_100\\DreamEncoder4_100.ipynb Cell 207\u001b[0m in \u001b[0;36mmap2layer\u001b[1;34m(x, layer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap2layer\u001b[39m(x, layer):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     feed_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m([model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minput], [preprocess_input(x\u001b[39m.\u001b[39mcopy())]))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m K\u001b[39m.\u001b[39;49mget_session()\u001b[39m.\u001b[39;49mrun(model\u001b[39m.\u001b[39;49mlayers[layer]\u001b[39m.\u001b[39;49minput, feed_dict)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:967\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    964\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 967\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    968\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    970\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:1164\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1160\u001b[0m   np_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(subfeed_val, dtype\u001b[39m=\u001b[39msubfeed_dtype)\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m is_tensor_handle_feed \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m     \u001b[39mnot\u001b[39;00m subfeed_t\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39mis_compatible_with(np_val\u001b[39m.\u001b[39mshape)):\n\u001b[1;32m-> 1164\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1165\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCannot feed value of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(np_val\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m for Tensor \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1166\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msubfeed_t\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m, which has shape \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1167\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(subfeed_t\u001b[39m.\u001b[39mget_shape())\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mis_feedable(subfeed_t):\n\u001b[0;32m   1169\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensor \u001b[39m\u001b[39m{\u001b[39;00msubfeed_t\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m may not be fed.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (50, 224, 224, 3) for Tensor img_2:0, which has shape (None, 28, 28, 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "X,y = shap.datasets.imagenet50()\n",
    "to_explain = X[[39,41]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test set and the synthetic dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestSet(test_path):\n",
    "    testFiles = []\n",
    "    for np_name in glob(os.path.join(data_path_testSet,'*.np[yz]')):\n",
    "        k = np.load(os.path.join(data_path_testSet,np_name))\n",
    "        testFiles.append(k)\n",
    "#         print(np_name)\n",
    "#         print(k.shape)\n",
    "    return np.array(testFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PKjzdFCMwFg"
   },
   "outputs": [],
   "source": [
    "def read_files(data_path):\n",
    "\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        try:\n",
    "            #print('file name:',file)\n",
    "            #print('value of i:',i)\n",
    "            genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "            #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "            replicates.append(np.array(genes_intensities_data_matrix.values, dtype = float))\n",
    "            # i+=1\n",
    "        except PermissionError:\n",
    "            print(\"Not a CSV: \", os.path.join(data_path , file))\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    rep = np.array(replicates).astype(np.float32)\n",
    "    \n",
    "    return rep, rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCwo4LwlO_FF"
   },
   "outputs": [],
   "source": [
    "beanIntensities, regulator_gene_matrix= read_files(data_path_syn)\n",
    "matrix = regulator_gene_matrix\n",
    "replicates = beanIntensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros(shape = (3,6,8))\n",
    "id = np.unravel_index(3*6*8 - 1, shape = d.shape)\n",
    "d[id] = 1\n",
    "plt.imshow(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0][ : , parentIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)\n",
    "print(testCandidate.shape)\n",
    "syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(change[0][22])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"change in weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossMatrix)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change.shape, lossMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diff(change[0][22])\n",
    "plt.plot(d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change[0][0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_tresh(change, stop = 0.05):\n",
    "#     diffs = []\n",
    "#     for parent in range(len(change)):\n",
    "#         for child in range(len(change[0])):\n",
    "#             diffs.append(np.diff(change[parent][child]))\n",
    "#     inflection = []\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         for d in diffs:\n",
    "#             print(np.argwhere(np.abs(d) < stop))\n",
    "#             inflection.append(np.min(np.argwhere(np.abs(d) < stop))) #return where the second derivative is first 0. \n",
    "\n",
    "#     except ValueError:\n",
    "#         print(\"Stop value \", stop, \" is too high, trying stop = \", stop + 0.05)\n",
    "#         # s = stop + 0.05\n",
    "#         # return compute_tresh(change, stop = s)\n",
    "        \n",
    "\n",
    "#     return np.average(inflection)\n",
    "        \n",
    "# d = compute_tresh(change)\n",
    "# d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"mse3.npy\", avgMSE) #mse2/3 is with -1 fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sciencedirect.com/science/article/pii/S0925231220314570?casa_token=lcEJANqO0JwAAAAA:uL3DGUZctPUZz_sPz1K1i2klMtb83TyKnc9CI3_N-uSOaM7VHL8GhM0jCGYfo25NmpDQQ9Cvlw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rshp = Flatten()(looseParent.layers[-1].output)\n",
    "\n",
    "modelTemp = tf.keras.Model(inputs=looseParent.input, outputs = [rshp])\n",
    "modelTemp.summary()\n",
    "type(modelTemp)\n",
    "explainer = shap.DeepExplainer(modelTemp, syntheticDataTrain)\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough #this solves the \"shap_ADDV2\" problem but another one will appear\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"FusedBatchNormV3\"] = shap.explainers._deep.deep_tf.passthrough #this solves the next problem which allows you to run the DeepExplainer.\n",
    "\n",
    "shap_values = explainer.shap_values(testCandidate[0:1])\n",
    "def f(x):\n",
    "    return modelTemp.predict(x)\n",
    "\n",
    "print(f(testCandidate))\n",
    "explainer = shap.KernelExplainer(f , testCandidate[0:1], link=\"logit\") #svm.predict_proba, X_train, link=\"logit\")\n",
    "shap_values = explainer.shap_values(testCandidate[0:1], nsamples=100)\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import json\n",
    "import shap\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "X,y = shap.datasets.imagenet50()\n",
    "to_explain = X[[39,41]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(enc_dec_Synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [5,6]\n",
    "u = tf.concat([a,b], axis = 0)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newConnections = superParent - regulator_gene_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(newConnections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nC = []\n",
    "# for i in range(len(newConnections[0])):\n",
    "#     for j in range(len(newConnections[1])):\n",
    "#         if newConnections[i][j] > 0:\n",
    "#             nC.append([i,j])\n",
    "# nC = np.array(nC)\n",
    "# nC = pd.DataFrame(nC)\n",
    "# nC.to_csv(\"new_connections_in_superParents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code for testing loss function\n",
    "# print(outSyn.shape)\n",
    "# print(testCandidate.shape)\n",
    "# syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "# syntheticLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Auto Encoder\n",
    "Autoencoder has not been trained on synthetic version of experiement 1. We test on the original experiment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrRuJ_bsrHll"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAOZEEvRRVU4"
   },
   "outputs": [],
   "source": [
    "# enc_dec_Synthetic.compile(optimizer='adam',loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticDataTrain = beanIntensities[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1649273035573,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "EuNDZx5Ov34I",
    "outputId": "74597aa1-c71c-4491-b8c9-4fefed309325"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.fit(syntheticDataTrain,syntheticDataTrain,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = enc_dec_Synthetic(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = enc_dec_Synthetic.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w[0], cmap = \"hot\", vmin=0,vmax=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBzDmjqJViEw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we do not need to use this function for the testset\n",
    "def getCSVs(data_path_head):\n",
    "    PATH = data_path_head\n",
    "    EXT = \"*.csv\"\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "    actual = []\n",
    "    for p in all_csv_files:\n",
    "        actual.append(pd.read_csv(p, index_col = 0).to_numpy())\n",
    "    return np.array(actual)\n",
    "    \n",
    "experiment1 = getCSVs(data_path_og_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate = test.numpy().astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outSyn = enc_dec_Synthetic.predict(testCandidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymagn(A, B):\n",
    "    mse = (np.square(A - B)).mean(axis=None)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticLoss = ignore_noParent_MSE(testCandidate, outSyn )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(outSyn-testCandidate).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install keras-visualizer\n",
    "#!pip install pydot\n",
    "#data_path_og_exp1 = data_path_testSet \n",
    "# !pip install pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = loadTestSet(data_path_testSet)\n",
    "# testCandidate = test.astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPRV0OAMpKPH"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS) #we can just change the time steps to something higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1649273028683,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "D6aPT5_cpKK0",
    "outputId": "c261d824-0f38-4eee-b139-f03a80f093e1"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated dataset Auto Encoder\n",
    "Once again, we do not train on any version of exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filesV2(data_path):\n",
    "    '''\n",
    "    *Changed*\n",
    "    currently hardcoded for only one file. \n",
    "    change code a bit for reading multiple files.\n",
    "    '''\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        #print('file name:',file)\n",
    "        #print('value of i:',i)\n",
    "        genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "        #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "        replicates.append(genes_intensities_data_matrix.values)\n",
    "        # i+=1\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    \n",
    "    return np.asarray(replicates), rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes, _ = read_filesV2(data_path_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(interpolated_genes[2]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = []\n",
    "for k in range(len(interpolated_genes)):\n",
    "    #print(k)\n",
    "    if k == 2 or k == 3 or k == 4:\n",
    "        inter.append(np.reshape(interpolated_genes[k], (4,6,NUM_TARGETS)))\n",
    "    else: \n",
    "        inter.append(np.reshape(interpolated_genes[k], (5,6,NUM_TARGETS)))\n",
    "inter = np.vstack(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beanIntensities[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter = model(regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS) \n",
    "enc_dec_inter.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter.fit(inter, inter,epochs=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter = enc_dec_inter.predict(testCandidate)\n",
    "interpolationLoss = ignore_noParent_MSE(testCandidate, outInter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolationLoss #used to be 3.84 on broke ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = enc_dec_inter.history\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons between various outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(np.reshape((np.abs(outSyn)), (24,NUM_TARGETS)), cmap = \"hot\", vmin=0,vmax=1.0 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(outSyn[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(testCandidate[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape((np.abs(outSyn-outInter)), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 );\n",
    "plt.title(\"Difference Between the Outputs of Both Autoencoders\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"DiffBetweenOut.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outInter-experiment1), (24,NUM_TARGETS)) , cmap = \"hot\") #, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Interpolated Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30)\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"InterDiffImage.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outSyn-experiment1), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"SynDiffImage.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx = parentIndex.numpy()\n",
    "#print(parent_idx)\n",
    "oSyn = (np.reshape((outSyn), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "oSyn = oSyn.T\n",
    "oSyn.shape\n",
    "\n",
    "exp1_col = (np.reshape((experiment1), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "exp1_col = exp1_col.T\n",
    "print(exp1_col.shape)\n",
    "\n",
    "u = plt.imshow(np.abs(oSyn - exp1_col), cmap = 'hot') #TODO use TF loss function instead of difference.\n",
    "ddff = oSyn-exp1_col\n",
    "plt.colorbar(u)\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data. Only parents.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(oSyn).head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(exp1_col).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ddff).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"interpolated_v2.npy\", inter) #the interpolated dataset\n",
    "# np.save(\"synthetic_v2.npy\", beanIntensities[1:]) # the synthetic dataset\n",
    "# np.save(\"synOut_v2.npy\", outSyn) #the output of the encoder trained on synthetic data with the input being exp1\n",
    "# np.save(\"interOut_v2.npy\", outInter) #the output of the encoder trained on interpolated data with the input being exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6J5QSynTTgZ+lFOhgOtAG",
   "collapsed_sections": [],
   "name": "cnn-third.ipynb",
   "provenance": [
    {
     "file_id": "1mmRxO5jnBc5CdzxXj8sMtPpG0BQ0ulSs",
     "timestamp": 1648921397876
    },
    {
     "file_id": "10758zFj2UnTnwpQaSFIr5r9MqZWdiMsf",
     "timestamp": 1648674775255
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
