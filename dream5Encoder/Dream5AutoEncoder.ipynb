{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import synapseclient \n",
    "#  import synapseutils \n",
    " \n",
    "#  syn = synapseclient.Synapse() \n",
    "#  syn.login('finamintoastcrunch','1Hjldria!') \n",
    "#  files = synapseutils.syncFromSynapse(syn, ' syn2825306 ') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qTo_HuQkGgAq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_visualizer import visualizer \n",
    "\n",
    "from tensorflow.keras.layers import*\n",
    "import shap\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Conv1D\n",
    "# from keras.layers import Conv1DTranspose\n",
    "# from keras.layers import Flatten, Reshape\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Constants and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS = 334\n",
    "NUM_TARGETS = 4510\n",
    "NUM_TIME_STEPS = 805\n",
    "NUM_REPLICATES = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gXpISOijEYqQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# matrix_path = \"regulator-gene-matrix.csv\"\n",
    "# data_path_syn = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\synData\"\n",
    "# data_path_inter =  r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\interpolatedOnly\"\n",
    "# data_path_og_exp1 = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\datasets\\exp1\"\n",
    "# data_path_testSet = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\testSetFixed\"\n",
    "# data_path_petal = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\petal_len.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirty_RGM = r'Regulations_Control_Altona.csv' #For dream5, we dont have an RGM so we have to do things blindly... I think?\n",
    "dirty_regulations = r'net3_expression_data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(805, 4510)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dirtyRGM = pd.read_csv(dirty_RGM,  index_col = 0, )#on_bad_lines='skip')\n",
    "dirtyReg = pd.read_table(dirty_regulations,  index_col = 0,)# on_bad_lines='skip')\n",
    "dirtyReg = dirtyReg.select_dtypes(include=np.number)\n",
    "dirtyReg = dirtyReg.to_numpy()\n",
    "dirtyReg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.3293,  9.5997,  6.9998, ...,  6.4199,  7.3635,  9.9146],\n",
       "       [ 9.2212,  9.4961,  6.8697, ...,  6.7346,  7.3835,  8.6026],\n",
       "       [ 8.631 , 10.273 ,  7.0817, ...,  6.6771,  8.3439,  8.6646],\n",
       "       ...,\n",
       "       [ 8.0524, 10.778 ,  6.607 , ...,  7.573 ,  6.8951,  8.9295],\n",
       "       [ 8.3784, 11.008 ,  6.7755, ...,  6.5779,  7.2277,  8.7563],\n",
       "       [ 8.1178, 11.274 ,  6.5021, ...,  7.3586,  6.9848,  9.0106]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtyReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(805, 4510)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Fix this. it puts nans where it should not. \n",
    "\n",
    "def fix_dataset(dirtyR):\n",
    "    shape = dirtyR.shape\n",
    "    # dataset = np.zeros(shape=(dirtyR.shape))\n",
    "\n",
    "    tmp = dirtyR.flatten()\n",
    "    regScaled = MinMaxScaler().fit_transform(tmp.reshape((-1,1)))\n",
    "    ret = regScaled.reshape(shape)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "dataset = fix_dataset(dirtyR=dirtyReg)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 805, 4510) (1, 805, 4510)\n"
     ]
    }
   ],
   "source": [
    "#only one dataset so we set bean and etc all to the same \n",
    "ecoli_Intensities = np.array([dataset])\n",
    "validation = np.array([dataset])\n",
    "print(ecoli_Intensities.shape, validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the third replicate is trash. we will not use it. \n",
    "# df(dataset[3]).head(11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regulator_gene_matrix = np.load(\"soyBeanRGM.npy\")\n",
    "# regulator_gene_matrix = regulator_gene_matrix.astype('float32')\n",
    "# regulator_gene_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent Matrix + Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 333 genes are parents\n",
    "regulator_gene_matrix = np.ones((NUM_TARGETS,NUM_TARGETS), dtype='float32')\n",
    "regulator_gene_matrix[NUM_PARENTS:] = 0\n",
    "\n",
    "superParent = np.ones((NUM_TARGETS, NUM_TARGETS), dtype = 'float32')\n",
    "superParent[NUM_PARENTS:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentIndex = np.arange(stop=NUM_PARENTS)\n",
    "parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "parent_idx = parentIndex.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of parent index (334,)\n"
     ]
    }
   ],
   "source": [
    "# superParent = regulator_gene_matrix.copy() #init the super parent with the ordinary RGM, and do forward passes with super parent\n",
    "#print(superParent.shape)\n",
    "\n",
    "# ones = np.ones((NUM_TARGETS))\n",
    "# parentIndex = []\n",
    "# not_parentIndex = []\n",
    "# for i in range(len(regulator_gene_matrix)):\n",
    "#     if (np.isin(regulator_gene_matrix[i], [1])).any():\n",
    "#         #print(i)\n",
    "#         superParent[i] = ones \n",
    "#         parentIndex.append(i)\n",
    "#     else:\n",
    "#         not_parentIndex.append(i)\n",
    "\n",
    "# parentIndex = np.array(parentIndex)\n",
    "# parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "# parent_idx = parentIndex.numpy()\n",
    "# not_parentIndex = np.array(not_parentIndex)\n",
    "# not_parentIndex = tf.convert_to_tensor(not_parentIndex)\n",
    "print(\"shape of parent index\", parentIndex.shape)\n",
    "\n",
    "def ignore_noParent_MSE_old(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "    y_true_pruned = tf.gather(y_true,parentIndex, axis =2) \n",
    "    #print(y_true_pruned.shape\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis =2)   \n",
    "    return l(y_true_pruned, y_pred_pruned)\n",
    "\n",
    "#this will not work if the entire dataset is -1 (degenerate), or has only one actual value (also degen)\n",
    "def ignore_noParent_MSE(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "   # print(y_true.shape) #(None, 44, 372)\n",
    "\n",
    "    #get the parents and flatten them\n",
    "    y_true_pruned = tf.gather(y_true, parentIndex, axis = 2) #axis 2 because batch, time, gene\n",
    "    y_true_pruned = tf.reshape(y_true_pruned, shape=([tf.size(y_true_pruned)] ) )\n",
    "\n",
    "   # print(y_true_pruned.shape)\n",
    "   # print(\"tf size\", tf.size(y_true_pruned))\n",
    "\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis = 2) \n",
    "    y_pred_pruned = tf.reshape(y_pred_pruned, shape=([tf.size(y_pred_pruned)]) )\n",
    "\n",
    "    #get the index of the parents which are not -1\n",
    "    y_true_posID = tf.where(y_true_pruned >= 0) #gets args\n",
    "    y_true_posID = tf.squeeze(y_true_posID)\n",
    "    #get the idx of all the -1s \n",
    "    y_true_negID = tf.where(y_true_pruned < 0) \n",
    "    y_true_negID = tf.squeeze(y_true_negID)\n",
    "\n",
    "    #get all the -1s in the parents \n",
    "    y_true_neg = tf.gather(y_true_pruned, y_true_negID) #get all the -1s in y_true\n",
    "    y_pred_neg = tf.gather(y_pred_pruned, y_true_negID) #get the corresponding values for y_pred\n",
    "\n",
    "    #get the indexes where pred should be -1 but is not. get the corresponding index for ytrue\n",
    "    y_shouldBeNegButIsntID = tf.where(y_pred_neg >= 0)  \n",
    "    y_shouldBeNegButIsntID = tf.squeeze(y_shouldBeNegButIsntID) #get the idx which should be -1 for prediction but are not\n",
    "    y_true_wrong = tf.gather(y_true_pruned, y_shouldBeNegButIsntID) #get the same corresponding values from ytrue\n",
    "    y_shouldBeNegButIsnt = tf.gather(y_pred_pruned, y_shouldBeNegButIsntID) #this has all the wrongly predicted values which should be -1 but are not\n",
    "\n",
    "    y_true_pos = tf.gather(y_true_pruned, y_true_posID)\n",
    "    y_pred_pos = tf.gather(y_pred_pruned, y_true_posID)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0. \n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 1: #dim goes away if size = 1. \n",
    "        y_shouldBeNegButIsnt = tf.expand_dims(y_shouldBeNegButIsnt, axis = 0) #should all be flattened\n",
    "        y_true_wrong = tf.expand_dims(y_true_wrong, axis=0)\n",
    "\n",
    "    #print(\"y_pred\", (y_pred_pos), \"y_true\", (y_shouldBeNegButIsnt))\n",
    "    try:\n",
    "        y_pred_total = tf.concat([y_pred_pos, y_shouldBeNegButIsnt], axis = 0) #concatenate for total mse\n",
    "        y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\n",
    "    except Exception as e:\n",
    "        print(y_pred_pos.shape, y_shouldBeNegButIsnt.shape, tf.size(y_shouldBeNegButIsnt))\n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "    return l(y_true_total, y_pred_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(superParent, cmap='hot');\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAFKCAYAAACzRfzTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcUUlEQVR4nO3de5hdVX3/8fcnIYAkglBIMkiQcAmh2AoSCUkoJkgwgA8/9WktUEGrNYI0PyqKlYuKIkSxgNwUwyWoyA9BBRRKCKEGm4uUREBJ+aVCAxJJHkgDBHLBhHz7x9oHdg5nJuckZ9aZOfN5Pc88M2ft7+xZa4gf96y9zl6KCMzMrHv1a3UHzMz6AoetmVkGDlszswwctmZmGThszcwycNiamWXgsDUzy6Ctw1bSsZIekfSqpKckndnqPplZ39S2YStpFHAnMAM4CDgfuEjSqS3slpn1UWrXd5BJuhnYKyLGltq+Bfx1RAxvXc/MrC9q2ytbYBzpqrZsBrCXpD1a0B8z68O2aXUHulEHsLyqbXnp2NLyAUmTgckAAwcOPGTkyJHd3kEzaz8LFy5cERG7Vbe3c9h25U1zJxExDZgGMGrUqFiwYEH2TplZ7yfp6Vrt7TyNsAwYWtU2pPhcfcVrZtat2jls5wLvr2qbBDwdEUtr1JuZdZt2DtvLgEMlXShppKRTgCnAN1rcLzPrg9o2bCPiIeCDwAeAR4ELgHMj4ppW9svM+qa2vkEWEXcDd7e6H2ZmbXtla2bWkzhszcwycNiamWXgsDUzy8Bha2aWgcPWzCwDh62ZWQYOWzOzDBy2ZmYZOGzNzDJw2JqZZeCwNTPLoK0fRLPl1gCPtLoTZtZGHLY1LF34OP+sg1vdDTNrIw7bGp4Hrmp1J8ysrXjO1swsA4etmVkGDlszswwctmZmGThszcwycNiamWXgsDUzy8Bha2aWgcPWzCwDh62ZWQYOWzOzDBy2ZmYZOGzNzDJw2JqZZeCwNTPLwGFrZpaBw9bMLAOHrZlZBg5bM7MMHLZmZhk4bM3MMnDYmpll4LA1M8vAYWtmloHD1swsA4etmVkGDlszswwctmZmGThszcwycNiamWXgsDUzy8Bha2aWgcPWzCwDh62ZWQZZw1bSEZLulPS0pJB0Xo2a0ZLmSVonaZmkqZL6V9WMkHSvpDWSVki6RtLAqpoOSbdKWlV83CJpcHeP0cysltxXtoOA/wS+ACyvPihpGHAfsBg4BDgN+DRwYalmEHA/sAEYC3wEmARcX6rpB9wFDAcmAkcDI4A7JKkbxmVm1iVFRGt+sPQUcF1EfL3UdhFwCrBnRGws2k4HLgYGR8RqSZOBy4GhEfFSUXMcKVz3joglko4G7gVGRsTiouZA4DFgQkTM7qpv/aXYvqmjNbO+Yg0sjIhR1e09bc52HDCzErSFGcAOwMGlmvmVoC3MBDYWxyo1SypBCxARi4ClwOHd1Hczs071tLDt4M3TC8tLx2rWRMR6YGVXNaVzddRoR9JkSQskLWjNtb6ZtbNtWt2BOkTV53pqG66JiGnANEjTCPV1zcysPj3tynYZMLSqrfJ6eWc1kgYAu3RVUxhC7SteM7Nu1dPCdi4wsVhNUDEJWAM8XKoZI2nHUs1E0ljmlmqGS9qvUiDpAGAYMKeb+m5m1qnc62wHSTpI0kHAtsDQ4vW+Rcl3gZ2AayUdKOl44ALgyohYXdTcDKwAbpb0LkkTgKuBH0fEkqJmFvAb4CZJh0oaDfwQ+DXwQIahmpltIuvSL0njgV/WOPRARIwvag4DLgXeDbwITAfOi4jXSufZH7iStLJgLfAT4MxSICOpA7iCdGUcwD3AlIh4bnP99NIvM9tSnS39atk6257MYWtmW6q3rLM1M2tLDlszswwctmZmGThszcwycNiamWXgsDUzy8Bha2aWgcPWzCwDh62ZWQYOWzOzDBy2ZmYZOGzNzDJw2JqZZeCwNTPLwGFrZpaBw9bMLAOHrZlZBg5bM7MMHLZmZhk4bM3MMnDYmpll4LA1M8vAYWtmloHD1swsA4etmVkGDlszswwctmZmGThszcwycNiamWXgsDUzy8Bha2aWgcPWzCwDh62ZWQYOWzOzDBy2ZmYZOGzNzDJw2JqZZVB32EoaIOnHkvbtzg6ZmbWjusM2ItYD7wde677umJm1p0anEe4Gju2OjpiZtbNtGqz/NXC+pHcBDwGrywcj4uZmdczMrJ0oIuovljZ2cTgiov/Wd6n1+kuxfas7YWa90hpYGBGjqtsburKNCK9eMDPbAg5PM7MMGg5bSRMkzZK0TNKzku6TNL75XTMzax8Nha2kE4FZwCrgG8DFwCvALEl/2/zumZm1h0ZvkC0CboqIqVXt5wAnRcQ7m9y/lvANMjPbUp3dIGt0GmFf4LYa7bcWx8zMrIZGw/Z54C9rtB9UHDMzsxoafVPDTcD3JO0G/DsQwHuBC4Brm9w3M7O20eiV7XnAjcDlwO+ARcBlwA3Al7v6RklnSZov6QVJL0qaI2lSjbrRkuZJWleseJgqqX9VzQhJ90paI2mFpGskDayq6ZB0q6RVxcctkgY3OF4zs6ZoKGwjYkNEnAXsTJo6eBewS0T8c0Rs2My3H0kK5QnAaNJbf++SNK5SIGkYcB+wGDgEOA34NHBhqWYQcD+wARgLfASYBFxfqukH3AUMByYCRwMjgDskqZExm5k1Q6OrEW4AzoiIl6vaBwJXRsQnGvrh0u+AmRHxueL1RcApwJ4RsbFoO520xGxwRKyWNJl0ZT00Il4qao4jheveEbFE0tHAvcDIiFhc1BwIPAZMiIjZXfXLqxHMbEs1azXCx4C31Gh/Cykk61Zcfb4VWFFqHkcK3/IzGGYAOwAHl2rmV4K2MBPYWByr1CypBC1ARCwClgKHN9JPM7NmaDRsRbop9kZD+rP8cBpfjXAO8Dbgh6W2DmB5Vd3y0rGaNcWzdld2VVM6V0eNdiRNlrRA0oL6r/XNzOpT12qE4mlfUXws72Ta8/J6f6ikz5DC9viIWLqZ8qj6XE9twzURMQ2YBmkaoY7zmJnVrd6lXyeTrmp/APwjUP4T/k+kP9kX1HMiSZ8HvkoK2llVh5cBQ6vaKq+Xl2qGVZ1zALBLVc1RNX78EGpf8ZqZdau6wjYifgQg6RlgXvFne8MkfQ34LHBsRDxQo2QucLKkfqV520nAGuDhUs3lknaMiFVF20TSlMjcUs2XJe0XEb8vfvYBpJCesyV9NzPbGg2tRtjkG6WhwLbltoj4Qxf13yYt4zqRtOyrYm1pVcEw0trd24BLgX2A6cC1EfHFomYQ8DjwKHAu6Yr2BuDBiDihqOlH2kliAzCFdFV+NbAeGBubGbRXI5jZlupsNUKjS7/eClwBnEBV0AJ0tVODOp8H/X5EfLxUdxgpaN8NvEgK2/Mi4rVSzf7AlaQbc2uBnwBnRsTqUk1H0ddJpHnae4ApEfHc5sbpsDWzLdWssP0u6e255wA/Ak4l/Wl+GvD5iPhxc7rbWg5bM9tSTdkWBzgO+FhE/LJYoTA/In4oaSnpJlpbhK2ZWbM1us72z4Ani69Xkd62C+mhNO9tVqfMzNpNo2H7NLBH8fUTwAeKryeQdmwwM7MaGg3bnwHji68vB86VtIz0ZoBpTeyXmVlb2eKlX5Aeh0h6DsHiiLi7ab1qMd8gM7Mt1awbZJuIiAeBB7fmHGZmfcFmw1bS2HpPFhHztq47ZmbtqZ4r2zmkNwVs7qHbAXT6pgYzs76snrAd3u29MDNrc5sN24h4OkdHzMzaWUM3yCQd0dXxiPjV1nXHzKw9NboaYTZvnr8trx3znK2ZWQ2Nhu2wqtcDSLvgfgk4qyk9MjNrQw2FbUT8sUbzU5JWA18mbUNuZmZVGn27bmeeID1/1szMatjqsJW0G3A28NRW98bMrE01uhphPW/enbY/6YlfJzSrU2Zm7abRG2SfYtOw3Qg8B/xHRLzQtF6ZmbWZRm+Q3dhN/TAza2uNTiPs3smhANb56tbMrLZGpxGW8uY529dJWkl6iPiXImLj1nTMzKydNBq2pwDfBH4AzC/axpA2ezwf2B34PPAScHFzumhm1vs1GrZ/B5wdET8otf1c0uPAiRFxjKRngc/isDUze12j62yPAObWaJ9bHAP4N/xYRjOzTTQatiuBY2q0H1McAxgIvLw1nTIzazeNTiNcAlwm6T3Ar0k3y8aQ3tDwhaLmWOA3TeuhmVkbaHh3XUkfBs4E/rxo+k/gkoi4vTi+DRAR8VozO5qTd9c1sy3V2e66W7WVebty2JrZluosbBt+EI2kbSUdL+lzknYq2vaS9LYm9NPMrC01+g6yPUnPrN0D2A64nbSm9p+A7YFTm9w/M7O20OiV7WXAI8AuwNpS+53AkU3qk5lZ22l0NcJfARMi4lWpvA0ZS4C3N61XZmZtptEr27cAf6rRvhuwbuu7Y2bWnhoN23nAiaXXlaUMZwDextzMrBONTiOcA8yWNLL43rMl/SVpze2YZnfOzKxdNHRlGxELgdHAq8CTwOHAfwGHAoOb3jszszbR6NKvQcCSiPj7UtshwBXA+0j7kZmZWZW6rmwl7S5pDmlN7UuSvilpO0nXk56RsIa0UsHMzGqo98p2KrAj6UbY35AeEH448AfgnRGxuHu6Z2bWHuoN2yNJDwefI+lnpO1x7ouI87utZ2ZmbaTeG2QdpBtiRMSzpHeP3dpdnTIzazf1hm0/YEPp9UY2fbuumZl1oZHVCLdJqrx7bHvgB5I2CdyIOLppPTMzayP1hu33q17f1OyOmJm1s7rCtryu1szMGtfww8PNzKxxDlszswwctmZmGThszcwyyBa2kk6WtFDSC5LWSnq82DRSpZrRkuZJWidpmaSpkvpXnWeEpHslrZG0QtI1kgZW1XRIulXSquLjFkl+KpmZtUyjz7PdGs8BFwCLSY9o/CvgO6Q3S1wuaRhpM8mfAp8C9gNuAAR8EV5/6tj9wG+BsaS90G4A3gacUNT0A+4ivfFiYvH93wHukDQuvHe7mbWAWpk9km4HiIgPSboIOAXYMyI2FsdPBy4GBkfEakmTgcuBoRHxUlFzHClc946IJZKOBu4FRlYekCPpQOAx0v5pszfXr/5SbN/ksZpZ37AGFkbEqOr2lszZKjkUGAf8smgeB8ysBG1hBrADcHCpZn4laAszSVex40o1S8pPIouIRaSH5xze7LGYmdUja9hK2knSK6RphPnAVRFxRXG4A1he9S3LS8dq1kTEemBlVzWlc3XUaK/0bbKkBZIWeJ7BzJot55wtwMvAQaSr1bHAVEnPRsR1ndRH1eeubFVNREwDpkGaRqjjXGZmdcsatsUUwRPFy99K2hn4OnAdsAwYWvUtldeVK9VlwLBygaQBpBtl5Zqjavz4IdS+4jUz63atXmfbD9iu+HouMLFYTVAxibTlzsOlmjGSdizVTCzOM7dUM1zSfpUCSQeQQnpO00dgZlaHbKsRJH0V+Hfgv4EBwBHAJcD0iDijWPq1CLgNuBTYB5gOXBsR5aVfjwOPAufyxtKvByOivPTrIdKSsimkpV9XA+uBsfUs/fJqBDPbUp2tRsg5jbAjcA3wdmAdKXTPLtqIiGeKZVuXAguBF0lzqOdVThARr0g6CriSdINtLfAT4MxSzUZJHyDt+Hs/aZ72HmCK19iaWau0dJ1tT+UrWzPbUj1qna2ZWV/jsDUzy8Bha2aWgcPWzCwDh62ZWQYOWzOzDBy2ZmYZOGzNzDJw2JqZZeCwNTPLwGFrZpaBw9bMLAOHrZlZBg5bM7MMHLZmZhk4bM3MMnDYmpll4LA1M8vAYWtmloHD1swsA4etmVkGDlszswwctmZmGThszcwycNiamWXgsDUzy8Bha2aWgcPWzCwDh62ZWQYOWzOzDBy2ZmYZOGzNzDJw2JqZZeCwNTPLwGFrZpaBw9bMLAOHrZlZBg5bM7MMHLZmZhk4bM3MMnDYmpll4LA1M8vAYWtmloHD1swsA4etmVkGDlszswwctmZmGThszcwycNiamWXQsrCVdKSk1yQ9UdU+WtI8SeskLZM0VVL/qpoRku6VtEbSCknXSBpYVdMh6VZJq4qPWyQNzjE2M7NqLQlbSUOA7wP3VbUPK9oWA4cApwGfBi4s1QwC7gc2AGOBjwCTgOtLNf2Au4DhwETgaGAEcIckdde4zMw6s03uH1gE4Y+Aq4HtgX1Lh08DVgGfjIiNwCJJbwculnRBRKwGTgJ2BU6KiJeKc54O3CXp7IhYAhwFvBsYGRGLi5qTgceA9wKzu3+kZmZvaMWV7ZeAAC6ucWwcMLMI2ooZwA7AwaWa+ZWgLcwENhbHKjVLKkELEBGLgKXA4c0YhJlZI7Je2UqaAJwKHBwRG2v8Rd8BzK1qW146Vvm8vFwQEeslreyqpnSujhrtSJoMTAbwPIOZNVu2sJW0K3AT8ImIqBWEnYmqz/XUNlwTEdOAaQD9pXrOY2ZWt5xXtu8Edgd+Ubqi7QdI0gbgFGAZMLTq+yqvKwG9DBhWLpA0ANilquaoGn0YQu0rXjOzbpVzzvYh4C+Ag0of1wDPFF/fTZpCmFjcRKuYBKwBHi5ezwXGSNqxVDORNJa5pZrhkvarFEg6gBTSc5o3JDOz+iiidX8xSzof+GhE7Fu8HgYsAm4DLgX2AaYD10bEF4uaQcDjwKPAuaQr2huAByPihKKmHyncNwBTSNOwVwPrgbGxmUH3l2L7po7UzPqKNbAwIkZVt/eod5BFxDOkNbEHAAtJc6jTSKFaqXmFNEWwLTAf+AlpNcInSzUbgQ8AfyCtyb0PeBL4P5sLWjOz7tDSK9ueyle2ZralesWVrZlZu3LYmpll4LA1M8vAYWtmloHD1swsA4etmVkGDlszswwctmZmGThszcwycNiamWXgsDUzy8Bha2aWgcPWzCwDh62ZWQYOWzOzDBy2ZmYZOGzNzDJw2JqZZeCwNTPLwGFrZpaBw9bMLAOHrZlZBg5bM7MMHLZmZhls0+oO9EQb4ZU1sLjV/choV2BFqzuRUV8bL/S9MbdyvO+o1eiwrW1xRIxqdSdykbTA421vfW3MPXG8nkYwM8vAYWtmloHDtrZpre5AZh5v++trY+5x41VEtLoPZmZtz1e2ZmYZOGzNzDJw2BYkHSvpEUmvSnpK0pmt7lM9JB0h6U5JT0sKSefVqBktaZ6kdZKWSZoqqX9VzQhJ90paI2mFpGskDayq6ZB0q6RVxcctkgZ39xir+nCWpPmSXpD0oqQ5kibVqGuLMUs6WdLCYrxrJT0u6XOS1G5jrUXSkZJek/REVXvvG3NE9PkPYBSwHvgGcADwcWAdcGqr+1ZH348FpgJ/CywDzqs6PgxYBUwHDgQ+CKwEvlGqGQQ8A9wNHAQcCTwF3FKq6QcsBB4CRgOHAb8B5lHM/Wca7z3Ap4p+7g/8C7ABGNeOYwbeX/T/AGBv4GPAauCMdhtrjbEPKfo9A3iit//3zf4L7IkfwM3AvKq2bwFLWt23BsfxFG8O24uApUC/Utvpxf9gBxavJwNrgZ1KNccBAQwvXh9dvN6/VHNg0Ta+xeP+HXBJXxkzcDtwezuPtQjCWcAXgfOrwrZXjtnTCMk40v97ls0A9pK0Rwv600zjgJkRsbHUNgPYATi4VDM/Il4q1cwENhbHKjVLIuL1tzFHxCLSP/rDu6nvmyWpH/BWNn1rZluOWcmhRb9+Wepj240V+BIp9C6ucaxXjtlhm3QAy6valpeO9Wb1jO1NNRGxnvSnWac1pXO18nd0DvA24IeltrYas6SdJL0CvArMB66KiCuKw201VgBJE4BTgZOrArWiV47Zz0bYvHZciBxVn+up3dqappP0GVLYHh8RSzdT3pvH/DJp3nEHYCwwVdKzEXFdJ/W9dqySdgVuAj4REbWCsDM9fswO22QZMLSqbUjxuZH/4D1RrbFVXi8v1QwrF0gaAOxSVXNUjfMPoQW/I0mfB75KCtpZVYfbaszF1V3lbvxvJe0MfB24jjYbK/BOYHfgF6UFF/1IsygbgFPopWP2NEIyl3TXt2wS8HQdV0w93VxgYjG3WTEJWAM8XKoZI2nHUs1E0r+PuaWa4ZL2qxRIOoD0D3pON/W9JklfA74CHFsjaKENx1ylH7Bd8XW7jfUh4C9IV/KVj2tIKwsOIq0u6J1jzn2XsSd+AO8hLf26EBhJ+n/PtfSOpV+DeOMf5bPAVcXX+xbHK8tkrifdaT0e+B9qL5O5C3gXMAFYQu1lMg8Ch5KWyiwgzSHmXPr17eK/zQdJVzOVj51KNW0zZtLV+1GkZV/7k5a9rQIub7exdvE7OJ/aS7961Zhb9gvsaR+kZSGPkm5CPA2c2eo+1dnv8aT5peqP2aWaw0hrB9eR/jyaCvSvOs/+pLu1a4p/uN+jWEZTqukAbiPNIa4CfgwMzjzeWmMN4MaqurYYM3AZaQphLfBCEQ6nl8fSLmPt4newSdj21jH7QTRmZhl4ztbMLAOHrZlZBg5bM7MMHLZmZhk4bM3MMnDYmpll4LC1PkfS+dUPozbrbg5b6zUkvUXSBZJ+X+xa8D+SHpL0f1vQlycknd/E810naXazzmc9jx9EY73Jd0lvuzyD9G6/HUnPL92zlZ3qiqRtI+JPre6HtZ6vbK03+SDwrYi4IyKWRMSjEXFjRHytUiDpRkmbPJxG0kclvemtkpJOkvTfxT5WsyQNLx3bQ9JPi72r1hZ1ZxXHZgP7AF9R2vctJO0laXzx9XFKe6OtAyZL2lnSTZL+UJxrsUr7iBVXyJ8E3ls638eLY4MkXS7pj8VeWg9L+nBzf62Wg69srTdZBkySdHNErNzKc3UAnyHt3QbpAT53SDoo0nvYv0N6fuxRwIvAcN54jN+HSc8o+ClpDzSA54G9iq8vAb5A2q5nPekJXb8DLiU932Ac6UlWK0n7aP0LsF/xMypB+lIRxr8AVPTz2aI/t0g6JiLu38rfgWXksLXe5B9I+8U9L2kR8GvSI/d+Ho0/5GMH4OMR8QSkXWyBxcD7SHtfvYO0z9cjRf1TlW+MiJWSXgNeidIDrkvPX70wIn5e9fO+Wfp6iaT3ACcB0yPiFUlrgT9VnW88MAYYEm9s7zJN0mHAFMBh24s4bK3XiIi5kvYhPQ5vDHAE6eryHknHNxi4z1eCtjj3f0laAfw5KWy/DXxP0jHAbODuiPhVnef+j/KL4rmrXwBOAPYAtgcGkJ4u15X3ANsCfywFOUXb7+vsi/UQDlvrVSJiA+nRevOASyR9lLT/2BHAA6QN/VT1bQPqPP3r3xcR0yXNID2UegIp0G+PiI/WcZ7VVa8/B5wNnEnaKvtl4LOkx3p2pR/wEil0q/mmWy/jsLXe7vHi8+Di83Okq96yd9f4vt0k7RMRTwJIGgH8Wel8RMQy0pzqdEn/Cvw/SZ+JiFWksOtfZx+PAGZExPWVhvLuAIVa51tA2sxy+4h4rM6fZT2UVyNYryHpAUmnShol6R2S3ke6kfUib2ztPQsYKekfJe0j6VPAR2qcbg0pRA+RNAr4Pukm1qziZ10l6djiHAeSblw9Q7oqhfTU/3GS9pS0a9UWLdUWA+MlTZA0QtLXSbsClC0p+n1gcb7tgH8r+vMzSR+StHfR3ynFuKwXcdhab3IP8HfAv5ICbDpp7nJcRKwAiLQn2XmkP9sfBY4EvlbjXMuAaaQ537mknRA+VJr3FWne9jHgV8BA4JjS8a8AOxX9eJ6u1/peQJriuJO05crOwBVVNdeT9t+aV5zvxOJnHQ/8jLSS4f+TbggeBzzZxc+zHsg7NZiZZeArWzOzDBy2ZmYZOGzNzDJw2JqZZeCwNTPLwGFrZpaBw9bMLAOHrZlZBg5bM7MM/hcuAOH9NOStkgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(regulator_gene_matrix, cmap='hot');\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1yhzyikoFncq"
   },
   "outputs": [],
   "source": [
    "class EncoderLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, input_dim=32, units=32):\n",
    "        super(EncoderLinear, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.rgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4uchxU-bmBDe"
   },
   "outputs": [],
   "source": [
    "class DecoderLinear(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, input_dim=32, units=32):\n",
    "        super(DecoderLinear, self).__init__()\n",
    "        self.rgm = rgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        #return tf.matmul(inputs, self.w)\n",
    "        # v = tf.zeros_like(X)\n",
    "        # u = tf.ones_like(X)\n",
    "        # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "        return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PQVz4BCpmNMd"
   },
   "outputs": [],
   "source": [
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association)\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(128)(X)#Dense(128)(X)\n",
    "    de_dense = Reshape((1, 128))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def model(rgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 32):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinaryAE = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS)\n",
    "# ordinaryAE.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = ordinaryAE.fit(ecoli_Intensities, ecoli_Intensities, epochs=200, verbose = True, validation_data=(validation, validation))\n",
    "#print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(EncoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.OGrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "\n",
    "\n",
    "class DecoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DecoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        return X\n",
    "        \n",
    "        \n",
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(128)(X)\n",
    "    de_dense = Reshape((1, 128))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelSuperParent(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSuperParentSequential(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 32): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    m = tf.keras.Sequential()\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del ordinaryAE #to fix OOM error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "# looseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "# o = looseParent.fit(ecoli_Intensities, ecoli_Intensities, epochs=200, verbose = True,  validation_data=(validation, validation))\n",
    "#print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNetAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a second copy of the layers which will be modified to be a denseNET auto encoder\n",
    "'''\n",
    "\n",
    "class DenseEncoderLinear2(tf.keras.layers.Layer): \n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DenseEncoderLinear2, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.oldrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "\n",
    "class DenseDecoderLinear2(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DenseDecoderLinear2, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.oldrgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) #used to have a transpose\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "        \n",
    "\n",
    "def denseencoder2(parent_child_biological_association, inp, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(16, 3, activation = \"tanh\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    inp = Flatten()(inp)\n",
    "    d = Concatenate()([en_dense, inp]) #dense layer\n",
    "    phenotype = Dense(num_hidden_units, activation=\"tanh\")(d)\n",
    "    return phenotype\n",
    "\n",
    "def densedecoder2(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(16, activation = 'tanh')(X)\n",
    "    de_dense = Reshape((1, 16))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"tanh\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelDense2(rgm, oldrgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = DenseEncoderLinear2(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    #x = EncoderLinear2(x)\n",
    "    enc = denseencoder2(x, inp, num_hidden_units)\n",
    "    dec = densedecoder2(enc, num_protein_gene, time_steps)\n",
    "    out = DenseDecoderLinear2(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "# dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "# dense.fit(ecoli_Intensities, ecoli_Intensities, epochs=20,  verbose=True, validation_data=(validation, validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# del dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = dense.predict(validation)\n",
    "# o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = pd.DataFrame(np.squeeze(o))\n",
    "# v = pd.DataFrame(np.squeeze(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore_noParent_MSE(validation, o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Size Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N = 40\n",
    "# hidden = np.arange(10,NUM_PARENTS, 20) #range(1,32)\n",
    "# lossMatrix = []\n",
    "# for i in tqdm(range(N), colour='red'):\n",
    "    \n",
    "#     losses = []\n",
    "#     for value in (hidden):\n",
    "#         dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, value)\n",
    "#         dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "#         dense.fit(ecoli_Intensities, ecoli_Intensities, epochs=30,  verbose=0)\n",
    "#         test_hat = dense(validation) #, verbose = 0)\n",
    "#         loss = ignore_noParent_MSE(validation, test_hat)\n",
    "#         losses.append(loss)\n",
    "#         tf.keras.backend.clear_session()\n",
    "#         del dense\n",
    "#     lossMatrix.append(losses)\n",
    "# lossMatrix = np.array(lossMatrix)\n",
    "\n",
    "#run 100 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avgMSE = np.average(lossMatrix, axis = 0)\n",
    "# plt.plot(hidden, avgMSE);\n",
    "# plt.xlabel(\"Latent Space Size\");\n",
    "# plt.ylabel(\"MSE\");\n",
    "# plt.title(\"MSE of the AutoEncoder with Respect to the Latent Space Size\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = pd.DataFrame(lossMatrix)\n",
    "# lm.to_csv(\"lossmatrix4lisaDREAM5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.read_csv(\"lossmatrix4lisaDREAM5.csv\").to_numpy()\n",
    "# print(temp.shape)\n",
    "# avgMSE = np.average(lossMatrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(hidden, avgMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lazy_train(epochs=30, hidden = 24):\n",
    "    ep = epochs\n",
    "    hidden = [hidden,] #range(1,32)\n",
    "    lossMatrix = []\n",
    "    lazy_weights = []\n",
    "    dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, num_hidden_units=16)\n",
    "    dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "\n",
    "    #for i in tqdm(range(ep)):\n",
    "    for i in range(ep):\n",
    "        dense.fit(ecoli_Intensities, ecoli_Intensities, validation_data=(validation, validation), epochs=1,  verbose=0)\n",
    "        lazy_weights.append(dense.get_weights()[0])\n",
    "        test = dense(validation) #, verbose = 0)\n",
    "        loss = ignore_noParent_MSE(validation, test)\n",
    "        lossMatrix.append(loss)\n",
    "        \n",
    "    lossMatrix = np.array(lossMatrix)\n",
    "    lazy_weights = np.array(lazy_weights, dtype = 'float16')\n",
    "\n",
    "    return lossMatrix, lazy_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://proceedings.mlr.press/v162/rachwan22a/rachwan22a.pdf Winning the Lottery Ticket Ahead of Time:\n",
    "def lazyKernelRegime(w, parent_idx=parent_idx):\n",
    "\n",
    "    firstLayer = []\n",
    "    for i in range(len(w)):\n",
    "        firstLayer.append(w[i][parent_idx])\n",
    "\n",
    "    fL = np.array(firstLayer)    \n",
    "    d0 = np.square(fL[1] - fL[0])\n",
    "    kernelChange = []\n",
    "    for i in range(1,len(fL)):\n",
    "        dt = np.square(fL[i] - fL[0])\n",
    "        dt_minus1 = np.square(fL[i-1] - fL[0])   \n",
    "        d = np.abs(dt - dt_minus1)/d0                        #eq 11 from the paper\n",
    "        kernelChange.append(d)\n",
    "    \n",
    "    kernelChange = np.moveaxis(kernelChange, 0, 2)\n",
    "    # plt.plot(kernelChange[0][0]);\n",
    "    # plt.title(\"$|\\Delta W|$ vs Epoch\")\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"$|\\Delta W|$\")\n",
    "\n",
    "    return np.array(kernelChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distrib(change, t = .05, raw = False): #raw means return the unshaped indicies. \n",
    "    stop = []\n",
    "\n",
    "    for parent in range(len(change)):\n",
    "        for child in range(len(change[0])):\n",
    "            try:\n",
    "                stop.append(np.min(np.argwhere(change[parent][child] < t)))\n",
    "            except ValueError:\n",
    "                stop.append(len(change))\n",
    "                # print(parent, child)\n",
    "                # plt.plot(change[parent][child])\n",
    "                # assert(False)\n",
    "    # print(change.shape)\n",
    "    # assert(False)\n",
    "    \n",
    "    stop = np.array(stop).flatten()\n",
    "    # print(stop.shape)\n",
    "    var = np.std(stop)\n",
    "    # print(var)\n",
    "    # assert(False)\n",
    "    mean = np.average(stop)\n",
    "    top_parents = np.argwhere(stop > mean + 2*var) #get the parents which take more than 2 stds to stop training\n",
    "    top_parent_child = []\n",
    "    for tp in top_parents:\n",
    "        top_parent_child.append(np.unravel_index(tp, shape = (NUM_PARENTS, NUM_TARGETS)))\n",
    "\n",
    "    if raw == False:\n",
    "        plt.hist(stop)\n",
    "        plt.xlabel(\"Epoch where regulator-target-weight began changing by at most \"+ str(t));\n",
    "        plt.ylabel(\"Number of parent-child-weights\");\n",
    "        plt.title(\"Histogram of weight stops\");\n",
    "        print(\"average stop: \", mean);\n",
    "\n",
    "    if raw == True:\n",
    "        return np.array(top_parents)\n",
    "\n",
    "    return np.array(top_parent_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazyKernels(N=100):\n",
    "\n",
    "    candidates = []\n",
    "    final_w = []\n",
    "    for i in tqdm(range(N)):\n",
    "        lm, lazy_weights = do_lazy_train(epochs=30)\n",
    "        change = lazyKernelRegime(lazy_weights)\n",
    "        top_pr = np.squeeze(compute_distrib(change, t = 0.2, raw=True))\n",
    "        candidates.append(top_pr)\n",
    "        final_w.append(lazy_weights[-1]) #final w is used for magntiude calculations at the end\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    final_w = np.array(final_w, dtype= 'float16')\n",
    "    firstLayer = []\n",
    "    for i in range(len(final_w)):\n",
    "        firstLayer.append(np.abs(final_w[i][parent_idx]))\n",
    "    fw = np.array(firstLayer, dtype = 'float16')\n",
    "    #print(fw.shape)\n",
    "    fw_avg = np.average(fw, axis = 0)\n",
    "    #print(fw_avg.shape)\n",
    "    \n",
    "    candidates = np.hstack(candidates)\n",
    "    candidates = candidates.reshape(candidates.size)\n",
    "    #print(candidates.shape)\n",
    "\n",
    "    # plt.hist(candidates, bins=np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    # plt.title(\"Parent-Child Regulator Histogram\")\n",
    "    # plt.xlabel(\"Parent-Child Weight\")\n",
    "    # plt.ylabel(\"Num-Times parent-child relationship trained for top 5% of time\")\n",
    "    return candidates, fw_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\dream5Encoder\\Dream5AutoEncoder.ipynb Cell 60\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m can, mag \u001b[39m=\u001b[39m lazyKernels(\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\dream5Encoder\\Dream5AutoEncoder.ipynb Cell 60\u001b[0m in \u001b[0;36mlazyKernels\u001b[1;34m(N)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m final_w \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(N)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     lm, lazy_weights \u001b[39m=\u001b[39m do_lazy_train(epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     change \u001b[39m=\u001b[39m lazyKernelRegime(lazy_weights)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     top_pr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(compute_distrib(change, t \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m, raw\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\dream5Encoder\\Dream5AutoEncoder.ipynb Cell 60\u001b[0m in \u001b[0;36mdo_lazy_train\u001b[1;34m(epochs, hidden)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#for i in tqdm(range(ep)):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ep):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     dense\u001b[39m.\u001b[39;49mfit(ecoli_Intensities, ecoli_Intensities, validation_data\u001b[39m=\u001b[39;49m(validation, validation), epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,  verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     lazy_weights\u001b[39m.\u001b[39mappend(dense\u001b[39m.\u001b[39mget_weights()[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream5Encoder/Dream5AutoEncoder.ipynb#Y113sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     test \u001b[39m=\u001b[39m dense(validation) \u001b[39m#, verbose = 0)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "can, mag = lazyKernels(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"candidate1.npy\", can) #the interpolated dataset\n",
    "np.save(\"magnitude1.npy\", mag) #the interpolated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf, bins = np.histogram(can, np.arange(NUM_PARENTS*NUM_TARGETS))\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitudes After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS*NUM_TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unravel_index(NUM_PARENTS*NUM_TARGETS, shape = (NUM_PARENTS, NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_reg_targets(can, mag):\n",
    "    \n",
    "    pdf, bins = np.histogram(can, np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    pdf2d = np.zeros(shape = (NUM_PARENTS,NUM_TARGETS))\n",
    "\n",
    "    for i in bins:\n",
    "      idx2d = np.unravel_index(i, shape = (NUM_PARENTS, NUM_TARGETS))\n",
    "      try:\n",
    "        pdf2d[idx2d] = pdf[i]\n",
    "      except IndexError:\n",
    "        print(i, idx2d, len(pdf), len(bins))\n",
    "    \n",
    "\n",
    "    importance = np.multiply(pdf2d, mag)\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = get_top_reg_targets(can, mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top[6][122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(top, cmap = 'inferno', vmin = 0, vmax = np.max(top));\n",
    "plt.colorbar(u ,fraction=0.0086, pad=0.02);\n",
    "plt.title(\"The Most Important Regulator-Target Combinations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topID = np.array(np.unravel_index(np.argsort(top, axis=None), top.shape))\n",
    "topID = np.flip(topID, axis=1)\n",
    "topID[0] = parent_idx[topID[0]]\n",
    "topID = topID.T\n",
    "topID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topR_T = pd.DataFrame(topID)\n",
    "# topR_T.to_csv(\"Top_reg_target_decendingOrder_firstColIsRegulator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.zeros(NUM_PARENTS)\n",
    "for i in range(NUM_PARENTS):\n",
    "    best[i] = np.sum(top[i])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,NUM_PARENTS, dtype=int)\n",
    "plt.plot(x, best); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.flip(np.argsort(best))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(can).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Petal Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal = pd.read_excel(data_path_petal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train = petal[petal[\"Line\"] == \"WT\"]\n",
    "petal_train = petal_train.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_train.head(12)\n",
    "petal_train = petal_train.groupby(['Plate']).mean()\n",
    "petal_train.head()\n",
    "petal_train = petal_train.to_numpy()\n",
    "print(petal_train.shape)\n",
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(petal_train)\n",
    "petal_train = scaler1.transform(petal_train)\n",
    "mm = MinMaxScaler()\n",
    "mm.fit(petal_train)\n",
    "petal_train = mm.transform(petal_train)\n",
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test = petal[petal[\"Line\"] != \"WT\"]\n",
    "petal_test = petal_test.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_test = petal_test.groupby(['Plate']).mean()\n",
    "petal_test.head()\n",
    "petal_test = petal_test.to_numpy()\n",
    "print(petal_test.shape)\n",
    "petal_test = scaler1.transform(petal_test)\n",
    "petal_test = mm.transform(petal_test)\n",
    "petal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1.shape\n",
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densePredictor = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS, 22)\n",
    "densePredictor.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "densePredictor.fit(ecoli_Intensities, ecoli_Intensities,validation_data=(experiment1, experiment1),  epochs=100,  verbose=1)\n",
    "test = densePredictor(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgm = superParent\n",
    "time_steps = 6\n",
    "num_kinase_regulators = NUM_TARGETS\n",
    "num_hidden_units = 22\n",
    "\n",
    "inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "x = DenseEncoderLinear2(rgm, regulator_gene_matrix, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "enc = denseencoder2(x, inp, num_hidden_units)\n",
    "denseP = tf.keras.Model(inputs=inp, outputs=enc)\n",
    "#set the weights of the encoder to the weights of auto encoder\n",
    "dw = densePredictor.get_weights()\n",
    "enc_w = dw[0:5]\n",
    "denseP.set_weights(enc_w)\n",
    "#add a dense layer  because we are ouputing 1 number\n",
    "l = Dense(32, activation = 'swish', use_bias=True, kernel_regularizer='l1_l2')(denseP.layers[-1].output)\n",
    "l = Dense(1, activation = 'linear', use_bias = True)(l)\n",
    "denseP = tf.keras.Model(denseP.inputs, l)\n",
    "#denseP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = np.concatenate([experiment1, experiment1, experiment1, experiment1])\n",
    "bp.shape\n",
    "#bigexperiment1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = petal_train[0]\n",
    "b = petal_train[1]\n",
    "c = petal_train[2]\n",
    "d = petal_train[3]\n",
    "\n",
    "petal_train1 = np.array([a,a,a,a, b,b,b,b, c,c,c,c, d,d,d,d]) #does this make sense? we are training network to predict .5\n",
    "petal_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "denseP.fit(experiment1, petal_train, epochs=500, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP(experiment1) #experiment1 is part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(denseP(testCandidate)) #model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test #true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.evaluate(testCandidate, petal_test) #eval gave 1.3999 before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test set and the synthetic dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestSet(test_path):\n",
    "    testFiles = []\n",
    "    for np_name in glob(os.path.join(data_path_testSet,'*.np[yz]')):\n",
    "        k = np.load(os.path.join(data_path_testSet,np_name))\n",
    "        testFiles.append(k)\n",
    "#         print(np_name)\n",
    "#         print(k.shape)\n",
    "    return np.array(testFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PKjzdFCMwFg"
   },
   "outputs": [],
   "source": [
    "def read_files(data_path):\n",
    "\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        try:\n",
    "            #print('file name:',file)\n",
    "            #print('value of i:',i)\n",
    "            genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "            #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "            replicates.append(np.array(genes_intensities_data_matrix.values, dtype = float))\n",
    "            # i+=1\n",
    "        except PermissionError:\n",
    "            print(\"Not a CSV: \", os.path.join(data_path , file))\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    rep = np.array(replicates).astype(np.float32)\n",
    "    \n",
    "    return rep, rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCwo4LwlO_FF"
   },
   "outputs": [],
   "source": [
    "ecoli_Intensities, regulator_gene_matrix= read_files(data_path_syn)\n",
    "matrix = regulator_gene_matrix\n",
    "replicates = ecoli_Intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros(shape = (3,6,8))\n",
    "id = np.unravel_index(3*6*8 - 1, shape = d.shape)\n",
    "d[id] = 1\n",
    "plt.imshow(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0][ : , parentIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)\n",
    "print(testCandidate.shape)\n",
    "syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(change[0][22])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"change in weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossMatrix)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change.shape, lossMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diff(change[0][22])\n",
    "plt.plot(d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change[0][0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_tresh(change, stop = 0.05):\n",
    "#     diffs = []\n",
    "#     for parent in range(len(change)):\n",
    "#         for child in range(len(change[0])):\n",
    "#             diffs.append(np.diff(change[parent][child]))\n",
    "#     inflection = []\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         for d in diffs:\n",
    "#             print(np.argwhere(np.abs(d) < stop))\n",
    "#             inflection.append(np.min(np.argwhere(np.abs(d) < stop))) #return where the second derivative is first 0. \n",
    "\n",
    "#     except ValueError:\n",
    "#         print(\"Stop value \", stop, \" is too high, trying stop = \", stop + 0.05)\n",
    "#         # s = stop + 0.05\n",
    "#         # return compute_tresh(change, stop = s)\n",
    "        \n",
    "\n",
    "#     return np.average(inflection)\n",
    "        \n",
    "# d = compute_tresh(change)\n",
    "# d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"mse3.npy\", avgMSE) #mse2/3 is with -1 fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sciencedirect.com/science/article/pii/S0925231220314570?casa_token=lcEJANqO0JwAAAAA:uL3DGUZctPUZz_sPz1K1i2klMtb83TyKnc9CI3_N-uSOaM7VHL8GhM0jCGYfo25NmpDQQ9Cvlw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rshp = Flatten()(looseParent.layers[-1].output)\n",
    "\n",
    "modelTemp = tf.keras.Model(inputs=looseParent.input, outputs = [rshp])\n",
    "modelTemp.summary()\n",
    "type(modelTemp)\n",
    "explainer = shap.DeepExplainer(modelTemp, syntheticDataTrain)\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough #this solves the \"shap_ADDV2\" problem but another one will appear\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"FusedBatchNormV3\"] = shap.explainers._deep.deep_tf.passthrough #this solves the next problem which allows you to run the DeepExplainer.\n",
    "\n",
    "shap_values = explainer.shap_values(testCandidate[0:1])\n",
    "def f(x):\n",
    "    return modelTemp.predict(x)\n",
    "\n",
    "print(f(testCandidate))\n",
    "explainer = shap.KernelExplainer(f , testCandidate[0:1], link=\"logit\") #svm.predict_proba, X_train, link=\"logit\")\n",
    "shap_values = explainer.shap_values(testCandidate[0:1], nsamples=100)\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import json\n",
    "import shap\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "X,y = shap.datasets.imagenet50()\n",
    "to_explain = X[[39,41]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(enc_dec_Synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [5,6]\n",
    "u = tf.concat([a,b], axis = 0)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newConnections = superParent - regulator_gene_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(newConnections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nC = []\n",
    "# for i in range(len(newConnections[0])):\n",
    "#     for j in range(len(newConnections[1])):\n",
    "#         if newConnections[i][j] > 0:\n",
    "#             nC.append([i,j])\n",
    "# nC = np.array(nC)\n",
    "# nC = pd.DataFrame(nC)\n",
    "# nC.to_csv(\"new_connections_in_superParents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code for testing loss function\n",
    "# print(outSyn.shape)\n",
    "# print(testCandidate.shape)\n",
    "# syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "# syntheticLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Auto Encoder\n",
    "Autoencoder has not been trained on synthetic version of experiement 1. We test on the original experiment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrRuJ_bsrHll"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAOZEEvRRVU4"
   },
   "outputs": [],
   "source": [
    "# enc_dec_Synthetic.compile(optimizer='adam',loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticDataTrain = ecoli_Intensities[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1649273035573,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "EuNDZx5Ov34I",
    "outputId": "74597aa1-c71c-4491-b8c9-4fefed309325"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.fit(syntheticDataTrain,syntheticDataTrain,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = enc_dec_Synthetic(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = enc_dec_Synthetic.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w[0], cmap = \"hot\", vmin=0,vmax=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBzDmjqJViEw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we do not need to use this function for the testset\n",
    "def getCSVs(data_path_head):\n",
    "    PATH = data_path_head\n",
    "    EXT = \"*.csv\"\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "    actual = []\n",
    "    for p in all_csv_files:\n",
    "        actual.append(pd.read_csv(p, index_col = 0).to_numpy())\n",
    "    return np.array(actual)\n",
    "    \n",
    "experiment1 = getCSVs(data_path_og_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate = test.numpy().astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([ecoli_Intensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([ecoli_Intensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outSyn = enc_dec_Synthetic.predict(testCandidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymagn(A, B):\n",
    "    mse = (np.square(A - B)).mean(axis=None)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticLoss = ignore_noParent_MSE(testCandidate, outSyn )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(outSyn-testCandidate).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install keras-visualizer\n",
    "#!pip install pydot\n",
    "#data_path_og_exp1 = data_path_testSet \n",
    "# !pip install pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = loadTestSet(data_path_testSet)\n",
    "# testCandidate = test.astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPRV0OAMpKPH"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS) #we can just change the time steps to something higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1649273028683,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "D6aPT5_cpKK0",
    "outputId": "c261d824-0f38-4eee-b139-f03a80f093e1"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated dataset Auto Encoder\n",
    "Once again, we do not train on any version of exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filesV2(data_path):\n",
    "    '''\n",
    "    *Changed*\n",
    "    currently hardcoded for only one file. \n",
    "    change code a bit for reading multiple files.\n",
    "    '''\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        #print('file name:',file)\n",
    "        #print('value of i:',i)\n",
    "        genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "        #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "        replicates.append(genes_intensities_data_matrix.values)\n",
    "        # i+=1\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    \n",
    "    return np.asarray(replicates), rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes, _ = read_filesV2(data_path_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(interpolated_genes[2]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = []\n",
    "for k in range(len(interpolated_genes)):\n",
    "    #print(k)\n",
    "    if k == 2 or k == 3 or k == 4:\n",
    "        inter.append(np.reshape(interpolated_genes[k], (4,6,NUM_TARGETS)))\n",
    "    else: \n",
    "        inter.append(np.reshape(interpolated_genes[k], (5,6,NUM_TARGETS)))\n",
    "inter = np.vstack(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoli_Intensities[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter = model(regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS) \n",
    "enc_dec_inter.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter.fit(inter, inter,epochs=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter = enc_dec_inter.predict(testCandidate)\n",
    "interpolationLoss = ignore_noParent_MSE(testCandidate, outInter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolationLoss #used to be 3.84 on broke ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = enc_dec_inter.history\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons between various outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(np.reshape((np.abs(outSyn)), (24,NUM_TARGETS)), cmap = \"hot\", vmin=0,vmax=1.0 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(outSyn[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(testCandidate[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape((np.abs(outSyn-outInter)), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 );\n",
    "plt.title(\"Difference Between the Outputs of Both Autoencoders\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"DiffBetweenOut.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outInter-experiment1), (24,NUM_TARGETS)) , cmap = \"hot\") #, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Interpolated Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30)\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"InterDiffImage.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outSyn-experiment1), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"SynDiffImage.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx = parentIndex.numpy()\n",
    "#print(parent_idx)\n",
    "oSyn = (np.reshape((outSyn), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "oSyn = oSyn.T\n",
    "oSyn.shape\n",
    "\n",
    "exp1_col = (np.reshape((experiment1), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "exp1_col = exp1_col.T\n",
    "print(exp1_col.shape)\n",
    "\n",
    "u = plt.imshow(np.abs(oSyn - exp1_col), cmap = 'hot') #TODO use TF loss function instead of difference.\n",
    "ddff = oSyn-exp1_col\n",
    "plt.colorbar(u)\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data. Only parents.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(oSyn).head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(exp1_col).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ddff).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"interpolated_v2.npy\", inter) #the interpolated dataset\n",
    "# np.save(\"synthetic_v2.npy\", ecoli_Intensities[1:]) # the synthetic dataset\n",
    "# np.save(\"synOut_v2.npy\", outSyn) #the output of the encoder trained on synthetic data with the input being exp1\n",
    "# np.save(\"interOut_v2.npy\", outInter) #the output of the encoder trained on interpolated data with the input being exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6J5QSynTTgZ+lFOhgOtAG",
   "collapsed_sections": [],
   "name": "cnn-third.ipynb",
   "provenance": [
    {
     "file_id": "1mmRxO5jnBc5CdzxXj8sMtPpG0BQ0ulSs",
     "timestamp": 1648921397876
    },
    {
     "file_id": "10758zFj2UnTnwpQaSFIr5r9MqZWdiMsf",
     "timestamp": 1648674775255
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
