{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install synapseutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import synapseclient \n",
    "#  import synapseutils \n",
    " \n",
    "#  syn = synapseclient.Synapse() \n",
    "#  syn.login('finamintoastcrunch','1Hjldria!') \n",
    "#  files = synapseutils.syncFromSynapse(syn, ' syn2825306 ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qTo_HuQkGgAq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import*\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "# from keras.layers import Input\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import Conv1D\n",
    "# from keras.layers import Conv1DTranspose\n",
    "# from keras.layers import Flatten, Reshape\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import losses\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "\n",
    "# from interpret import show\n",
    "# from interpret.blackbox import ShapKernel\n",
    "import shap\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pca = PCA()\n",
    "# rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=seed)\n",
    "\n",
    "# blackbox_model = Pipeline([('pca', pca), ('rf', rf)])\n",
    "# blackbox_model.fit(X_train, y_train)\n",
    "\n",
    "# shap = ShapKernel(blackbox_model.predict_proba, X_train)\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings(\"ignore\")\n",
    "#     shap_local = shap.explain_local(X_test[:5], y_test[:5])\n",
    "\n",
    "# show(shap_local, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isEager = tf.executing_eagerly()\n",
    "isEager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS = 41\n",
    "NUM_TARGETS = 100\n",
    "NUM_TIME_STEPS = 21\n",
    "NUM_REPLICATES = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gXpISOijEYqQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# matrix_path = \"regulator-gene-matrix.csv\"\n",
    "# data_path_syn = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\synData\"\n",
    "# data_path_inter =  r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\interpolatedOnly\"\n",
    "# data_path_og_exp1 = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\Fin_preProcessed\\datasets\\exp1\"\n",
    "# data_path_testSet = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\testSetFixed\"\n",
    "# data_path_petal = r\"C:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\petal_len.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirty_RGM = r'Regulations_Control_Altona.csv'\n",
    "dirty_regulations = r'insilico_size100_1_timeseries.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 21, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirtyReg = pd.read_csv(dirty_regulations,  index_col = 0,)# on_bad_lines='skip')\n",
    "dirtyReg = dirtyReg.dropna(axis=0)\n",
    "dirtyReg = dirtyReg.select_dtypes(include=np.number)\n",
    "dirtyReg = dirtyReg.to_numpy() #reshape would not work in this case. \n",
    "\n",
    "dRegs = np.zeros(shape=(NUM_REPLICATES, NUM_TIME_STEPS, NUM_TARGETS), dtype=np.float)\n",
    "for i in range(NUM_REPLICATES):\n",
    "    dRegs[i] = dirtyReg[NUM_TIME_STEPS*i : NUM_TIME_STEPS*(i+1)]\n",
    "dRegs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 21, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def fix_dataset(dirtyR):\n",
    "    ds = dirtyR\n",
    "    ret = np.zeros(shape=(NUM_REPLICATES,NUM_TIME_STEPS, NUM_TARGETS))\n",
    "\n",
    "    # for i in range(0,NUM_REPLICATES*NUM_TIME_STEPS, 4):\n",
    "    #     for j in range(0, NUM_REPLICATES):\n",
    "    #         dataset[j][:,i//NUM_REPLICATES] = dirtyR[:,(i+j)]\n",
    "    \n",
    "    ds[ds==0.0] = np.nan #we do this so the scaling ignores 0.0 #CHECKED\n",
    "\n",
    "\n",
    "    for i in range(NUM_REPLICATES):\n",
    "        regScaled = StandardScaler().fit_transform(ds[i].flatten().reshape((-1,1)))\n",
    "        regScaled = MinMaxScaler().fit_transform(ds[i].flatten().reshape((-1,1))) #ignores np.nan\n",
    "        regScaled = regScaled.reshape((NUM_TARGETS, NUM_TIME_STEPS))\n",
    "        regScaled = np.nan_to_num(regScaled, nan= 0.0)\n",
    "        ret[i] = regScaled.T\n",
    "    return ret\n",
    "\n",
    "dataset = fix_dataset(dirtyR=dRegs)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15300698, 0.17242697, 0.79758853, ..., 0.08191029, 0.12975706,\n",
       "        0.03934641],\n",
       "       [0.27451402, 0.22530717, 0.61496231, ..., 0.07429333, 0.2986516 ,\n",
       "        0.67589515],\n",
       "       [0.70780315, 0.16051352, 0.909139  , ..., 0.97443771, 0.01375137,\n",
       "        0.16942395],\n",
       "       ...,\n",
       "       [0.09252425, 0.74741258, 0.01219214, ..., 0.45413509, 0.03334866,\n",
       "        0.75625035],\n",
       "       [0.00253184, 0.09286092, 0.39454038, ..., 0.40550657, 0.12740767,\n",
       "        0.63230157],\n",
       "       [0.06762939, 0.72373928, 0.69529719, ..., 0.28241947, 0.1940238 ,\n",
       "        0.72104984]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 21, 100) (2, 21, 100) (10, 21, 100)\n"
     ]
    }
   ],
   "source": [
    "beanIntensities = dataset[0:8]\n",
    "validation = dataset[8:]\n",
    "allData = dataset\n",
    "print(beanIntensities.shape, validation.shape, allData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the third replicate is trash. we will not use it. \n",
    "# df(dataset[3]).head(11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regulator_gene_matrix = np.load(\"soyBeanRGM.npy\")\n",
    "# regulator_gene_matrix = regulator_gene_matrix.astype('float32')\n",
    "# regulator_gene_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Compare Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G1</td>\n",
       "      <td>G2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G1</td>\n",
       "      <td>G3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G1</td>\n",
       "      <td>G4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G5</td>\n",
       "      <td>G2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G5</td>\n",
       "      <td>G3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1  2\n",
       "0  G1  G2  1\n",
       "1  G1  G3  1\n",
       "2  G1  G4  1\n",
       "3  G5  G2  1\n",
       "4  G5  G3  1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = pd.read_csv(\"DREAM4_GoldStandard_InSilico_Size100_1.csv\", header=None)\n",
    "gold.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_numeric(df):\n",
    "\n",
    "    return df.applymap(lambda x: ''.join(filter(str.isdigit, str(x))) if isinstance(x, (int,float)) else ''.join(filter(str.isdigit, x)) )\n",
    "gold = keep_numeric(gold)\n",
    "goldnp = np.array(gold, dtype = 'int')\n",
    "#subtract 1 from each index to match python index\n",
    "goldnp[:,0] = goldnp[:,0] - 1\n",
    "goldnp[:,1] = goldnp[:,1] - 1\n",
    "\n",
    "goldIm = np.zeros(shape=(NUM_TARGETS,NUM_TARGETS))\n",
    "\n",
    "for g in goldnp:\n",
    "    reg = g[0]\n",
    "    tar = g[1]\n",
    "    connection = g[2]\n",
    "    goldIm[reg][tar] = connection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x292a0cb1280>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(goldIm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero = np.nonzero(goldIm) #non-zero hold all the true connections\n",
    "#non_zero = np.array(np.unravel_index(non_zero, shape = (NUM_PARENTS, NUM_TARGETS)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGM and RGM+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of parent index (41,)\n"
     ]
    }
   ],
   "source": [
    "regulator_gene_matrix = goldIm.astype(np.float32) #I set rgm to gold because I didnt not want ot re-write everything\n",
    "superParent = regulator_gene_matrix.copy()\n",
    "ones = np.ones((NUM_TARGETS))\n",
    "parentIndex = []\n",
    "not_parentIndex = []\n",
    "for i in range(len(regulator_gene_matrix)):\n",
    "    if (np.isin(regulator_gene_matrix[i], [1])).any():\n",
    "        #print(i)\n",
    "        superParent[i] = ones \n",
    "        parentIndex.append(i)\n",
    "    else:\n",
    "        not_parentIndex.append(i)\n",
    "\n",
    "parentIndex = np.array(parentIndex)\n",
    "parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "parent_idx = parentIndex.numpy()\n",
    "not_parentIndex = np.array(not_parentIndex)\n",
    "not_parentIndex = tf.convert_to_tensor(not_parentIndex)\n",
    "print(\"shape of parent index\", parentIndex.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#the code below runs the experiment without any known regulators\n",
    "regulator_gene_matrix = np.ones((NUM_TARGETS,NUM_TARGETS), dtype='float32') #we keep this because we want to not mess with the weight init\n",
    "# superParent = np.ones((NUM_TARGETS, NUM_TARGETS), dtype = 'float32')\n",
    "# parentIndex = np.arange(stop=NUM_PARENTS)\n",
    "# parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "# parent_idx = parentIndex.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(41,), dtype=int32, numpy=\n",
       "array([ 0,  4,  9, 14, 22, 24, 25, 35, 36, 37, 39, 41, 42, 43, 44, 45, 53,\n",
       "       54, 56, 61, 62, 63, 64, 65, 66, 68, 71, 72, 74, 81, 82, 84, 86, 89,\n",
       "       90, 91, 92, 95, 97, 98, 99])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parentIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(superParent, cmap='hot');\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(regulator_gene_matrix, cmap='hot');\n",
    "plt.xlabel(\"Substrate\");\n",
    "plt.ylabel(\"Regulator\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# superParent = regulator_gene_matrix.copy() #init the super parent with the ordinary RGM, and do forward passes with super parent\n",
    "# #print(superParent.shape)\n",
    "\n",
    "# ones = np.ones((NUM_TARGETS))\n",
    "# parentIndex = []\n",
    "# not_parentIndex = []\n",
    "# for i in range(len(regulator_gene_matrix)):\n",
    "#     if (np.isin(regulator_gene_matrix[i], [1])).any():\n",
    "#         #print(i)\n",
    "#         superParent[i] = ones \n",
    "#         parentIndex.append(i)\n",
    "#     else:\n",
    "#         not_parentIndex.append(i)\n",
    "\n",
    "# parentIndex = np.array(parentIndex)\n",
    "# parentIndex = tf.convert_to_tensor(parentIndex)\n",
    "# parent_idx = parentIndex.numpy()\n",
    "# not_parentIndex = np.array(not_parentIndex)\n",
    "# not_parentIndex = tf.convert_to_tensor(not_parentIndex)\n",
    "# print(\"shape of parent index\", parentIndex.shape)\n",
    "\n",
    "def ignore_noParent_MSE_old(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "    y_true_pruned = tf.gather(y_true,parentIndex, axis =2) \n",
    "    #print(y_true_pruned.shape\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis =2)   \n",
    "    return l(y_true_pruned, y_pred_pruned)\n",
    "\n",
    "#this will not work if the entire dataset is -1 (degenerate), or has only one actual value (also degen)\n",
    "def ignore_noParent_MSE(y_true, y_pred): \n",
    "    l = tf.keras.losses.MeanSquaredError()\n",
    "   # print(y_true.shape) #(None, 44, 372)\n",
    "\n",
    "    #get the parents and flatten them\n",
    "    y_true_pruned = tf.gather(y_true, parentIndex, axis = 2) #axis 2 because batch, time, gene\n",
    "    y_true_pruned = tf.reshape(y_true_pruned, shape=([tf.size(y_true_pruned)] ) )\n",
    "\n",
    "   # print(y_true_pruned.shape)\n",
    "   # print(\"tf size\", tf.size(y_true_pruned))\n",
    "\n",
    "    y_pred_pruned = tf.gather(y_pred, parentIndex, axis = 2) \n",
    "    y_pred_pruned = tf.reshape(y_pred_pruned, shape=([tf.size(y_pred_pruned)]) )\n",
    "\n",
    "    #get the index of the parents which are not -1\n",
    "    y_true_posID = tf.where(y_true_pruned >= 0) #gets args\n",
    "    y_true_posID = tf.squeeze(y_true_posID)\n",
    "    #get the idx of all the -1s \n",
    "    y_true_negID = tf.where(y_true_pruned < 0) \n",
    "    y_true_negID = tf.squeeze(y_true_negID)\n",
    "\n",
    "    #get all the -1s in the parents \n",
    "    y_true_neg = tf.gather(y_true_pruned, y_true_negID) #get all the -1s in y_true\n",
    "    y_pred_neg = tf.gather(y_pred_pruned, y_true_negID) #get the corresponding values for y_pred\n",
    "\n",
    "    #get the indexes where pred should be -1 but is not. get the corresponding index for ytrue\n",
    "    y_shouldBeNegButIsntID = tf.where(y_pred_neg >= 0)  \n",
    "    y_shouldBeNegButIsntID = tf.squeeze(y_shouldBeNegButIsntID) #get the idx which should be -1 for prediction but are not\n",
    "    y_true_wrong = tf.gather(y_true_pruned, y_shouldBeNegButIsntID) #get the same corresponding values from ytrue\n",
    "    y_shouldBeNegButIsnt = tf.gather(y_pred_pruned, y_shouldBeNegButIsntID) #this has all the wrongly predicted values which should be -1 but are not\n",
    "\n",
    "    y_true_pos = tf.gather(y_true_pruned, y_true_posID)\n",
    "    y_pred_pos = tf.gather(y_pred_pruned, y_true_posID)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 0: #we can not concatenate if the size is 0. \n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "    if tf.size(y_shouldBeNegButIsnt) == 1: #dim goes away if size = 1. \n",
    "        y_shouldBeNegButIsnt = tf.expand_dims(y_shouldBeNegButIsnt, axis = 0) #should all be flattened\n",
    "        y_true_wrong = tf.expand_dims(y_true_wrong, axis=0)\n",
    "\n",
    "    #print(\"y_pred\", (y_pred_pos), \"y_true\", (y_shouldBeNegButIsnt))\n",
    "    try:\n",
    "        y_pred_total = tf.concat([y_pred_pos, y_shouldBeNegButIsnt], axis = 0) #concatenate for total mse\n",
    "        y_true_total = tf.concat([y_true_pos, y_true_wrong], axis = 0)\n",
    "    except Exception as e:\n",
    "        print(y_pred_pos.shape, y_shouldBeNegButIsnt.shape, tf.size(y_shouldBeNegButIsnt))\n",
    "        return l(y_true_pos, y_pred_pos)\n",
    "\n",
    "    return l(y_true_total, y_pred_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  4,  9, 14, 22, 24, 25, 35, 36, 37, 39, 41, 42, 43, 44, 45, 53,\n",
       "       54, 56, 61, 62, 63, 64, 65, 66, 68, 71, 72, 74, 81, 82, 84, 86, 89,\n",
       "       90, 91, 92, 95, 97, 98, 99])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1yhzyikoFncq"
   },
   "outputs": [],
   "source": [
    "# class EncoderLinear(tf.keras.layers.Layer):\n",
    "#     def __init__(self, rgm, input_dim=32, units=32):\n",
    "#         super(EncoderLinear, self).__init__()\n",
    "#         self.rgm = rgm\n",
    "        \n",
    "#         def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "#             w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.rgm, dtype=dtype)\n",
    "\n",
    "#             return w_init\n",
    "        \n",
    "\n",
    "#         self.w = tf.Variable(\n",
    "#             initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "#             trainable=True,\n",
    "#         )\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         X = inputs\n",
    "#         return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4uchxU-bmBDe"
   },
   "outputs": [],
   "source": [
    "# class DecoderLinear(tf.keras.layers.Layer):\n",
    "#     def __init__(self, rgm, input_dim=32, units=32):\n",
    "#         super(DecoderLinear, self).__init__()\n",
    "#         self.rgm = rgm\n",
    "\n",
    "#         def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "#             w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "#             return w_init\n",
    "    \n",
    "        \n",
    "#         self.w = tf.Variable(\n",
    "#             initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "#             trainable=True,\n",
    "#         )\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         X = inputs\n",
    "#         #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "#         X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "#         #return tf.matmul(inputs, self.w)\n",
    "#         # v = tf.zeros_like(X)\n",
    "#         # u = tf.ones_like(X)\n",
    "#         # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "#         return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "PQVz4BCpmNMd"
   },
   "outputs": [],
   "source": [
    "# def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "#     '''\n",
    "#     Encoder structure\n",
    "#     '''\n",
    "#     '''\n",
    "#     The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "#     the intensities for each gene.\n",
    "#     '''\n",
    "#     en_conv = Conv1D(490, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association)\n",
    "#     en_dense = Flatten()(en_conv)\n",
    "#     phenotype = Dense(num_hidden_units)(en_dense)\n",
    "#     return phenotype\n",
    "\n",
    "# def decoder(X, num_protein_gene, time_steps):\n",
    "#     '''\n",
    "#     Decoder structure\n",
    "#     '''\n",
    "#     de_dense = Dense(1024)(X)#Dense(128)(X)\n",
    "#     de_dense = Reshape((1, 1024))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "#     de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "#     #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "#     # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "#     return de_deconv\n",
    "\n",
    "# def model(rgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 32):\n",
    "#     inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "#     x = EncoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "#     enc = encoder(x, num_hidden_units)\n",
    "#     dec = decoder(enc, num_protein_gene, time_steps)\n",
    "#     out = DecoderLinear(rgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "#     _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "#     return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinaryAE = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS)\n",
    "# ordinaryAE.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = ordinaryAE.fit(beanIntensities, beanIntensities, epochs=200, verbose = True, validation_data=(validation, validation))\n",
    "#print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super Parent AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLinearSuperParent(tf.keras.layers.Layer, tfmot.sparsity.keras.PrunableLayer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(EncoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "\n",
    "\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.OGrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def get_prunable_weights(self):\n",
    "        # Prune bias also, though that usually harms model accuracy too much.\n",
    "        return [self.w]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "class DecoderLinearSuperParent(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DecoderLinearSuperParent, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.OGrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.rgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "    \n",
    "        \n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        #return tf.matmul(X, tf.multiply((self.rgm), self.w))\n",
    "        X = tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) \n",
    "        #return tf.matmul(inputs, self.w)\n",
    "        # v = tf.zeros_like(X)\n",
    "        # u = tf.ones_like(X)\n",
    "        # u = tf.math.scalar_mul(-3.0, u)\n",
    "        \n",
    "        return X#tf.where(tf.math.less(X, v), u, X) #where X is less than 0, return -1 \n",
    "        \n",
    "        \n",
    "def encoder(parent_child_biological_association, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    rnn = LSTM(units = num_hidden_units)(parent_child_biological_association)\n",
    "    en_conv = Conv1D(32, 3, activation = \"relu\")(parent_child_biological_association) # 6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    phenotype = Dense(num_hidden_units)(en_dense)\n",
    "    return phenotype\n",
    "\n",
    "def decoder(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(128)(X)\n",
    "    de_dense = Reshape((1, 128))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"relu\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelSuperParent(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21, sparsity = 0.0): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = tfmot.sparsity.keras.prune_low_magnitude(EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS),\n",
    "                                                 pruning_schedule = tfmot.sparsity.keras.ConstantSparsity(sparsity, 0))(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSuperParentSequential(rgm, oldRGM, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21): #rgm is set to superparent, oldrgm is original rgm unmodified\n",
    "    m = tf.keras.Sequential()\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    x = EncoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    enc = encoder(x, num_hidden_units)\n",
    "    dec = decoder(enc, num_protein_gene, time_steps)\n",
    "    out = DecoderLinearSuperParent(rgm, oldRGM, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.2220 - val_loss: 0.2030\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2193 - val_loss: 0.1996\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2151 - val_loss: 0.1925\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2069 - val_loss: 0.1803\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1927 - val_loss: 0.1611\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1708 - val_loss: 0.1345\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1409 - val_loss: 0.1054\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1087 - val_loss: 0.0913\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0939 - val_loss: 0.1001\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1045 - val_loss: 0.0860\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0888 - val_loss: 0.0656\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0654 - val_loss: 0.0552\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0531 - val_loss: 0.0534\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0505 - val_loss: 0.0533\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0500 - val_loss: 0.0510\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0473 - val_loss: 0.0466\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0425 - val_loss: 0.0421\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0378 - val_loss: 0.0401\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0361 - val_loss: 0.0408\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0373 - val_loss: 0.0412\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0378 - val_loss: 0.0391\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0351 - val_loss: 0.0362\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0312 - val_loss: 0.0348\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0290 - val_loss: 0.0349\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0286 - val_loss: 0.0351\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0286 - val_loss: 0.0345\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0276 - val_loss: 0.0329\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0259 - val_loss: 0.0312\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0241 - val_loss: 0.0303\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0232 - val_loss: 0.0301\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0232 - val_loss: 0.0299\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0230 - val_loss: 0.0292\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0219 - val_loss: 0.0283\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0206 - val_loss: 0.0278\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0198 - val_loss: 0.0279\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0196 - val_loss: 0.0280\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0195 - val_loss: 0.0277\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0191 - val_loss: 0.0270\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0184 - val_loss: 0.0264\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0178 - val_loss: 0.0261\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0175 - val_loss: 0.0260\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0175 - val_loss: 0.0258\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0172 - val_loss: 0.0255\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0167 - val_loss: 0.0253\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0163 - val_loss: 0.0252\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0161 - val_loss: 0.0252\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0160 - val_loss: 0.0251\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0158 - val_loss: 0.0248\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0155 - val_loss: 0.0245\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0152 - val_loss: 0.0243\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0151 - val_loss: 0.0242\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0150 - val_loss: 0.0241\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0149 - val_loss: 0.0240\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0147 - val_loss: 0.0239\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0145 - val_loss: 0.0240\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0145 - val_loss: 0.0240\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0144 - val_loss: 0.0239\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0143 - val_loss: 0.0238\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0142 - val_loss: 0.0237\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0141 - val_loss: 0.0236\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0140 - val_loss: 0.0236\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0140 - val_loss: 0.0235\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0139 - val_loss: 0.0235\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0139 - val_loss: 0.0235\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0138 - val_loss: 0.0236\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0138 - val_loss: 0.0236\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0137 - val_loss: 0.0236\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0137 - val_loss: 0.0235\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0136 - val_loss: 0.0234\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0136 - val_loss: 0.0234\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0136 - val_loss: 0.0234\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0135 - val_loss: 0.0234\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0135 - val_loss: 0.0234\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0135 - val_loss: 0.0234\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0134 - val_loss: 0.0234\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0134 - val_loss: 0.0234\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0134 - val_loss: 0.0233\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0134 - val_loss: 0.0233\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0133 - val_loss: 0.0233\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0133 - val_loss: 0.0233\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0133 - val_loss: 0.0232\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0133 - val_loss: 0.0232\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0133 - val_loss: 0.0232\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0132 - val_loss: 0.0232\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0132 - val_loss: 0.0232\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0132 - val_loss: 0.0232\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0132 - val_loss: 0.0231\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0132 - val_loss: 0.0231\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0131 - val_loss: 0.0231\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0131 - val_loss: 0.0231\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0131 - val_loss: 0.0231\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0131 - val_loss: 0.0231\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0130 - val_loss: 0.0231\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0130 - val_loss: 0.0231\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0130 - val_loss: 0.0230\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0130 - val_loss: 0.0230\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0129 - val_loss: 0.0230\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0129 - val_loss: 0.0230\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0129 - val_loss: 0.0230\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0128 - val_loss: 0.0230\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0128 - val_loss: 0.0230\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0128 - val_loss: 0.0229\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0127 - val_loss: 0.0229\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0127 - val_loss: 0.0229\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0127 - val_loss: 0.0229\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0126 - val_loss: 0.0228\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0126 - val_loss: 0.0228\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0125 - val_loss: 0.0228\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0125 - val_loss: 0.0228\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0124 - val_loss: 0.0227\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0124 - val_loss: 0.0227\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0122 - val_loss: 0.0226\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0120 - val_loss: 0.0224\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0120 - val_loss: 0.0224\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0119 - val_loss: 0.0223\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0118 - val_loss: 0.0223\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0117 - val_loss: 0.0222\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0117 - val_loss: 0.0222\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0116 - val_loss: 0.0221\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0115 - val_loss: 0.0221\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0114 - val_loss: 0.0220\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0113 - val_loss: 0.0220\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0113 - val_loss: 0.0220\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0112 - val_loss: 0.0219\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0111 - val_loss: 0.0219\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0110 - val_loss: 0.0218\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0109 - val_loss: 0.0218\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0108 - val_loss: 0.0218\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0107 - val_loss: 0.0217\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0107 - val_loss: 0.0217\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0106 - val_loss: 0.0217\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0105 - val_loss: 0.0216\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0104 - val_loss: 0.0216\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0103 - val_loss: 0.0216\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0102 - val_loss: 0.0215\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0101 - val_loss: 0.0215\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0101 - val_loss: 0.0215\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0100 - val_loss: 0.0214\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0099 - val_loss: 0.0214\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0098 - val_loss: 0.0214\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0097 - val_loss: 0.0213\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0096 - val_loss: 0.0213\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0095 - val_loss: 0.0213\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0094 - val_loss: 0.0212\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0093 - val_loss: 0.0212\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0093 - val_loss: 0.0212\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0092 - val_loss: 0.0212\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0091 - val_loss: 0.0211\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0090 - val_loss: 0.0211\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0089 - val_loss: 0.0211\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0088 - val_loss: 0.0210\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0087 - val_loss: 0.0210\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0086 - val_loss: 0.0210\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0085 - val_loss: 0.0210\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0084 - val_loss: 0.0209\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0083 - val_loss: 0.0209\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0082 - val_loss: 0.0209\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0081 - val_loss: 0.0209\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0081 - val_loss: 0.0208\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0080 - val_loss: 0.0208\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0079 - val_loss: 0.0208\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0078 - val_loss: 0.0208\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0077 - val_loss: 0.0208\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0076 - val_loss: 0.0207\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0075 - val_loss: 0.0207\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0074 - val_loss: 0.0207\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0073 - val_loss: 0.0207\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0072 - val_loss: 0.0206\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0072 - val_loss: 0.0206\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0071 - val_loss: 0.0206\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0070 - val_loss: 0.0206\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0069 - val_loss: 0.0206\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0068 - val_loss: 0.0205\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0067 - val_loss: 0.0205\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0067 - val_loss: 0.0205\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0066 - val_loss: 0.0205\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0065 - val_loss: 0.0205\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0064 - val_loss: 0.0205\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0063 - val_loss: 0.0205\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.0063 - val_loss: 0.0205\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0062 - val_loss: 0.0204\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0061 - val_loss: 0.0204\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0060 - val_loss: 0.0204\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0059 - val_loss: 0.0204\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0059 - val_loss: 0.0204\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0058 - val_loss: 0.0204\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0057 - val_loss: 0.0204\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0056 - val_loss: 0.0204\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0056 - val_loss: 0.0204\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0055 - val_loss: 0.0204\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0054 - val_loss: 0.0204\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0053 - val_loss: 0.0204\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0053 - val_loss: 0.0204\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0052 - val_loss: 0.0204\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0051 - val_loss: 0.0203\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0051 - val_loss: 0.0203\n",
      "0.005069492384791374\n"
     ]
    }
   ],
   "source": [
    "looseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "looseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "o = looseParent.fit(beanIntensities, beanIntensities, epochs=200, verbose = True,  \n",
    "                    validation_data=(validation, validation), callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()])\n",
    "print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 21, 100)]         0         \n",
      "                                                                 \n",
      " prune_low_magnitude_encoder  (None, 21, 100)          20002     \n",
      " _linear_super_parent (Prune                                     \n",
      " LowMagnitude)                                                   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 19, 32)            9632      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 608)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                19488     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               4224      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 128)            0         \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 21, 100)          268900    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " decoder_linear_super_parent  (None, 21, 100)          10000     \n",
      "  (DecoderLinearSuperParent)                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 332,246\n",
      "Trainable params: 322,244\n",
      "Non-trainable params: 10,002\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "looseParent.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNetAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "a second copy of the layers which will be modified to be a denseNET auto encoder\n",
    "'''\n",
    "\n",
    "class DAE_Encoder_MASK(tf.keras.layers.Layer): \n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DAE_Encoder_MASK, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "        \n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.convert_to_tensor(self.oldrgm, dtype=dtype)\n",
    "\n",
    "            return w_init\n",
    "        \n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(self.rgm, self.w))\n",
    "    #tf.matmul(inputs, self.w)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "\n",
    "class DAE_Decoder_MASK(tf.keras.layers.Layer):\n",
    "    def __init__(self, rgm, oldrgm, input_dim=32, units=32):\n",
    "        super(DAE_Decoder_MASK, self).__init__()\n",
    "        self.rgm = rgm\n",
    "        self.oldrgm = oldrgm\n",
    "\n",
    "        def init_weights(shape, dtype=\"float32\"):\n",
    "\n",
    "            w_init = tf.random_normal_initializer()(shape=shape, dtype=dtype) * tf.transpose(tf.convert_to_tensor(self.oldrgm, dtype=dtype))\n",
    "\n",
    "            return w_init\n",
    "\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=init_weights(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        return tf.matmul(X, tf.multiply(tf.transpose(self.rgm), self.w)) #used to have a transpose\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"rgm\": self.rgm,\n",
    "            \"oldrgm\": self.oldrgm,\n",
    "            'input_dim': 32,\n",
    "            'units' : 32\n",
    "        })\n",
    "        \n",
    "\n",
    "def denseencoder2(parent_child_biological_association, inp, num_hidden_units=21):\n",
    "    '''\n",
    "    Encoder structure\n",
    "    '''\n",
    "    '''\n",
    "    The data is time-series. Therefore, CNN to learn the temporal relationship between \n",
    "    the intensities for each gene.\n",
    "    '''\n",
    "    en_conv = Conv1D(NUM_TARGETS, 7, activation = \"tanh\")(parent_child_biological_association) # Conv1D(NUM_TARGETS, NUM_TIME_STEPS, activation = \"tanh\")(parent_child_biological_association)6*NUM_TARGETS\n",
    "    en_dense = Flatten()(en_conv)\n",
    "    inp = Flatten()(inp)\n",
    "    d = Concatenate()([en_dense, inp]) #dense layer\n",
    "    phenotype = Dense(num_hidden_units, activation=\"tanh\")(d)\n",
    "    return phenotype\n",
    "\n",
    "def densedecoder2(X, num_protein_gene, time_steps):\n",
    "    '''\n",
    "    Decoder structure\n",
    "    '''\n",
    "    de_dense = Dense(NUM_TARGETS, activation = 'tanh')(X)\n",
    "    de_dense = Reshape((1, NUM_TARGETS))(de_dense) #tf.reshape(de_dense, (self.batch_size,1,128))\n",
    "    de_deconv = Conv1DTranspose(num_protein_gene, time_steps, activation = \"tanh\")(de_dense) #used to be transpose\n",
    "    #de_deconv = Conv1D(num_protein_gene, time_steps, activation = \"relu\")(de_dense) \n",
    "    # gene_reconstruction = self.decoder_biological_operation(de_deconv)\n",
    "    return de_deconv\n",
    "\n",
    "def modelDense2(rgm, oldrgm, num_protein_gene, time_steps, num_kinase_regulators, num_hidden_units = 21):\n",
    "    inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "    \n",
    "    x = DAE_Encoder_MASK(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "    #x = EncoderLinear2(x)\n",
    "    enc = denseencoder2(x, inp, num_hidden_units)\n",
    "    dec = densedecoder2(enc, num_protein_gene, time_steps)\n",
    "    out = DAE_Decoder_MASK(rgm, oldrgm, NUM_TARGETS, NUM_TARGETS)(dec)\n",
    "\n",
    "    _model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2235 - val_loss: 0.2029\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2200 - val_loss: 0.1771\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1922 - val_loss: 0.1520\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1650 - val_loss: 0.1282\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1391 - val_loss: 0.1047\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1134 - val_loss: 0.0827\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0890 - val_loss: 0.0646\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0682 - val_loss: 0.0522\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0528 - val_loss: 0.0463\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0440 - val_loss: 0.0455\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0405 - val_loss: 0.0463\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0391 - val_loss: 0.0462\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0374 - val_loss: 0.0442\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0345 - val_loss: 0.0409\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0309 - val_loss: 0.0374\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0273 - val_loss: 0.0345\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0246 - val_loss: 0.0327\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0232 - val_loss: 0.0318\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0227 - val_loss: 0.0314\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0226 - val_loss: 0.0311\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0225 - val_loss: 0.0307\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0221 - val_loss: 0.0300\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0214 - val_loss: 0.0293\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0205 - val_loss: 0.0286\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0196 - val_loss: 0.0281\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0187 - val_loss: 0.0278\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0181 - val_loss: 0.0276\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0176 - val_loss: 0.0275\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0172 - val_loss: 0.0274\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0169 - val_loss: 0.0272\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0166 - val_loss: 0.0269\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0163 - val_loss: 0.0266\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0160 - val_loss: 0.0263\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0158 - val_loss: 0.0260\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0156 - val_loss: 0.0258\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0156 - val_loss: 0.0256\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0155 - val_loss: 0.0255\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0155 - val_loss: 0.0253\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0154 - val_loss: 0.0252\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0153 - val_loss: 0.0251\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0152 - val_loss: 0.0250\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0151 - val_loss: 0.0249\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0150 - val_loss: 0.0249\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0149 - val_loss: 0.0249\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0148 - val_loss: 0.0249\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0148 - val_loss: 0.0249\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0147 - val_loss: 0.0248\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0147 - val_loss: 0.0248\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0146 - val_loss: 0.0246\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0146 - val_loss: 0.0245\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0146 - val_loss: 0.0244\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0145 - val_loss: 0.0243\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0145 - val_loss: 0.0242\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0145 - val_loss: 0.0241\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0145 - val_loss: 0.0241\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0145 - val_loss: 0.0240\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0145 - val_loss: 0.0240\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0144 - val_loss: 0.0240\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0144 - val_loss: 0.0241\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0144 - val_loss: 0.0241\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0144 - val_loss: 0.0242\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0144 - val_loss: 0.0242\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0144 - val_loss: 0.0242\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0143 - val_loss: 0.0242\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0143 - val_loss: 0.0242\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0143 - val_loss: 0.0242\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0143 - val_loss: 0.0242\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0143 - val_loss: 0.0241\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0143 - val_loss: 0.0241\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0143 - val_loss: 0.0240\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0143 - val_loss: 0.0239\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0143 - val_loss: 0.0239\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0143 - val_loss: 0.0240\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0142 - val_loss: 0.0240\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0142 - val_loss: 0.0240\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0142 - val_loss: 0.0239\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0142 - val_loss: 0.0239\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0141 - val_loss: 0.0240\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0141 - val_loss: 0.0240\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0141 - val_loss: 0.0240\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0140 - val_loss: 0.0239\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0140 - val_loss: 0.0239\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0140 - val_loss: 0.0239\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0139 - val_loss: 0.0239\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0139 - val_loss: 0.0238\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0139 - val_loss: 0.0238\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0138 - val_loss: 0.0237\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0138 - val_loss: 0.0237\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0138 - val_loss: 0.0237\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0137 - val_loss: 0.0237\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0137 - val_loss: 0.0236\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0136 - val_loss: 0.0236\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0136 - val_loss: 0.0235\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0135 - val_loss: 0.0235\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0135 - val_loss: 0.0235\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0134 - val_loss: 0.0234\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0134 - val_loss: 0.0234\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0133 - val_loss: 0.0233\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0133 - val_loss: 0.0233\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0132 - val_loss: 0.0232\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0132 - val_loss: 0.0232\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0131 - val_loss: 0.0232\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0131 - val_loss: 0.0231\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0130 - val_loss: 0.0231\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0130 - val_loss: 0.0230\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0129 - val_loss: 0.0230\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0129 - val_loss: 0.0230\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0128 - val_loss: 0.0230\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0128 - val_loss: 0.0229\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0127 - val_loss: 0.0229\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0127 - val_loss: 0.0228\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0126 - val_loss: 0.0228\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0126 - val_loss: 0.0228\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0125 - val_loss: 0.0228\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0125 - val_loss: 0.0227\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0124 - val_loss: 0.0227\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0124 - val_loss: 0.0226\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0123 - val_loss: 0.0226\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0122 - val_loss: 0.0226\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.0122 - val_loss: 0.0225\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.0121 - val_loss: 0.0225\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0120 - val_loss: 0.0224\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0120 - val_loss: 0.0224\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0119 - val_loss: 0.0224\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0119 - val_loss: 0.0223\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0118 - val_loss: 0.0223\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0118 - val_loss: 0.0223\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0117 - val_loss: 0.0223\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0117 - val_loss: 0.0223\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0117 - val_loss: 0.0222\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0116 - val_loss: 0.0222\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0116 - val_loss: 0.0222\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0115 - val_loss: 0.0222\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0115 - val_loss: 0.0222\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0115 - val_loss: 0.0221\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0114 - val_loss: 0.0221\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0114 - val_loss: 0.0221\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0114 - val_loss: 0.0221\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0113 - val_loss: 0.0221\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0113 - val_loss: 0.0221\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0113 - val_loss: 0.0220\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0112 - val_loss: 0.0220\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0112 - val_loss: 0.0220\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0112 - val_loss: 0.0220\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0112 - val_loss: 0.0220\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0112 - val_loss: 0.0219\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0112 - val_loss: 0.0220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x292aecdd9d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, num_hidden_units=NUM_PARENTS)\n",
    "dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "dense.fit(beanIntensities, beanIntensities, epochs=150,  verbose=True, validation_data=(validation, validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 21, 100)]    0           []                               \n",
      "                                                                                                  \n",
      " dae__encoder_mask (DAE_Encoder  (None, 21, 100)     10000       ['input_2[0][0]']                \n",
      " _MASK)                                                                                           \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 15, 100)      70100       ['dae__encoder_mask[0][0]']      \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 1500)         0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 2100)         0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 3600)         0           ['flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 41)           147641      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 100)          4200        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 100)       0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_transpose_1 (Conv1DTran  (None, 21, 100)     210100      ['reshape_1[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " dae__decoder_mask (DAE_Decoder  (None, 21, 100)     10000       ['conv1d_transpose_1[0][0]']     \n",
      " _MASK)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 452,041\n",
      "Trainable params: 452,041\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dense.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 182ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 21, 100)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = dense.predict(validation)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(o[0].reshape(NUM_TIME_STEPS,NUM_TARGETS))\n",
    "v = pd.DataFrame(validation[0].reshape(NUM_TIME_STEPS,NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>4</th>\n",
       "      <th>9</th>\n",
       "      <th>14</th>\n",
       "      <th>22</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>...</th>\n",
       "      <th>84</th>\n",
       "      <th>86</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>95</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.136075</td>\n",
       "      <td>0.644643</td>\n",
       "      <td>0.655718</td>\n",
       "      <td>0.431823</td>\n",
       "      <td>0.574567</td>\n",
       "      <td>0.653610</td>\n",
       "      <td>0.633507</td>\n",
       "      <td>0.314956</td>\n",
       "      <td>0.255302</td>\n",
       "      <td>0.165078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520167</td>\n",
       "      <td>0.715751</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.652095</td>\n",
       "      <td>0.394871</td>\n",
       "      <td>0.186444</td>\n",
       "      <td>0.742264</td>\n",
       "      <td>0.044102</td>\n",
       "      <td>0.112685</td>\n",
       "      <td>0.019643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.329785</td>\n",
       "      <td>0.172821</td>\n",
       "      <td>0.524054</td>\n",
       "      <td>0.742346</td>\n",
       "      <td>0.290984</td>\n",
       "      <td>0.070555</td>\n",
       "      <td>0.638568</td>\n",
       "      <td>0.204389</td>\n",
       "      <td>0.333625</td>\n",
       "      <td>0.375461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459012</td>\n",
       "      <td>0.476434</td>\n",
       "      <td>0.152263</td>\n",
       "      <td>0.506406</td>\n",
       "      <td>0.044304</td>\n",
       "      <td>0.147512</td>\n",
       "      <td>0.259074</td>\n",
       "      <td>0.021597</td>\n",
       "      <td>0.242836</td>\n",
       "      <td>0.434171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.496259</td>\n",
       "      <td>0.194111</td>\n",
       "      <td>0.420594</td>\n",
       "      <td>0.334187</td>\n",
       "      <td>0.429614</td>\n",
       "      <td>0.708858</td>\n",
       "      <td>0.463489</td>\n",
       "      <td>0.083636</td>\n",
       "      <td>0.168406</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436818</td>\n",
       "      <td>0.062092</td>\n",
       "      <td>0.002873</td>\n",
       "      <td>0.646824</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>0.750723</td>\n",
       "      <td>0.653464</td>\n",
       "      <td>0.692647</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.127510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.383825</td>\n",
       "      <td>0.053211</td>\n",
       "      <td>0.599283</td>\n",
       "      <td>0.666747</td>\n",
       "      <td>0.412962</td>\n",
       "      <td>0.548038</td>\n",
       "      <td>0.373123</td>\n",
       "      <td>0.150179</td>\n",
       "      <td>0.367594</td>\n",
       "      <td>0.462709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536762</td>\n",
       "      <td>0.457959</td>\n",
       "      <td>0.070745</td>\n",
       "      <td>0.463101</td>\n",
       "      <td>0.031537</td>\n",
       "      <td>0.773964</td>\n",
       "      <td>0.594639</td>\n",
       "      <td>0.078160</td>\n",
       "      <td>0.265225</td>\n",
       "      <td>0.720895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.653280</td>\n",
       "      <td>-0.003362</td>\n",
       "      <td>0.486367</td>\n",
       "      <td>0.567220</td>\n",
       "      <td>0.437471</td>\n",
       "      <td>0.072625</td>\n",
       "      <td>0.480086</td>\n",
       "      <td>0.704411</td>\n",
       "      <td>0.122520</td>\n",
       "      <td>0.140194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484223</td>\n",
       "      <td>0.018821</td>\n",
       "      <td>0.047032</td>\n",
       "      <td>0.529248</td>\n",
       "      <td>0.103376</td>\n",
       "      <td>0.698409</td>\n",
       "      <td>0.730481</td>\n",
       "      <td>0.628533</td>\n",
       "      <td>0.686405</td>\n",
       "      <td>0.221547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.045528</td>\n",
       "      <td>0.694261</td>\n",
       "      <td>0.526924</td>\n",
       "      <td>0.687678</td>\n",
       "      <td>0.428343</td>\n",
       "      <td>0.110254</td>\n",
       "      <td>0.063979</td>\n",
       "      <td>0.043601</td>\n",
       "      <td>0.283937</td>\n",
       "      <td>0.674977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061738</td>\n",
       "      <td>0.348861</td>\n",
       "      <td>0.359173</td>\n",
       "      <td>0.591871</td>\n",
       "      <td>0.403512</td>\n",
       "      <td>0.109635</td>\n",
       "      <td>0.094345</td>\n",
       "      <td>0.593782</td>\n",
       "      <td>0.472435</td>\n",
       "      <td>0.638942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.701916</td>\n",
       "      <td>0.712294</td>\n",
       "      <td>0.724926</td>\n",
       "      <td>0.136221</td>\n",
       "      <td>0.465259</td>\n",
       "      <td>0.049291</td>\n",
       "      <td>0.014988</td>\n",
       "      <td>0.603787</td>\n",
       "      <td>0.441639</td>\n",
       "      <td>0.259940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195111</td>\n",
       "      <td>0.088260</td>\n",
       "      <td>0.710912</td>\n",
       "      <td>0.254999</td>\n",
       "      <td>0.473680</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.280511</td>\n",
       "      <td>0.500214</td>\n",
       "      <td>0.511894</td>\n",
       "      <td>0.222266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.546010</td>\n",
       "      <td>0.416507</td>\n",
       "      <td>0.266918</td>\n",
       "      <td>0.245136</td>\n",
       "      <td>-0.009617</td>\n",
       "      <td>0.269479</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.265470</td>\n",
       "      <td>0.254461</td>\n",
       "      <td>0.598631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038735</td>\n",
       "      <td>0.059057</td>\n",
       "      <td>-0.013714</td>\n",
       "      <td>0.690454</td>\n",
       "      <td>0.031382</td>\n",
       "      <td>0.759830</td>\n",
       "      <td>0.462607</td>\n",
       "      <td>0.560391</td>\n",
       "      <td>0.393360</td>\n",
       "      <td>0.175719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.025380</td>\n",
       "      <td>0.689546</td>\n",
       "      <td>0.477830</td>\n",
       "      <td>0.515672</td>\n",
       "      <td>0.162360</td>\n",
       "      <td>-0.012898</td>\n",
       "      <td>0.089360</td>\n",
       "      <td>0.206481</td>\n",
       "      <td>0.562323</td>\n",
       "      <td>0.194302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001373</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>0.087525</td>\n",
       "      <td>0.517900</td>\n",
       "      <td>-0.029892</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>0.438668</td>\n",
       "      <td>0.635387</td>\n",
       "      <td>0.343381</td>\n",
       "      <td>0.077775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.467970</td>\n",
       "      <td>0.428290</td>\n",
       "      <td>0.569689</td>\n",
       "      <td>0.387544</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.089873</td>\n",
       "      <td>0.661347</td>\n",
       "      <td>0.485723</td>\n",
       "      <td>0.322449</td>\n",
       "      <td>0.180848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099019</td>\n",
       "      <td>0.129751</td>\n",
       "      <td>0.098598</td>\n",
       "      <td>0.667496</td>\n",
       "      <td>0.040024</td>\n",
       "      <td>0.647102</td>\n",
       "      <td>0.676867</td>\n",
       "      <td>0.559685</td>\n",
       "      <td>0.516585</td>\n",
       "      <td>0.007522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.017642</td>\n",
       "      <td>0.465589</td>\n",
       "      <td>0.603563</td>\n",
       "      <td>0.577018</td>\n",
       "      <td>0.060323</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.237237</td>\n",
       "      <td>0.527605</td>\n",
       "      <td>0.397382</td>\n",
       "      <td>0.080872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342933</td>\n",
       "      <td>0.393625</td>\n",
       "      <td>0.017460</td>\n",
       "      <td>0.123986</td>\n",
       "      <td>0.144014</td>\n",
       "      <td>0.627169</td>\n",
       "      <td>0.025419</td>\n",
       "      <td>0.672621</td>\n",
       "      <td>0.495283</td>\n",
       "      <td>0.629574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.428028</td>\n",
       "      <td>0.615465</td>\n",
       "      <td>0.144674</td>\n",
       "      <td>0.013241</td>\n",
       "      <td>0.051789</td>\n",
       "      <td>0.343448</td>\n",
       "      <td>0.303521</td>\n",
       "      <td>0.542848</td>\n",
       "      <td>0.514456</td>\n",
       "      <td>0.281812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646868</td>\n",
       "      <td>0.499452</td>\n",
       "      <td>0.418559</td>\n",
       "      <td>0.256804</td>\n",
       "      <td>0.147372</td>\n",
       "      <td>0.525119</td>\n",
       "      <td>0.670532</td>\n",
       "      <td>0.465711</td>\n",
       "      <td>0.006253</td>\n",
       "      <td>0.642802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.038607</td>\n",
       "      <td>0.284245</td>\n",
       "      <td>0.237859</td>\n",
       "      <td>0.722047</td>\n",
       "      <td>0.446806</td>\n",
       "      <td>0.541948</td>\n",
       "      <td>0.046072</td>\n",
       "      <td>0.423340</td>\n",
       "      <td>0.329035</td>\n",
       "      <td>0.497751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001688</td>\n",
       "      <td>0.051127</td>\n",
       "      <td>0.162988</td>\n",
       "      <td>0.567601</td>\n",
       "      <td>0.193875</td>\n",
       "      <td>0.546047</td>\n",
       "      <td>0.502918</td>\n",
       "      <td>0.189808</td>\n",
       "      <td>0.189063</td>\n",
       "      <td>0.447276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.019655</td>\n",
       "      <td>0.668216</td>\n",
       "      <td>0.501783</td>\n",
       "      <td>0.371863</td>\n",
       "      <td>0.691665</td>\n",
       "      <td>0.478823</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.388910</td>\n",
       "      <td>0.061605</td>\n",
       "      <td>0.338697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164434</td>\n",
       "      <td>0.034401</td>\n",
       "      <td>0.676346</td>\n",
       "      <td>0.455427</td>\n",
       "      <td>0.124188</td>\n",
       "      <td>0.657226</td>\n",
       "      <td>0.058692</td>\n",
       "      <td>0.659859</td>\n",
       "      <td>0.021396</td>\n",
       "      <td>0.669880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.574416</td>\n",
       "      <td>0.337399</td>\n",
       "      <td>0.091577</td>\n",
       "      <td>0.133313</td>\n",
       "      <td>0.088239</td>\n",
       "      <td>0.646339</td>\n",
       "      <td>0.198467</td>\n",
       "      <td>0.444161</td>\n",
       "      <td>0.337331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208309</td>\n",
       "      <td>0.014947</td>\n",
       "      <td>0.167581</td>\n",
       "      <td>0.722351</td>\n",
       "      <td>0.681049</td>\n",
       "      <td>0.603132</td>\n",
       "      <td>0.540308</td>\n",
       "      <td>0.725603</td>\n",
       "      <td>0.083069</td>\n",
       "      <td>0.499746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.104965</td>\n",
       "      <td>0.717097</td>\n",
       "      <td>0.657615</td>\n",
       "      <td>0.113521</td>\n",
       "      <td>0.069508</td>\n",
       "      <td>-0.012974</td>\n",
       "      <td>0.051518</td>\n",
       "      <td>0.525215</td>\n",
       "      <td>0.294074</td>\n",
       "      <td>0.548270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.165524</td>\n",
       "      <td>0.586331</td>\n",
       "      <td>0.029135</td>\n",
       "      <td>0.507182</td>\n",
       "      <td>0.606235</td>\n",
       "      <td>0.007034</td>\n",
       "      <td>0.376602</td>\n",
       "      <td>0.036376</td>\n",
       "      <td>0.489163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.386335</td>\n",
       "      <td>0.158890</td>\n",
       "      <td>0.032618</td>\n",
       "      <td>-0.019089</td>\n",
       "      <td>0.183273</td>\n",
       "      <td>0.131765</td>\n",
       "      <td>0.679337</td>\n",
       "      <td>0.630493</td>\n",
       "      <td>0.211758</td>\n",
       "      <td>0.281544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427899</td>\n",
       "      <td>0.165924</td>\n",
       "      <td>0.220244</td>\n",
       "      <td>0.675285</td>\n",
       "      <td>0.327599</td>\n",
       "      <td>0.507508</td>\n",
       "      <td>0.384228</td>\n",
       "      <td>0.241889</td>\n",
       "      <td>0.415134</td>\n",
       "      <td>0.752098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.601405</td>\n",
       "      <td>0.251883</td>\n",
       "      <td>0.651988</td>\n",
       "      <td>0.299814</td>\n",
       "      <td>-0.002835</td>\n",
       "      <td>0.213190</td>\n",
       "      <td>0.210597</td>\n",
       "      <td>0.346955</td>\n",
       "      <td>0.235610</td>\n",
       "      <td>0.357577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092760</td>\n",
       "      <td>0.125332</td>\n",
       "      <td>0.167577</td>\n",
       "      <td>0.424392</td>\n",
       "      <td>0.172164</td>\n",
       "      <td>0.261645</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.572136</td>\n",
       "      <td>0.685282</td>\n",
       "      <td>0.229736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.079224</td>\n",
       "      <td>0.586778</td>\n",
       "      <td>0.431715</td>\n",
       "      <td>-0.040535</td>\n",
       "      <td>0.404613</td>\n",
       "      <td>0.079037</td>\n",
       "      <td>0.161397</td>\n",
       "      <td>0.249580</td>\n",
       "      <td>0.333749</td>\n",
       "      <td>0.616155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698314</td>\n",
       "      <td>0.072382</td>\n",
       "      <td>0.075494</td>\n",
       "      <td>0.014136</td>\n",
       "      <td>0.471052</td>\n",
       "      <td>0.733010</td>\n",
       "      <td>-0.040867</td>\n",
       "      <td>0.436009</td>\n",
       "      <td>0.009265</td>\n",
       "      <td>0.617360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.020044</td>\n",
       "      <td>0.412599</td>\n",
       "      <td>0.091569</td>\n",
       "      <td>0.085987</td>\n",
       "      <td>0.058910</td>\n",
       "      <td>0.135259</td>\n",
       "      <td>0.499118</td>\n",
       "      <td>0.548343</td>\n",
       "      <td>0.604036</td>\n",
       "      <td>0.277324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169519</td>\n",
       "      <td>0.686182</td>\n",
       "      <td>0.119496</td>\n",
       "      <td>0.500029</td>\n",
       "      <td>0.208575</td>\n",
       "      <td>0.656080</td>\n",
       "      <td>-0.014108</td>\n",
       "      <td>0.333260</td>\n",
       "      <td>0.091499</td>\n",
       "      <td>0.597202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.030611</td>\n",
       "      <td>0.585070</td>\n",
       "      <td>0.210743</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>0.682494</td>\n",
       "      <td>0.125303</td>\n",
       "      <td>0.697050</td>\n",
       "      <td>0.264833</td>\n",
       "      <td>0.228829</td>\n",
       "      <td>0.573711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561816</td>\n",
       "      <td>0.483454</td>\n",
       "      <td>0.644255</td>\n",
       "      <td>0.021464</td>\n",
       "      <td>0.032146</td>\n",
       "      <td>0.439301</td>\n",
       "      <td>0.105617</td>\n",
       "      <td>0.197041</td>\n",
       "      <td>0.166986</td>\n",
       "      <td>0.670419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         4         9         14        22        24        25  \\\n",
       "0   0.136075  0.644643  0.655718  0.431823  0.574567  0.653610  0.633507   \n",
       "1   0.329785  0.172821  0.524054  0.742346  0.290984  0.070555  0.638568   \n",
       "2   0.496259  0.194111  0.420594  0.334187  0.429614  0.708858  0.463489   \n",
       "3   0.383825  0.053211  0.599283  0.666747  0.412962  0.548038  0.373123   \n",
       "4   0.653280 -0.003362  0.486367  0.567220  0.437471  0.072625  0.480086   \n",
       "5   0.045528  0.694261  0.526924  0.687678  0.428343  0.110254  0.063979   \n",
       "6   0.701916  0.712294  0.724926  0.136221  0.465259  0.049291  0.014988   \n",
       "7   0.546010  0.416507  0.266918  0.245136 -0.009617  0.269479  0.193600   \n",
       "8   0.025380  0.689546  0.477830  0.515672  0.162360 -0.012898  0.089360   \n",
       "9   0.467970  0.428290  0.569689  0.387544  0.009497  0.089873  0.661347   \n",
       "10  0.017642  0.465589  0.603563  0.577018  0.060323  0.002411  0.237237   \n",
       "11  0.428028  0.615465  0.144674  0.013241  0.051789  0.343448  0.303521   \n",
       "12  0.038607  0.284245  0.237859  0.722047  0.446806  0.541948  0.046072   \n",
       "13 -0.019655  0.668216  0.501783  0.371863  0.691665  0.478823  0.226200   \n",
       "14  0.007357  0.574416  0.337399  0.091577  0.133313  0.088239  0.646339   \n",
       "15  0.104965  0.717097  0.657615  0.113521  0.069508 -0.012974  0.051518   \n",
       "16  0.386335  0.158890  0.032618 -0.019089  0.183273  0.131765  0.679337   \n",
       "17  0.601405  0.251883  0.651988  0.299814 -0.002835  0.213190  0.210597   \n",
       "18  0.079224  0.586778  0.431715 -0.040535  0.404613  0.079037  0.161397   \n",
       "19 -0.020044  0.412599  0.091569  0.085987  0.058910  0.135259  0.499118   \n",
       "20  0.030611  0.585070  0.210743 -0.009612  0.682494  0.125303  0.697050   \n",
       "\n",
       "          35        36        37  ...        84        86        89        90  \\\n",
       "0   0.314956  0.255302  0.165078  ...  0.520167  0.715751  0.004154  0.652095   \n",
       "1   0.204389  0.333625  0.375461  ...  0.459012  0.476434  0.152263  0.506406   \n",
       "2   0.083636  0.168406  0.018926  ...  0.436818  0.062092  0.002873  0.646824   \n",
       "3   0.150179  0.367594  0.462709  ...  0.536762  0.457959  0.070745  0.463101   \n",
       "4   0.704411  0.122520  0.140194  ...  0.484223  0.018821  0.047032  0.529248   \n",
       "5   0.043601  0.283937  0.674977  ...  0.061738  0.348861  0.359173  0.591871   \n",
       "6   0.603787  0.441639  0.259940  ...  0.195111  0.088260  0.710912  0.254999   \n",
       "7   0.265470  0.254461  0.598631  ...  0.038735  0.059057 -0.013714  0.690454   \n",
       "8   0.206481  0.562323  0.194302  ... -0.001373  0.012446  0.087525  0.517900   \n",
       "9   0.485723  0.322449  0.180848  ...  0.099019  0.129751  0.098598  0.667496   \n",
       "10  0.527605  0.397382  0.080872  ...  0.342933  0.393625  0.017460  0.123986   \n",
       "11  0.542848  0.514456  0.281812  ...  0.646868  0.499452  0.418559  0.256804   \n",
       "12  0.423340  0.329035  0.497751  ... -0.001688  0.051127  0.162988  0.567601   \n",
       "13  0.388910  0.061605  0.338697  ...  0.164434  0.034401  0.676346  0.455427   \n",
       "14  0.198467  0.444161  0.337331  ...  0.208309  0.014947  0.167581  0.722351   \n",
       "15  0.525215  0.294074  0.548270  ...  0.000530  0.165524  0.586331  0.029135   \n",
       "16  0.630493  0.211758  0.281544  ...  0.427899  0.165924  0.220244  0.675285   \n",
       "17  0.346955  0.235610  0.357577  ...  0.092760  0.125332  0.167577  0.424392   \n",
       "18  0.249580  0.333749  0.616155  ...  0.698314  0.072382  0.075494  0.014136   \n",
       "19  0.548343  0.604036  0.277324  ...  0.169519  0.686182  0.119496  0.500029   \n",
       "20  0.264833  0.228829  0.573711  ...  0.561816  0.483454  0.644255  0.021464   \n",
       "\n",
       "          91        92        95        97        98        99  \n",
       "0   0.394871  0.186444  0.742264  0.044102  0.112685  0.019643  \n",
       "1   0.044304  0.147512  0.259074  0.021597  0.242836  0.434171  \n",
       "2   0.085102  0.750723  0.653464  0.692647  0.005563  0.127510  \n",
       "3   0.031537  0.773964  0.594639  0.078160  0.265225  0.720895  \n",
       "4   0.103376  0.698409  0.730481  0.628533  0.686405  0.221547  \n",
       "5   0.403512  0.109635  0.094345  0.593782  0.472435  0.638942  \n",
       "6   0.473680  0.045741  0.280511  0.500214  0.511894  0.222266  \n",
       "7   0.031382  0.759830  0.462607  0.560391  0.393360  0.175719  \n",
       "8  -0.029892  0.034434  0.438668  0.635387  0.343381  0.077775  \n",
       "9   0.040024  0.647102  0.676867  0.559685  0.516585  0.007522  \n",
       "10  0.144014  0.627169  0.025419  0.672621  0.495283  0.629574  \n",
       "11  0.147372  0.525119  0.670532  0.465711  0.006253  0.642802  \n",
       "12  0.193875  0.546047  0.502918  0.189808  0.189063  0.447276  \n",
       "13  0.124188  0.657226  0.058692  0.659859  0.021396  0.669880  \n",
       "14  0.681049  0.603132  0.540308  0.725603  0.083069  0.499746  \n",
       "15  0.507182  0.606235  0.007034  0.376602  0.036376  0.489163  \n",
       "16  0.327599  0.507508  0.384228  0.241889  0.415134  0.752098  \n",
       "17  0.172164  0.261645  0.001706  0.572136  0.685282  0.229736  \n",
       "18  0.471052  0.733010 -0.040867  0.436009  0.009265  0.617360  \n",
       "19  0.208575  0.656080 -0.014108  0.333260  0.091499  0.597202  \n",
       "20  0.032146  0.439301  0.105617  0.197041  0.166986  0.670419  \n",
       "\n",
       "[21 rows x 41 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>4</th>\n",
       "      <th>9</th>\n",
       "      <th>14</th>\n",
       "      <th>22</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>...</th>\n",
       "      <th>84</th>\n",
       "      <th>86</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>95</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.165539</td>\n",
       "      <td>0.764241</td>\n",
       "      <td>0.263854</td>\n",
       "      <td>0.386694</td>\n",
       "      <td>0.509230</td>\n",
       "      <td>0.741127</td>\n",
       "      <td>0.675659</td>\n",
       "      <td>0.620824</td>\n",
       "      <td>0.200082</td>\n",
       "      <td>0.070399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659138</td>\n",
       "      <td>0.626613</td>\n",
       "      <td>0.067647</td>\n",
       "      <td>0.797150</td>\n",
       "      <td>0.418742</td>\n",
       "      <td>0.278836</td>\n",
       "      <td>0.649797</td>\n",
       "      <td>0.085651</td>\n",
       "      <td>0.117422</td>\n",
       "      <td>0.029811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.312090</td>\n",
       "      <td>0.220041</td>\n",
       "      <td>0.782636</td>\n",
       "      <td>0.579574</td>\n",
       "      <td>0.254949</td>\n",
       "      <td>0.050609</td>\n",
       "      <td>0.654473</td>\n",
       "      <td>0.751116</td>\n",
       "      <td>0.125721</td>\n",
       "      <td>0.504904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376792</td>\n",
       "      <td>0.491349</td>\n",
       "      <td>0.183897</td>\n",
       "      <td>0.527174</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.104773</td>\n",
       "      <td>0.301571</td>\n",
       "      <td>0.066040</td>\n",
       "      <td>0.325206</td>\n",
       "      <td>0.527713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.590390</td>\n",
       "      <td>0.134658</td>\n",
       "      <td>0.560478</td>\n",
       "      <td>0.297122</td>\n",
       "      <td>0.714888</td>\n",
       "      <td>0.783808</td>\n",
       "      <td>0.465749</td>\n",
       "      <td>0.328685</td>\n",
       "      <td>0.300857</td>\n",
       "      <td>0.113560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436288</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>0.042005</td>\n",
       "      <td>0.784495</td>\n",
       "      <td>0.049530</td>\n",
       "      <td>0.721235</td>\n",
       "      <td>0.787071</td>\n",
       "      <td>0.746020</td>\n",
       "      <td>0.015249</td>\n",
       "      <td>0.172700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.410304</td>\n",
       "      <td>0.044856</td>\n",
       "      <td>0.374979</td>\n",
       "      <td>0.921963</td>\n",
       "      <td>0.071594</td>\n",
       "      <td>0.390394</td>\n",
       "      <td>0.173835</td>\n",
       "      <td>0.230194</td>\n",
       "      <td>0.359003</td>\n",
       "      <td>0.570373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540599</td>\n",
       "      <td>0.491497</td>\n",
       "      <td>0.045832</td>\n",
       "      <td>0.447294</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>0.632848</td>\n",
       "      <td>0.033391</td>\n",
       "      <td>0.253742</td>\n",
       "      <td>0.739978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.824310</td>\n",
       "      <td>0.040861</td>\n",
       "      <td>0.353298</td>\n",
       "      <td>0.592411</td>\n",
       "      <td>0.464934</td>\n",
       "      <td>0.376307</td>\n",
       "      <td>0.403981</td>\n",
       "      <td>0.669590</td>\n",
       "      <td>0.018039</td>\n",
       "      <td>0.085080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551812</td>\n",
       "      <td>0.005616</td>\n",
       "      <td>0.098045</td>\n",
       "      <td>0.571893</td>\n",
       "      <td>0.098564</td>\n",
       "      <td>0.747784</td>\n",
       "      <td>0.748598</td>\n",
       "      <td>0.623549</td>\n",
       "      <td>0.761574</td>\n",
       "      <td>0.179846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.034440</td>\n",
       "      <td>0.737795</td>\n",
       "      <td>0.435759</td>\n",
       "      <td>0.312432</td>\n",
       "      <td>0.464916</td>\n",
       "      <td>0.228664</td>\n",
       "      <td>0.668561</td>\n",
       "      <td>0.398946</td>\n",
       "      <td>0.030819</td>\n",
       "      <td>0.060310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.455207</td>\n",
       "      <td>0.481704</td>\n",
       "      <td>0.751375</td>\n",
       "      <td>0.390982</td>\n",
       "      <td>0.129553</td>\n",
       "      <td>0.167465</td>\n",
       "      <td>0.651345</td>\n",
       "      <td>0.470322</td>\n",
       "      <td>0.610358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.627490</td>\n",
       "      <td>0.645237</td>\n",
       "      <td>0.622896</td>\n",
       "      <td>0.260557</td>\n",
       "      <td>0.603850</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>0.040399</td>\n",
       "      <td>0.540659</td>\n",
       "      <td>0.165949</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186601</td>\n",
       "      <td>0.026784</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.289008</td>\n",
       "      <td>0.598306</td>\n",
       "      <td>0.102617</td>\n",
       "      <td>0.292341</td>\n",
       "      <td>0.455817</td>\n",
       "      <td>0.523958</td>\n",
       "      <td>0.229819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.496774</td>\n",
       "      <td>0.583216</td>\n",
       "      <td>0.282470</td>\n",
       "      <td>0.222698</td>\n",
       "      <td>0.026805</td>\n",
       "      <td>0.594165</td>\n",
       "      <td>0.314487</td>\n",
       "      <td>0.100662</td>\n",
       "      <td>0.037279</td>\n",
       "      <td>0.161996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061217</td>\n",
       "      <td>0.041553</td>\n",
       "      <td>0.025505</td>\n",
       "      <td>0.765915</td>\n",
       "      <td>0.088971</td>\n",
       "      <td>0.783222</td>\n",
       "      <td>0.683340</td>\n",
       "      <td>0.668231</td>\n",
       "      <td>0.384354</td>\n",
       "      <td>0.136575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.015199</td>\n",
       "      <td>0.773253</td>\n",
       "      <td>0.680078</td>\n",
       "      <td>0.574472</td>\n",
       "      <td>0.014102</td>\n",
       "      <td>0.056176</td>\n",
       "      <td>0.139442</td>\n",
       "      <td>0.034239</td>\n",
       "      <td>0.867243</td>\n",
       "      <td>0.059017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024474</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.105239</td>\n",
       "      <td>0.730722</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.020981</td>\n",
       "      <td>0.395696</td>\n",
       "      <td>0.772237</td>\n",
       "      <td>0.310642</td>\n",
       "      <td>0.109342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.684259</td>\n",
       "      <td>0.633812</td>\n",
       "      <td>0.629710</td>\n",
       "      <td>0.083049</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>0.028873</td>\n",
       "      <td>0.769490</td>\n",
       "      <td>0.401291</td>\n",
       "      <td>0.110672</td>\n",
       "      <td>0.608051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084736</td>\n",
       "      <td>0.147846</td>\n",
       "      <td>0.208304</td>\n",
       "      <td>0.786774</td>\n",
       "      <td>0.045683</td>\n",
       "      <td>0.721827</td>\n",
       "      <td>0.690326</td>\n",
       "      <td>0.761193</td>\n",
       "      <td>0.537486</td>\n",
       "      <td>0.030887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.011594</td>\n",
       "      <td>0.577329</td>\n",
       "      <td>0.372607</td>\n",
       "      <td>0.713102</td>\n",
       "      <td>0.046360</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.775653</td>\n",
       "      <td>0.730533</td>\n",
       "      <td>0.429726</td>\n",
       "      <td>0.274019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442050</td>\n",
       "      <td>0.476141</td>\n",
       "      <td>0.018245</td>\n",
       "      <td>0.131473</td>\n",
       "      <td>0.293251</td>\n",
       "      <td>0.871893</td>\n",
       "      <td>0.047014</td>\n",
       "      <td>0.599185</td>\n",
       "      <td>0.655810</td>\n",
       "      <td>0.727063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.405849</td>\n",
       "      <td>0.665527</td>\n",
       "      <td>0.164442</td>\n",
       "      <td>0.058057</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.090245</td>\n",
       "      <td>0.696526</td>\n",
       "      <td>0.626599</td>\n",
       "      <td>0.563662</td>\n",
       "      <td>0.405495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720353</td>\n",
       "      <td>0.522971</td>\n",
       "      <td>0.551480</td>\n",
       "      <td>0.307872</td>\n",
       "      <td>0.110304</td>\n",
       "      <td>0.601547</td>\n",
       "      <td>0.723270</td>\n",
       "      <td>0.423549</td>\n",
       "      <td>0.049634</td>\n",
       "      <td>0.827323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.018716</td>\n",
       "      <td>0.295986</td>\n",
       "      <td>0.271475</td>\n",
       "      <td>0.694756</td>\n",
       "      <td>0.351440</td>\n",
       "      <td>0.363249</td>\n",
       "      <td>0.170652</td>\n",
       "      <td>0.676011</td>\n",
       "      <td>0.464850</td>\n",
       "      <td>0.221964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023182</td>\n",
       "      <td>0.051713</td>\n",
       "      <td>0.198958</td>\n",
       "      <td>0.608366</td>\n",
       "      <td>0.175552</td>\n",
       "      <td>0.705159</td>\n",
       "      <td>0.470867</td>\n",
       "      <td>0.149805</td>\n",
       "      <td>0.185998</td>\n",
       "      <td>0.592461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.025692</td>\n",
       "      <td>0.723154</td>\n",
       "      <td>0.669669</td>\n",
       "      <td>0.505729</td>\n",
       "      <td>0.076369</td>\n",
       "      <td>0.498476</td>\n",
       "      <td>0.233336</td>\n",
       "      <td>0.929324</td>\n",
       "      <td>0.024718</td>\n",
       "      <td>0.699495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147928</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.782490</td>\n",
       "      <td>0.430747</td>\n",
       "      <td>0.141708</td>\n",
       "      <td>0.807072</td>\n",
       "      <td>0.012605</td>\n",
       "      <td>0.708780</td>\n",
       "      <td>0.014176</td>\n",
       "      <td>0.777719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.010869</td>\n",
       "      <td>0.591201</td>\n",
       "      <td>0.052443</td>\n",
       "      <td>0.303873</td>\n",
       "      <td>0.287764</td>\n",
       "      <td>0.321426</td>\n",
       "      <td>0.871263</td>\n",
       "      <td>0.326869</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>0.474887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168227</td>\n",
       "      <td>0.071196</td>\n",
       "      <td>0.228098</td>\n",
       "      <td>0.901197</td>\n",
       "      <td>0.813183</td>\n",
       "      <td>0.683938</td>\n",
       "      <td>0.627277</td>\n",
       "      <td>0.767798</td>\n",
       "      <td>0.017979</td>\n",
       "      <td>0.472016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.105249</td>\n",
       "      <td>0.704497</td>\n",
       "      <td>0.728749</td>\n",
       "      <td>0.355738</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.057272</td>\n",
       "      <td>0.441855</td>\n",
       "      <td>0.110116</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>0.068461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028979</td>\n",
       "      <td>0.211353</td>\n",
       "      <td>0.682956</td>\n",
       "      <td>0.032548</td>\n",
       "      <td>0.516994</td>\n",
       "      <td>0.654863</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.481363</td>\n",
       "      <td>0.067009</td>\n",
       "      <td>0.500180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.372740</td>\n",
       "      <td>0.129610</td>\n",
       "      <td>0.055195</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.186399</td>\n",
       "      <td>0.044308</td>\n",
       "      <td>0.560169</td>\n",
       "      <td>0.857154</td>\n",
       "      <td>0.052204</td>\n",
       "      <td>0.059416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465494</td>\n",
       "      <td>0.132314</td>\n",
       "      <td>0.200187</td>\n",
       "      <td>0.702560</td>\n",
       "      <td>0.389853</td>\n",
       "      <td>0.388311</td>\n",
       "      <td>0.470126</td>\n",
       "      <td>0.191775</td>\n",
       "      <td>0.452660</td>\n",
       "      <td>0.808828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.557406</td>\n",
       "      <td>0.294744</td>\n",
       "      <td>0.704462</td>\n",
       "      <td>0.479871</td>\n",
       "      <td>0.017314</td>\n",
       "      <td>0.228513</td>\n",
       "      <td>0.086411</td>\n",
       "      <td>0.520940</td>\n",
       "      <td>0.074394</td>\n",
       "      <td>0.070661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180659</td>\n",
       "      <td>0.119487</td>\n",
       "      <td>0.109386</td>\n",
       "      <td>0.566359</td>\n",
       "      <td>0.272949</td>\n",
       "      <td>0.158618</td>\n",
       "      <td>0.038732</td>\n",
       "      <td>0.639309</td>\n",
       "      <td>0.837272</td>\n",
       "      <td>0.333071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.050136</td>\n",
       "      <td>0.606869</td>\n",
       "      <td>0.412444</td>\n",
       "      <td>0.029621</td>\n",
       "      <td>0.788348</td>\n",
       "      <td>0.012321</td>\n",
       "      <td>0.146713</td>\n",
       "      <td>0.295126</td>\n",
       "      <td>0.126954</td>\n",
       "      <td>0.623385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746387</td>\n",
       "      <td>0.128903</td>\n",
       "      <td>0.085676</td>\n",
       "      <td>0.030107</td>\n",
       "      <td>0.674605</td>\n",
       "      <td>0.754805</td>\n",
       "      <td>0.024203</td>\n",
       "      <td>0.441606</td>\n",
       "      <td>0.019347</td>\n",
       "      <td>0.755570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.166324</td>\n",
       "      <td>0.394222</td>\n",
       "      <td>0.032959</td>\n",
       "      <td>0.121612</td>\n",
       "      <td>0.113975</td>\n",
       "      <td>0.454374</td>\n",
       "      <td>0.578793</td>\n",
       "      <td>0.038540</td>\n",
       "      <td>0.299197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181749</td>\n",
       "      <td>0.803051</td>\n",
       "      <td>0.045435</td>\n",
       "      <td>0.510773</td>\n",
       "      <td>0.372729</td>\n",
       "      <td>0.751247</td>\n",
       "      <td>0.013245</td>\n",
       "      <td>0.272074</td>\n",
       "      <td>0.167211</td>\n",
       "      <td>0.764521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.039332</td>\n",
       "      <td>0.641739</td>\n",
       "      <td>0.293617</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.065248</td>\n",
       "      <td>0.053017</td>\n",
       "      <td>0.718501</td>\n",
       "      <td>0.365569</td>\n",
       "      <td>0.262201</td>\n",
       "      <td>0.766698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.617480</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>0.695618</td>\n",
       "      <td>0.011668</td>\n",
       "      <td>0.011269</td>\n",
       "      <td>0.444371</td>\n",
       "      <td>0.101566</td>\n",
       "      <td>0.215898</td>\n",
       "      <td>0.212707</td>\n",
       "      <td>0.648989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         4         9         14        22        24        25  \\\n",
       "0   0.165539  0.764241  0.263854  0.386694  0.509230  0.741127  0.675659   \n",
       "1   0.312090  0.220041  0.782636  0.579574  0.254949  0.050609  0.654473   \n",
       "2   0.590390  0.134658  0.560478  0.297122  0.714888  0.783808  0.465749   \n",
       "3   0.410304  0.044856  0.374979  0.921963  0.071594  0.390394  0.173835   \n",
       "4   0.824310  0.040861  0.353298  0.592411  0.464934  0.376307  0.403981   \n",
       "5   0.034440  0.737795  0.435759  0.312432  0.464916  0.228664  0.668561   \n",
       "6   0.627490  0.645237  0.622896  0.260557  0.603850  0.028477  0.040399   \n",
       "7   0.496774  0.583216  0.282470  0.222698  0.026805  0.594165  0.314487   \n",
       "8   0.015199  0.773253  0.680078  0.574472  0.014102  0.056176  0.139442   \n",
       "9   0.684259  0.633812  0.629710  0.083049  0.003627  0.028873  0.769490   \n",
       "10  0.011594  0.577329  0.372607  0.713102  0.046360  0.002722  0.775653   \n",
       "11  0.405849  0.665527  0.164442  0.058057  0.072266  0.090245  0.696526   \n",
       "12  0.018716  0.295986  0.271475  0.694756  0.351440  0.363249  0.170652   \n",
       "13  0.025692  0.723154  0.669669  0.505729  0.076369  0.498476  0.233336   \n",
       "14  0.010869  0.591201  0.052443  0.303873  0.287764  0.321426  0.871263   \n",
       "15  0.105249  0.704497  0.728749  0.355738  0.042969  0.057272  0.441855   \n",
       "16  0.372740  0.129610  0.055195  0.005987  0.186399  0.044308  0.560169   \n",
       "17  0.557406  0.294744  0.704462  0.479871  0.017314  0.228513  0.086411   \n",
       "18  0.050136  0.606869  0.412444  0.029621  0.788348  0.012321  0.146713   \n",
       "19  0.001668  0.166324  0.394222  0.032959  0.121612  0.113975  0.454374   \n",
       "20  0.039332  0.641739  0.293617  0.001897  0.065248  0.053017  0.718501   \n",
       "\n",
       "          35        36        37  ...        84        86        89        90  \\\n",
       "0   0.620824  0.200082  0.070399  ...  0.659138  0.626613  0.067647  0.797150   \n",
       "1   0.751116  0.125721  0.504904  ...  0.376792  0.491349  0.183897  0.527174   \n",
       "2   0.328685  0.300857  0.113560  ...  0.436288  0.007368  0.042005  0.784495   \n",
       "3   0.230194  0.359003  0.570373  ...  0.540599  0.491497  0.045832  0.447294   \n",
       "4   0.669590  0.018039  0.085080  ...  0.551812  0.005616  0.098045  0.571893   \n",
       "5   0.398946  0.030819  0.060310  ...  0.018091  0.455207  0.481704  0.751375   \n",
       "6   0.540659  0.165949  0.197917  ...  0.186601  0.026784  0.842857  0.289008   \n",
       "7   0.100662  0.037279  0.161996  ...  0.061217  0.041553  0.025505  0.765915   \n",
       "8   0.034239  0.867243  0.059017  ...  0.024474  0.014480  0.105239  0.730722   \n",
       "9   0.401291  0.110672  0.608051  ...  0.084736  0.147846  0.208304  0.786774   \n",
       "10  0.730533  0.429726  0.274019  ...  0.442050  0.476141  0.018245  0.131473   \n",
       "11  0.626599  0.563662  0.405495  ...  0.720353  0.522971  0.551480  0.307872   \n",
       "12  0.676011  0.464850  0.221964  ...  0.023182  0.051713  0.198958  0.608366   \n",
       "13  0.929324  0.024718  0.699495  ...  0.147928  0.002525  0.782490  0.430747   \n",
       "14  0.326869  0.004338  0.474887  ...  0.168227  0.071196  0.228098  0.901197   \n",
       "15  0.110116  0.006523  0.068461  ...  0.028979  0.211353  0.682956  0.032548   \n",
       "16  0.857154  0.052204  0.059416  ...  0.465494  0.132314  0.200187  0.702560   \n",
       "17  0.520940  0.074394  0.070661  ...  0.180659  0.119487  0.109386  0.566359   \n",
       "18  0.295126  0.126954  0.623385  ...  0.746387  0.128903  0.085676  0.030107   \n",
       "19  0.578793  0.038540  0.299197  ...  0.181749  0.803051  0.045435  0.510773   \n",
       "20  0.365569  0.262201  0.766698  ...  0.617480  0.432500  0.695618  0.011668   \n",
       "\n",
       "          91        92        95        97        98        99  \n",
       "0   0.418742  0.278836  0.649797  0.085651  0.117422  0.029811  \n",
       "1   0.032882  0.104773  0.301571  0.066040  0.325206  0.527713  \n",
       "2   0.049530  0.721235  0.787071  0.746020  0.015249  0.172700  \n",
       "3   0.009768  0.723892  0.632848  0.033391  0.253742  0.739978  \n",
       "4   0.098564  0.747784  0.748598  0.623549  0.761574  0.179846  \n",
       "5   0.390982  0.129553  0.167465  0.651345  0.470322  0.610358  \n",
       "6   0.598306  0.102617  0.292341  0.455817  0.523958  0.229819  \n",
       "7   0.088971  0.783222  0.683340  0.668231  0.384354  0.136575  \n",
       "8   0.000469  0.020981  0.395696  0.772237  0.310642  0.109342  \n",
       "9   0.045683  0.721827  0.690326  0.761193  0.537486  0.030887  \n",
       "10  0.293251  0.871893  0.047014  0.599185  0.655810  0.727063  \n",
       "11  0.110304  0.601547  0.723270  0.423549  0.049634  0.827323  \n",
       "12  0.175552  0.705159  0.470867  0.149805  0.185998  0.592461  \n",
       "13  0.141708  0.807072  0.012605  0.708780  0.014176  0.777719  \n",
       "14  0.813183  0.683938  0.627277  0.767798  0.017979  0.472016  \n",
       "15  0.516994  0.654863  0.004286  0.481363  0.067009  0.500180  \n",
       "16  0.389853  0.388311  0.470126  0.191775  0.452660  0.808828  \n",
       "17  0.272949  0.158618  0.038732  0.639309  0.837272  0.333071  \n",
       "18  0.674605  0.754805  0.024203  0.441606  0.019347  0.755570  \n",
       "19  0.372729  0.751247  0.013245  0.272074  0.167211  0.764521  \n",
       "20  0.011269  0.444371  0.101566  0.215898  0.212707  0.648989  \n",
       "\n",
       "[21 rows x 41 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[parent_idx].head(NUM_TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.022026002>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_noParent_MSE(validation, o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test OG SHAP\n",
    "\n",
    "ms shap sux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(NUM_TIME_STEPS,NUM_TARGETS))\n",
    "re = tf.keras.layers.Reshape(target_shape=(NUM_TIME_STEPS, NUM_TARGETS))(inp)\n",
    "dout = dense(re)\n",
    "doutRe = tf.squeeze(tf.keras.layers.Reshape(target_shape=(NUM_TIME_STEPS*NUM_TARGETS,1))(dout), axis=-1)\n",
    "denseRE = tf.keras.Model(inputs=inp, outputs=doutRe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 21, 100)]         0         \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 21, 100)           0         \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 21, 100)           452041    \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 2100, 1)           0         \n",
      "                                                                 \n",
      " tf.compat.v1.squeeze (TFOpL  (None, 2100)             0         \n",
      " ambda)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 452,041\n",
      "Trainable params: 452,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "denseRE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2100), dtype=float32, numpy=\n",
       "array([[0.13607456, 0.        , 0.        , ..., 0.19704118, 0.16698585,\n",
       "        0.6704188 ],\n",
       "       [0.16450553, 0.        , 0.        , ..., 0.3183537 , 0.18375535,\n",
       "        0.7955263 ]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denseRE(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2100/2100 [10:58<00:00,  3.19it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = denseRE\n",
    "# def map2layer(x, layer):\n",
    "#     feed_dict = dict(zip([model.layers[0].input.ref()], [x.copy()]))\n",
    "#     return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "\n",
    "\n",
    "e = shap.GradientExplainer(model,\n",
    "    #(model.layers[0].input, model.layers[-1].output),\n",
    "    allData,\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "sv = e.shap_values(validation)\n",
    "# shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv= np.array(sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2100, 2, 21, 100)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x540 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap.image_plot(sv[0:4], validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2025, 0, 12, 44)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv.argmax()\n",
    "np.unravel_index(8506244, sv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goldIm[12][44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 21, 100)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2100)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valR = validation.reshape(( validation.shape[0], validation.shape[1]*validation.shape[2]))\n",
    "valR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valR.mean(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 1.],\n",
       "       [0., 0., 1., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs = np.nonzero(valR >valR.mean())\n",
    "tmagIm = np.zeros(shape=valR.shape) \n",
    "tmagIm[bs] = 1\n",
    "plt.plot(tmagIm[0]);\n",
    "bs = tmagIm\n",
    "del tmagIm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "multiclass SHAP not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\dream4_100\\DreamEncoder4_TestNewShap.ipynb Cell 69\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_TestNewShap.ipynb#Y564sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# X, y = load_breast_cancer(return_X_y=True, as_frame=True)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_TestNewShap.ipynb#Y564sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_TestNewShap.ipynb#Y564sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_TestNewShap.ipynb#Y564sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# blackbox_model = Pipeline([('pca', pca), ('rf', rf)])\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_TestNewShap.ipynb#Y564sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# blackbox_model.fit(X_train, y_train)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_TestNewShap.ipynb#Y564sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m blackbox_model \u001b[39m=\u001b[39m denseRE\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_TestNewShap.ipynb#Y564sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m shap \u001b[39m=\u001b[39m ShapKernel(blackbox_model\u001b[39m.\u001b[39;49mpredict, bs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_TestNewShap.ipynb#Y564sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_TestNewShap.ipynb#Y564sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\interpret\\blackbox\\shap.py:46\u001b[0m, in \u001b[0;36mShapKernel.__init__\u001b[1;34m(self, model, data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m predict_fn, n_classes, _ \u001b[39m=\u001b[39m determine_classes(model, data, n_samples)\n\u001b[0;32m     45\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m3\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m n_classes:\n\u001b[1;32m---> 46\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmulticlass SHAP not supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m predict_fn \u001b[39m=\u001b[39m unify_predict_fn(predict_fn, data, \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m n_classes \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     49\u001b[0m data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names_in_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types_in_ \u001b[39m=\u001b[39m unify_data(\n\u001b[0;32m     50\u001b[0m     data, n_samples, feature_names, feature_types, \u001b[39mFalse\u001b[39;00m, \u001b[39m0\u001b[39m\n\u001b[0;32m     51\u001b[0m )\n",
      "\u001b[1;31mException\u001b[0m: multiclass SHAP not supported"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import warnings\n",
    "\n",
    "# from interpret import show\n",
    "# from interpret.blackbox import ShapKernel\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "# X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n",
    "\n",
    "# pca = PCA()\n",
    "# rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=seed)\n",
    "\n",
    "# blackbox_model = Pipeline([('pca', pca), ('rf', rf)])\n",
    "# blackbox_model.fit(X_train, y_train)\n",
    "\n",
    "blackbox_model = denseRE\n",
    "\n",
    "shap = ShapKernel(blackbox_model.predict, bs)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    shap_local = shap.explain_local(bs, bs)\n",
    "\n",
    "show(shap_local, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Size Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(NUM_PARENTS, NUM_TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #to test on entire DS: \n",
    "# N = 50\n",
    "# hidden = np.arange(1,NUM_PARENTS+1, 1) #range(1,32)\n",
    "# lossMatrix = []\n",
    "# for i in tqdm(range(N)):\n",
    "    \n",
    "#     losses = []\n",
    "#     for value in (hidden):\n",
    "#         dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, value)\n",
    "#         dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "#         dense.fit(allData, allData, epochs=40,  verbose=0)\n",
    "#         test_hat = dense(allData) #, verbose = 0)\n",
    "#         loss = ignore_noParent_MSE(allData, test_hat)\n",
    "#         losses.append(loss)\n",
    "#         tf.keras.backend.clear_session()\n",
    "#     lossMatrix.append(losses)\n",
    "    \n",
    "# lossMatrix = np.array(lossMatrix)\n",
    "# #run 100 times \n",
    "\n",
    "\n",
    "#To test on Test set:\n",
    "# N = 40\n",
    "# hidden = np.arange(2,24, 1) #range(1,32)\n",
    "# lossMatrix = []\n",
    "# for i in tqdm(range(N)):\n",
    "    \n",
    "#     losses = []\n",
    "#     for value in (hidden):\n",
    "#         dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, value)\n",
    "#         dense.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "#         dense.fit(beanIntensities, beanIntensities, epochs=40,  verbose=0)\n",
    "#         test_hat = dense(validation) #, verbose = 0)\n",
    "#         loss = ignore_noParent_MSE(validation, test_hat)\n",
    "#         losses.append(loss)\n",
    "#         tf.keras.backend.clear_session()\n",
    "#     lossMatrix.append(losses)\n",
    "    \n",
    "# lossMatrix = np.array(lossMatrix)\n",
    "# #run 100 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avgMSE = np.average(lossMatrix, axis = 0)\n",
    "# plt.plot(hidden, avgMSE);\n",
    "# plt.xlabel(\"Latent Space Size\",fontsize=15);\n",
    "# plt.ylabel(\"MSE\",fontsize=15);\n",
    "# plt.title(\"Glycine Max: MSE with Respect to the Latent Space Size\", fontweight='bold', fontsize = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = pd.DataFrame(lossMatrix)\n",
    "# lm.to_csv(\"lossmatrix4lisa_dream4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.read_csv(\"lossmatrix4lisa_dream4.csv\").to_numpy()\n",
    "# print(temp.shape)\n",
    "# avgMSE = np.average(lossMatrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(hidden, avgMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, NUM_PARENTS)\n",
    "dense.compile(optimizer=tf.keras.optimizers.Adam(), loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array(dense.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def do_lazy_train(epochs, hidden = NUM_PARENTS):\n",
    "\n",
    "    '''trains the auto encoder epoch by epoch and returns the weights of the first layer'''\n",
    "    ep = epochs\n",
    "    #hidden = [hidden,] #range(1,32)\n",
    "    lossMatrix = []\n",
    "    lazy_weights = []\n",
    "\n",
    "    dense = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, hidden)\n",
    "    dense.compile(optimizer=tf.keras.optimizers.Adam(), loss=ignore_noParent_MSE)\n",
    "    \n",
    "    # dense = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, num_hidden_units=hidden)\n",
    "    # dense.compile(optimizer=tf.keras.optimizers.Adam(), loss=ignore_noParent_MSE)\n",
    "\n",
    "    #for i in tqdm(range(ep)):\n",
    "    for i in range(ep):\n",
    "        dense.fit(allData, allData, validation_data=(validation, validation), epochs=1,  verbose=0, callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()])\n",
    "        lazy_weights.append(dw := dense.get_weights()[0])\n",
    "        test = dense(validation) #, verbose = 0)\n",
    "        loss = ignore_noParent_MSE(validation, test)\n",
    "        lossMatrix.append(loss)\n",
    "        \n",
    "    lossMatrix = np.array(lossMatrix) #loss matrix no longer returns anything \n",
    "    lazy_weights = np.array(lazy_weights)\n",
    "    # print(\"lazy weights\", lazy_weights.shape)\n",
    "\n",
    "    return lossMatrix, lazy_weights #loss matrix no longer returns anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://proceedings.mlr.press/v162/rachwan22a/rachwan22a.pdf Winning the Lottery Ticket Ahead of Time:\n",
    "def lazyKernelRegime(w, parent_idx=parent_idx):\n",
    "    '''compute when each weight enters lazy kernel regime'''\n",
    "    \n",
    "    # print(\"wshape\", w.shape)\n",
    "\n",
    "    firstLayer = [] \n",
    "    for i in range(len(w)):\n",
    "         firstLayer.append(w[i]) #firstLayer.append(w[i][parent_idx])\n",
    "\n",
    "    fL = np.array(firstLayer)    \n",
    "    # print(\"print fL\", fL.shape, fL.dtype, fL[0], fL[1],)\n",
    "    d0 = np.square(fL[1] - fL[0])\n",
    "    # print(\"do\", d0.shape, d0)\n",
    "    \n",
    "    kernelChange = []\n",
    "    for i in range(1,len(fL)):\n",
    "        dt = np.square(fL[i] - fL[0])\n",
    "        dt_minus1 = np.square(fL[i-1] - fL[0])   \n",
    "        d = np.abs(dt - dt_minus1)/d0                        #eq 11 from the paper\n",
    "        kernelChange.append(d)\n",
    "    \n",
    "    kernelChange = np.moveaxis(kernelChange, 0, 2)\n",
    "    # plt.plot(kernelChange[0][0]);\n",
    "    # plt.title(\"$|\\Delta W|$ vs Epoch\")\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"$|\\Delta W|$\")\n",
    "\n",
    "    return np.array(kernelChange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: try setting top parents to num epochs trained rather than culling process\n",
    "def compute_distrib(change, t, raw = False): #raw means return the unshaped indicies. \n",
    "    stop = []\n",
    "\n",
    "    z = np.zeros(shape=(NUM_TARGETS, NUM_TARGETS))\n",
    "\n",
    "\n",
    "    for parent in range(len(change)):\n",
    "        for child in range(len(change[0])):\n",
    "            try:\n",
    "                z[parent][child] = (np.min(np.argwhere(change[parent][child] < t)))\n",
    "            except ValueError:\n",
    "                stop.append(len(change))\n",
    "\n",
    "    return z \n",
    "    \n",
    "    stop = np.array(stop)\n",
    "    # print(\"stop shape\", stop.shape)\n",
    "    # print(stop.shape)\n",
    "    var = np.std(stop)\n",
    "    # print(var)\n",
    "    # assert(False)\n",
    "    mean = np.average(stop)\n",
    "   # print(\"mean, variance\", mean, var)\n",
    "    top_parents = np.argwhere(stop > (mean + 0.5*var)) #get the parents which take more than 2 stds to stop training\n",
    "    # print(\"top parents \", top_parents)\n",
    "    # assert(False)\n",
    "    top_parent_child = []\n",
    "    for tp in top_parents:\n",
    "        top_parent_child.append(np.unravel_index(tp, shape = (NUM_PARENTS, NUM_TARGETS)))\n",
    "\n",
    "    if raw == False:\n",
    "        plt.hist(stop)\n",
    "        plt.xlabel(\"Epoch where regulator-target-weight began changing by at most \"+ str(t));\n",
    "        plt.ylabel(\"Number of parent-child-weights\");\n",
    "        plt.title(\"Histogram of weight stops\");\n",
    "        print(\"average stop: \", mean);\n",
    "\n",
    "    if raw == True:\n",
    "        return np.array(top_parents)\n",
    "\n",
    "    return np.array(top_parent_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distrib_old(change, t, raw = False): #raw means return the unshaped indicies. \n",
    "    stop = []\n",
    "\n",
    "    for parent in range(len(change)):\n",
    "        for child in range(len(change[0])):\n",
    "            try:\n",
    "                stop.append(np.min(np.argwhere(change[parent][child] < t)))\n",
    "            except ValueError:\n",
    "                stop.append(len(change))\n",
    "                # print(parent, child)\n",
    "                # plt.plot(change[parent][child])\n",
    "                # assert(False)\n",
    "\n",
    "    \n",
    "    stop = np.array(stop).flatten()\n",
    "    # print(\"stop shape\", stop.shape)\n",
    "    # print(stop.shape)\n",
    "    var = np.std(stop)\n",
    "    # print(var)\n",
    "    # assert(False)\n",
    "    mean = np.average(stop)\n",
    "   # print(\"mean, variance\", mean, var)\n",
    "    top_parents = np.argwhere(stop > (mean + 0.5*var)) #get the parents which take more than 2 stds to stop training\n",
    "    # print(\"top parents \", top_parents)\n",
    "    # assert(False)\n",
    "    top_parent_child = []\n",
    "    for tp in top_parents:\n",
    "        top_parent_child.append(np.unravel_index(tp, shape = (NUM_PARENTS, NUM_TARGETS)))\n",
    "\n",
    "    if raw == False:\n",
    "        plt.hist(stop)\n",
    "        plt.xlabel(\"Epoch where regulator-target-weight began changing by at most \"+ str(t));\n",
    "        plt.ylabel(\"Number of parent-child-weights\");\n",
    "        plt.title(\"Histogram of weight stops\");\n",
    "        print(\"average stop: \", mean);\n",
    "\n",
    "    if raw == True:\n",
    "        return np.array(top_parents)\n",
    "\n",
    "    return np.array(top_parent_child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazyKernels(N):\n",
    "\n",
    "    candidates = []\n",
    "    final_w = []\n",
    "    for i in tqdm(range(N)):\n",
    "        lm, lazy_weights = do_lazy_train(epochs=100)\n",
    "        # print(\"lm\", lm)\n",
    "        change = lazyKernelRegime(lazy_weights)\n",
    "        # print(\"change\",change.shape)\n",
    "        # assert(False)\n",
    "        top_pr = np.squeeze(compute_distrib(change, t = 0.2, raw=True))\n",
    "        candidates.append(top_pr)\n",
    "        final_w.append(lazy_weights[-1]) #final w is used for magntiude calculations at the end\n",
    "        tf.keras.backend.clear_session()\n",
    "    \n",
    "    final_w = np.array(final_w)\n",
    "    firstLayer = []\n",
    "    for i in range(len(final_w)):\n",
    "        firstLayer.append(np.abs(final_w[i])) #firstLayer.append(np.abs(final_w[i][parent_idx]))\n",
    "    fw = np.array(firstLayer)\n",
    "    candidates = np.array(candidates)\n",
    "\n",
    "    fw_avg = np.average(fw, axis = 0)\n",
    "    stop_avg = np.average(candidates, axis=0) \n",
    "\n",
    "    return stop_avg, fw_avg\n",
    "    #print(fw_avg.shape)\n",
    "    \n",
    "    candidates = np.hstack(candidates)\n",
    "    candidates = candidates.reshape(candidates.size)\n",
    "    #print(candidates.shape)\n",
    "\n",
    "    # plt.hist(candidates, bins=np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    # plt.title(\"Parent-Child Regulator Histogram\")\n",
    "    # plt.xlabel(\"Parent-Child Weight\")\n",
    "    # plt.ylabel(\"Num-Times parent-child relationship trained for top 5% of time\")\n",
    "    return candidates, fw_avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can, mag = lazyKernels(N = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = pd.read_csv(\"DREAM4_GoldStandard_InSilico_Size100_1.csv\", header=None)\n",
    "\n",
    "def keep_numeric(df):\n",
    "\n",
    "    return df.applymap(lambda x: ''.join(filter(str.isdigit, str(x))) if isinstance(x, (int,float)) else ''.join(filter(str.isdigit, x)) )\n",
    "gold = keep_numeric(gold)\n",
    "goldnp = np.array(gold, dtype = 'int')\n",
    "#subtract 1 from each index to match python index\n",
    "goldnp[:,0] = goldnp[:,0] - 1\n",
    "goldnp[:,1] = goldnp[:,1] - 1\n",
    "\n",
    "goldIm = np.zeros(shape=(NUM_TARGETS,NUM_TARGETS))\n",
    "\n",
    "for g in goldnp:\n",
    "    reg = g[0]\n",
    "    tar = g[1]\n",
    "    connection = g[2]\n",
    "    goldIm[reg][tar] = connection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.multiply(can, mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(goldIm)\n",
    "plt.title('True RT Combos');\n",
    "print(sum(sum(goldIm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscore = np.nonzero(score > score.mean() + 0.8*score.std())\n",
    "tscoreIm = np.zeros(shape=goldIm.shape) \n",
    "tscoreIm[tscore] = 1\n",
    "plt.imshow(tscoreIm);\n",
    "plt.title('Score Out');\n",
    "print(sum(sum(tscoreIm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.abs(goldIm-tscoreIm));\n",
    "plt.title(\"Difference between Gold and our score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sum(np.abs(goldIm-tscoreIm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmag = np.nonzero(mag > mag.mean() + 3.0*mag.std())\n",
    "tmagIm = np.zeros(shape=goldIm.shape) \n",
    "tmagIm[tmag] = 1\n",
    "plt.imshow(tmagIm);\n",
    "plt.title('Magnitude Out');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: do box plot of scores and compare with score of goldIM\n",
    "#TODO: Try shapley tf1 version\n",
    "plt.boxplot(score[np.nonzero(goldIm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load this with the gold IM mask after copying the weights of an old auto encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "looseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "looseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "o = looseParent.fit(beanIntensities, beanIntensities, epochs=200, verbose = True,  validation_data=(validation, validation),\n",
    "                    callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()])\n",
    "print(o.history['loss'][-1]) #the final loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseParent = modelSuperParent(superParent, regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32, sparsity=0.98)\n",
    "sparseParent.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "o = sparseParent.fit(beanIntensities, beanIntensities, epochs=200, verbose = False,  validation_data=(validation, validation),\n",
    "                    callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep()])\n",
    "print(ignore_noParent_MSE(validation, sparseParent.predict(validation)))\n",
    "\n",
    "\n",
    "plt.imshow(tfMag := sparseParent.get_weights()[0])\n",
    "tfmag = np.nonzero(tfMag > tfMag.mean() + 0.0*tfMag.std())\n",
    "tfmagIm = np.zeros(shape=goldIm.shape) \n",
    "tfmagIm[tfmag] = 1\n",
    "plt.imshow(tfmagIm);\n",
    "plt.title('TF Magnitude Mask');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning_params = {\n",
    "#     'pruning_schedule': PolynomialDecay(initial_sparsity=0.5,\n",
    "#         final_sparsity=0.9824, begin_step=1000, end_step=2000),\n",
    "#     'block_size': (2, 3),\n",
    "#     'block_pooling_type': 'MAX'\n",
    "# }\n",
    "\n",
    "# model = keras.Sequential([\n",
    "#     Dense(10, activation='relu', input_shape=(100,)),\n",
    "#     prune_low_magnitude(layers.Dense(2, activation='tanh'), **pruning_params)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldMask = 1-goldIm\n",
    "goldMask = goldMask.astype(np.float32)\n",
    "plt.imshow(goldMask);\n",
    "plt.title(\"Mask created by true R-T\");\n",
    "sum(sum(goldMask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscoreMask = -1.0*tscoreIm + 1.0 \n",
    "tscoreMask = tscoreMask.astype(np.float32)\n",
    "plt.imshow(tscoreMask);\n",
    "plt.title(\"Mask created by our score\");\n",
    "sum(sum(tscoreMask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmagMask = -1.0*tmagIm + 1.0 \n",
    "tmagMask = tmagMask.astype(np.float32)\n",
    "plt.imshow(tmagMask);\n",
    "plt.title(\"Mask created by AE magntidue\");\n",
    "print(sum(sum(tmagMask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmagAC = modelSuperParent(tmagMask, tmagMask, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "tmagAC.set_weights( looseParent.get_weights() )\n",
    "tmagACout = tmagAC(validation) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(validation, tmagACout)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreAC = modelSuperParent(tscoreMask, tscoreMask, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "scoreAC.set_weights(looseParent.get_weights())\n",
    "scoreACout = scoreAC(validation) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(validation, scoreACout)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldAC = modelSuperParent(goldMask, goldMask, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS, 32)\n",
    "goldAC.set_weights(looseParent.get_weights())\n",
    "goldout = goldAC(validation) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(validation, goldout)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ignore_noParent_MSE(validation, looseParent.predict(validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(goldIm[parent_idx].flatten(), tscoreIm[parent_idx].flatten(), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print('true negative, false positive')\n",
    "print('false negative, true positive') \n",
    "print(confusion_matrix(goldIm[parent_idx].flatten(), tscoreIm[parent_idx].flatten())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    " \n",
    "# set width of bar\n",
    "barWidth = 0.15\n",
    "#fig = plt.subplots(figsize =(12, 8))\n",
    " \n",
    "# set height of bar\n",
    "IT = [0.019060984] #base\n",
    "score = [0.023739332]\n",
    "mag = [0.021395445]\n",
    "\n",
    "ECE = [0.024257105 ]  #gold\n",
    " \n",
    " \n",
    "# Set position of bar on X axis\n",
    "br1 = np.arange(len(IT))\n",
    "br2 = [x + barWidth for x in br1]\n",
    "br3 = [x + barWidth for x in br2]\n",
    "br4 = [x + barWidth for x in br3]\n",
    "\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(br1, IT, color ='cyan', width = barWidth,\n",
    "        edgecolor ='grey', label ='Control')\n",
    "plt.bar(br2, score, color ='blue', width = barWidth,\n",
    "        edgecolor ='grey', label ='OBD (us)')\n",
    "plt.bar(br3, mag, color ='red', width = barWidth,\n",
    "        edgecolor ='grey', label ='Magnitude Pruning')\n",
    "plt.bar(br4, ECE, color ='green', width = barWidth,\n",
    "        edgecolor ='grey', label ='Gold Standard')\n",
    "# plt.bar(br3, CSE, color ='b', width = barWidth,\n",
    "#         edgecolor ='grey', label ='CSE')\n",
    " \n",
    "# Adding Xticks\n",
    "plt.xlabel('Mask', fontweight ='bold', fontsize = 15)\n",
    "plt.ylabel('MSE', fontweight ='bold', fontsize = 15)\n",
    "plt.xticks([r + barWidth for r in range(len(IT))],\n",
    "        [''])\n",
    " \n",
    "plt.title(\"MSE on the Validation Set\", fontweight='bold', fontsize = 18);\n",
    "plt.legend(loc = 'upper left');\n",
    "plt.ylim(0.018, 0.028)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(tfMag := sparseParent.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfmag = np.nonzero(np.abs(tfMag) > np.abs(tfMag)+0.01)\n",
    "tfmagIm = np.zeros(shape=goldIm.shape) \n",
    "tfmagIm[tmag] = 1\n",
    "plt.imshow(tfmagIm);\n",
    "plt.title('Mask created by AE Magnitude. TF Implementation');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trash Under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import scipy\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "X = scipy.signal.resample_poly(allData[0], up = 5, down = 1, axis = 0)\n",
    "print(X.shape)\n",
    "transformer = FactorAnalysis(n_components=41, random_state=0)\n",
    "X_transformed = transformer.fit_transform(X.T)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(allData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transformer.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  val_loss: 0.0221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldIm[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag[78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "can[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.cov(score, goldIm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.cov(goldIm, score)[100:, 100:])\n",
    "#[100:, 100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.vstack([goldIm.flatten(), score.flatten()])\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(score);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score[score > score.mean() + score.var()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf, bins = np.histogram(can, np.arange(NUM_PARENTS*NUM_TARGETS))\n",
    "# pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitudes After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PARENTS*NUM_TARGETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unravel_index(NUM_PARENTS*NUM_TARGETS, shape = (NUM_PARENTS, NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf, bins = np.histogram(can, bins=NUM_PARENTS*NUM_TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf, bins = np.histogram(can, np.arange(0, NUM_PARENTS*NUM_TARGETS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(pdf, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: CHECK ME\n",
    "def get_top_reg_targets(can, mag):\n",
    "    \n",
    "    pdf, bins = np.histogram(can, np.arange(0, NUM_PARENTS*NUM_TARGETS))\n",
    "    \n",
    "    pdf2d = np.zeros(shape = (NUM_PARENTS,NUM_TARGETS))\n",
    "\n",
    "    for i in bins:\n",
    "      idx2d = np.unravel_index(i, shape = (NUM_PARENTS, NUM_TARGETS))\n",
    "      try:\n",
    "        pdf2d[idx2d] = pdf[i]\n",
    "      except IndexError:\n",
    "        print(i, idx2d, len(pdf), len(bins), bins[-1])\n",
    "    \n",
    "\n",
    "    importance = np.multiply(pdf2d, mag)\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = get_top_reg_targets(can, mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(top)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: try factorial data, ordinary Auto encoder, treshold, tensorflow pruning, check average epoch of lazy kernel\n",
    "\n",
    "#TODO: Actually measure MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = pd.DataFrame(top)\n",
    "# lm.to_csv(\"top_r_t_HepG2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topdf = pd.read_csv('top_r_t_HepG2.csv')\n",
    "# topdf.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top = np.array(topdf)\n",
    "# plt.imshow(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top = top[:,1:]\n",
    "# plt.imshow(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(non_zero).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top[non_zero].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag[non_zero].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_connect = np.where(goldIm == 0)\n",
    "no_connect = np.array(no_connect)\n",
    "no_connect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top[no_connect].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag[no_connect].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag01 = MinMaxScaler().fit_transform(mag.reshape((-1,1)))\n",
    "tff = mag01.flatten()\n",
    "std = np.std(tff)\n",
    "\n",
    "mu = np.mean(tff)\n",
    "mu, std\n",
    "magt = np.where(top > mu + 2*std, 1, 0)\n",
    "magt.dtype = 'int'\n",
    "plt.imshow(magt.reshape(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(goldIm.flatten(), magt.flatten(), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top01 = MinMaxScaler().fit_transform(top.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(top01.reshape(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff = top01.flatten()\n",
    "std = np.std(tff)\n",
    "\n",
    "mu = np.mean(tff)\n",
    "print(mu, std)\n",
    "yout = np.where(top01.flatten() > mu + 4*std, 1, 0)\n",
    "yout.dtype = 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(yout.reshape(100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(goldIm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(goldIm.flatten(), (yout).flatten(), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "PrecisionRecallDisplay.from_predictions(goldIm.flatten(), yout.flatten());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tff = top.flatten()\n",
    "std = np.std(tff)\n",
    "\n",
    "mu = np.mean(tff)\n",
    "mu, std\n",
    "yout = np.where(top > mu + 3*std, 1, 0)\n",
    "yout.dtype = 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(goldIm.flatten(), yout.flatten(), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magX = StandardScaler().fit_transform(top.reshape((-1,1)))\n",
    "magX = MinMaxScaler().fit_transform(magX.reshape((-1,1)))\n",
    "you2 = np.where(magX > mu + 3*std, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(goldIm.flatten(), you2.flatten(), average='samples')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw old graphs; probably trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(top, cmap = 'inferno', vmin = 0, vmax = np.max(top));\n",
    "plt.colorbar(u ,fraction=0.026, pad=0.1, orientation='horizontal');\n",
    "plt.title(\"HepG2: Regulator-Target Combination Importance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topID = np.array(np.unravel_index(np.argsort(top, axis=None), top.shape))\n",
    "topID = np.flip(topID, axis=1)\n",
    "topID[0] = parent_idx[topID[0]]\n",
    "topID = topID.T\n",
    "topID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topR_T = pd.DataFrame(topID)\n",
    "topR_T.to_csv(\"DREAM4_Top_reg_target_decendingOrder_firstColIsRegulator.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.zeros(NUM_PARENTS)\n",
    "for i in range(NUM_PARENTS):\n",
    "    best[i] = np.sum(top[i])\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,NUM_PARENTS, dtype=int)\n",
    "plt.plot(x, best); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.flip(np.argsort(best))\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(can).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Petal Len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal = pd.read_excel(data_path_petal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train = petal[petal[\"Line\"] == \"WT\"]\n",
    "petal_train = petal_train.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_train.head(12)\n",
    "petal_train = petal_train.groupby(['Plate']).mean()\n",
    "petal_train.head()\n",
    "petal_train = petal_train.to_numpy()\n",
    "print(petal_train.shape)\n",
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(petal_train)\n",
    "petal_train = scaler1.transform(petal_train)\n",
    "mm = MinMaxScaler()\n",
    "mm.fit(petal_train)\n",
    "petal_train = mm.transform(petal_train)\n",
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test = petal[petal[\"Line\"] != \"WT\"]\n",
    "petal_test = petal_test.drop(columns=['Line', 'ID', \"Treatment\"])\n",
    "petal_test = petal_test.groupby(['Plate']).mean()\n",
    "petal_test.head()\n",
    "petal_test = petal_test.to_numpy()\n",
    "print(petal_test.shape)\n",
    "petal_test = scaler1.transform(petal_test)\n",
    "petal_test = mm.transform(petal_test)\n",
    "petal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1.shape\n",
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densePredictor = modelDense2(superParent, regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS, 22)\n",
    "densePredictor.compile(optimizer='adam', loss=ignore_noParent_MSE)\n",
    "densePredictor.fit(beanIntensities, beanIntensities,validation_data=(experiment1, experiment1),  epochs=100,  verbose=1)\n",
    "test = densePredictor(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgm = superParent\n",
    "time_steps = 6\n",
    "num_kinase_regulators = NUM_TARGETS\n",
    "num_hidden_units = 22\n",
    "\n",
    "inp = Input(shape=(time_steps, num_kinase_regulators))\n",
    "x = DenseEncoderLinear2(rgm, regulator_gene_matrix, NUM_TARGETS, NUM_TARGETS)(inp)\n",
    "enc = denseencoder2(x, inp, num_hidden_units)\n",
    "denseP = tf.keras.Model(inputs=inp, outputs=enc)\n",
    "#set the weights of the encoder to the weights of auto encoder\n",
    "dw = densePredictor.get_weights()\n",
    "enc_w = dw[0:5]\n",
    "denseP.set_weights(enc_w)\n",
    "#add a dense layer  because we are ouputing 1 number\n",
    "l = Dense(32, activation = 'swish', use_bias=True, kernel_regularizer='l1_l2')(denseP.layers[-1].output)\n",
    "l = Dense(1, activation = 'linear', use_bias = True)(l)\n",
    "denseP = tf.keras.Model(denseP.inputs, l)\n",
    "#denseP.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp = np.concatenate([experiment1, experiment1, experiment1, experiment1])\n",
    "bp.shape\n",
    "#bigexperiment1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = petal_train[0]\n",
    "b = petal_train[1]\n",
    "c = petal_train[2]\n",
    "d = petal_train[3]\n",
    "\n",
    "petal_train1 = np.array([a,a,a,a, b,b,b,b, c,c,c,c, d,d,d,d]) #does this make sense? we are training network to predict .5\n",
    "petal_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "denseP.fit(experiment1, petal_train, epochs=500, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP(experiment1) #experiment1 is part of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(denseP(testCandidate)) #model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_test #true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denseP.evaluate(testCandidate, petal_test) #eval gave 1.3999 before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d_1 (Glo  (None, 16)               0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 6, 6, 16)         160       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 8, 8, 32)         4640      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_6 (Conv2DT  (None, 26, 26, 16)       4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_7 (Conv2DT  (None, 28, 28, 1)        145       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import json\n",
    "import shap\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "# encoder.summary()\n",
    "\n",
    "x = layers.Reshape((4, 4, 1))(encoder_output)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Fin Amin\\AppData\\Local\\Temp\\ipykernel_16428\\1144032549.py:13: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (50, 224, 224, 3) for Tensor img_2:0, which has shape (None, 28, 28, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\dream4_100\\DreamEncoder4_100.ipynb Cell 207\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     feed_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m([model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minput], [preprocess_input(x\u001b[39m.\u001b[39mcopy())]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m K\u001b[39m.\u001b[39mget_session()\u001b[39m.\u001b[39mrun(model\u001b[39m.\u001b[39mlayers[layer]\u001b[39m.\u001b[39minput, feed_dict)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m e \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mGradientExplainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     (model\u001b[39m.\u001b[39mlayers[\u001b[39m7\u001b[39m]\u001b[39m.\u001b[39minput, model\u001b[39m.\u001b[39mlayers[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput),\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     map2layer(X, \u001b[39m7\u001b[39;49m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     local_smoothing\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m \u001b[39m# std dev of smoothing noise\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m shap_values,indexes \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mshap_values(map2layer(to_explain, \u001b[39m7\u001b[39m), ranked_outputs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# get the names for the classes\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Fin Amin\\Desktop\\StemCellResearch\\dream4_100\\DreamEncoder4_100.ipynb Cell 207\u001b[0m in \u001b[0;36mmap2layer\u001b[1;34m(x, layer)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap2layer\u001b[39m(x, layer):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     feed_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m([model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39minput], [preprocess_input(x\u001b[39m.\u001b[39mcopy())]))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Fin%20Amin/Desktop/StemCellResearch/dream4_100/DreamEncoder4_100.ipynb#Y412sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m K\u001b[39m.\u001b[39;49mget_session()\u001b[39m.\u001b[39;49mrun(model\u001b[39m.\u001b[39;49mlayers[layer]\u001b[39m.\u001b[39;49minput, feed_dict)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:967\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    964\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 967\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    968\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    970\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py:1164\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1160\u001b[0m   np_val \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(subfeed_val, dtype\u001b[39m=\u001b[39msubfeed_dtype)\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m is_tensor_handle_feed \u001b[39mand\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m     \u001b[39mnot\u001b[39;00m subfeed_t\u001b[39m.\u001b[39mget_shape()\u001b[39m.\u001b[39mis_compatible_with(np_val\u001b[39m.\u001b[39mshape)):\n\u001b[1;32m-> 1164\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1165\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCannot feed value of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(np_val\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m for Tensor \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1166\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msubfeed_t\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m, which has shape \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1167\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(subfeed_t\u001b[39m.\u001b[39mget_shape())\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mis_feedable(subfeed_t):\n\u001b[0;32m   1169\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTensor \u001b[39m\u001b[39m{\u001b[39;00msubfeed_t\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m may not be fed.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (50, 224, 224, 3) for Tensor img_2:0, which has shape (None, 28, 28, 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "X,y = shap.datasets.imagenet50()\n",
    "to_explain = X[[39,41]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test set and the synthetic dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTestSet(test_path):\n",
    "    testFiles = []\n",
    "    for np_name in glob(os.path.join(data_path_testSet,'*.np[yz]')):\n",
    "        k = np.load(os.path.join(data_path_testSet,np_name))\n",
    "        testFiles.append(k)\n",
    "#         print(np_name)\n",
    "#         print(k.shape)\n",
    "    return np.array(testFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PKjzdFCMwFg"
   },
   "outputs": [],
   "source": [
    "def read_files(data_path):\n",
    "\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        try:\n",
    "            #print('file name:',file)\n",
    "            #print('value of i:',i)\n",
    "            genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "            #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "            replicates.append(np.array(genes_intensities_data_matrix.values, dtype = float))\n",
    "            # i+=1\n",
    "        except PermissionError:\n",
    "            print(\"Not a CSV: \", os.path.join(data_path , file))\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    rep = np.array(replicates).astype(np.float32)\n",
    "    \n",
    "    return rep, rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCwo4LwlO_FF"
   },
   "outputs": [],
   "source": [
    "beanIntensities, regulator_gene_matrix= read_files(data_path_syn)\n",
    "matrix = regulator_gene_matrix\n",
    "replicates = beanIntensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.zeros(shape = (3,6,8))\n",
    "id = np.unravel_index(3*6*8 - 1, shape = d.shape)\n",
    "d[id] = 1\n",
    "plt.imshow(d[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0][ : , parentIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)\n",
    "print(testCandidate.shape)\n",
    "syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(change[0][22])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"change in weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossMatrix)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(change.shape, lossMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.diff(change[0][22])\n",
    "plt.plot(d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change[0][0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def compute_tresh(change, stop = 0.05):\n",
    "#     diffs = []\n",
    "#     for parent in range(len(change)):\n",
    "#         for child in range(len(change[0])):\n",
    "#             diffs.append(np.diff(change[parent][child]))\n",
    "#     inflection = []\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         for d in diffs:\n",
    "#             print(np.argwhere(np.abs(d) < stop))\n",
    "#             inflection.append(np.min(np.argwhere(np.abs(d) < stop))) #return where the second derivative is first 0. \n",
    "\n",
    "#     except ValueError:\n",
    "#         print(\"Stop value \", stop, \" is too high, trying stop = \", stop + 0.05)\n",
    "#         # s = stop + 0.05\n",
    "#         # return compute_tresh(change, stop = s)\n",
    "        \n",
    "\n",
    "#     return np.average(inflection)\n",
    "        \n",
    "# d = compute_tresh(change)\n",
    "# d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"mse3.npy\", avgMSE) #mse2/3 is with -1 fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.sciencedirect.com/science/article/pii/S0925231220314570?casa_token=lcEJANqO0JwAAAAA:uL3DGUZctPUZz_sPz1K1i2klMtb83TyKnc9CI3_N-uSOaM7VHL8GhM0jCGYfo25NmpDQQ9Cvlw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rshp = Flatten()(looseParent.layers[-1].output)\n",
    "\n",
    "modelTemp = tf.keras.Model(inputs=looseParent.input, outputs = [rshp])\n",
    "modelTemp.summary()\n",
    "type(modelTemp)\n",
    "explainer = shap.DeepExplainer(modelTemp, syntheticDataTrain)\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough #this solves the \"shap_ADDV2\" problem but another one will appear\n",
    "#shap.explainers._deep.deep_tf.op_handlers[\"FusedBatchNormV3\"] = shap.explainers._deep.deep_tf.passthrough #this solves the next problem which allows you to run the DeepExplainer.\n",
    "\n",
    "shap_values = explainer.shap_values(testCandidate[0:1])\n",
    "def f(x):\n",
    "    return modelTemp.predict(x)\n",
    "\n",
    "print(f(testCandidate))\n",
    "explainer = shap.KernelExplainer(f , testCandidate[0:1], link=\"logit\") #svm.predict_proba, X_train, link=\"logit\")\n",
    "shap_values = explainer.shap_values(testCandidate[0:1], nsamples=100)\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import json\n",
    "import shap\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "X,y = shap.datasets.imagenet50()\n",
    "to_explain = X[[39,41]]\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "e = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(X, 7),\n",
    "    local_smoothing=0 # std dev of smoothing noise\n",
    ")\n",
    "shap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(enc_dec_Synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [5,6]\n",
    "u = tf.concat([a,b], axis = 0)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newConnections = superParent - regulator_gene_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(newConnections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nC = []\n",
    "# for i in range(len(newConnections[0])):\n",
    "#     for j in range(len(newConnections[1])):\n",
    "#         if newConnections[i][j] > 0:\n",
    "#             nC.append([i,j])\n",
    "# nC = np.array(nC)\n",
    "# nC = pd.DataFrame(nC)\n",
    "# nC.to_csv(\"new_connections_in_superParents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code for testing loss function\n",
    "# print(outSyn.shape)\n",
    "# print(testCandidate.shape)\n",
    "# syntheticLoss = ignore_noParent_MSE(np.array([testCandidate[0]]), np.array([outSyn[0]]) )\n",
    "# syntheticLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset Auto Encoder\n",
    "Autoencoder has not been trained on synthetic version of experiement 1. We test on the original experiment 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrRuJ_bsrHll"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAOZEEvRRVU4"
   },
   "outputs": [],
   "source": [
    "# enc_dec_Synthetic.compile(optimizer='adam',loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticDataTrain = beanIntensities[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1649273035573,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "EuNDZx5Ov34I",
    "outputId": "74597aa1-c71c-4491-b8c9-4fefed309325"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.fit(syntheticDataTrain,syntheticDataTrain,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = enc_dec_Synthetic(testCandidate) #, verbose = 0)\n",
    "loss = ignore_noParent_MSE(testCandidate, test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = enc_dec_Synthetic.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w[0], cmap = \"hot\", vmin=0,vmax=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBzDmjqJViEw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we do not need to use this function for the testset\n",
    "def getCSVs(data_path_head):\n",
    "    PATH = data_path_head\n",
    "    EXT = \"*.csv\"\n",
    "    all_csv_files = [file\n",
    "                     for path, subdir, files in os.walk(PATH)\n",
    "                     for file in glob(os.path.join(path, EXT))]\n",
    "    actual = []\n",
    "    for p in all_csv_files:\n",
    "        actual.append(pd.read_csv(p, index_col = 0).to_numpy())\n",
    "    return np.array(actual)\n",
    "    \n",
    "experiment1 = getCSVs(data_path_og_exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate = test.numpy().astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCandidate[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([beanIntensities[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outSyn = enc_dec_Synthetic.predict(testCandidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymagn(A, B):\n",
    "    mse = (np.square(A - B)).mean(axis=None)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outSyn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntheticLoss = ignore_noParent_MSE(testCandidate, outSyn )\n",
    "syntheticLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(outSyn-testCandidate).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install keras-visualizer\n",
    "#!pip install pydot\n",
    "#data_path_og_exp1 = data_path_testSet \n",
    "# !pip install pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = loadTestSet(data_path_testSet)\n",
    "# testCandidate = test.astype(np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPRV0OAMpKPH"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic = model(regulator_gene_matrix, NUM_TARGETS, NUM_TIME_STEPS, NUM_TARGETS) #we can just change the time steps to something higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1649273028683,
     "user": {
      "displayName": "Sahil Anish Palarpwar",
      "userId": "17757512684560375750"
     },
     "user_tz": 240
    },
    "id": "D6aPT5_cpKK0",
    "outputId": "c261d824-0f38-4eee-b139-f03a80f093e1"
   },
   "outputs": [],
   "source": [
    "enc_dec_Synthetic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolated dataset Auto Encoder\n",
    "Once again, we do not train on any version of exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filesV2(data_path):\n",
    "    '''\n",
    "    *Changed*\n",
    "    currently hardcoded for only one file. \n",
    "    change code a bit for reading multiple files.\n",
    "    '''\n",
    "    #genes_intensities_data_matrix = pd.read_csv(file_path_intensities, index_col = 0)\n",
    "    #print(os.listdir(data_path))\n",
    "    replicate_files = os.listdir(data_path)\n",
    "    #print('replicate files:',replicate_files)\n",
    "    replicates = []\n",
    "    # i = 0\n",
    "    for file in replicate_files:\n",
    "        \n",
    "        #print('file name:',file)\n",
    "        #print('value of i:',i)\n",
    "        genes_intensities_data_matrix = pd.read_csv(os.path.join(data_path , file), index_col = 0, on_bad_lines='skip')\n",
    "        #print('genes_intensities_data_matrix:',  genes_intensities_data_matrix.head())\n",
    "        replicates.append(genes_intensities_data_matrix.values)\n",
    "        # i+=1\n",
    "        \n",
    "    genes_intensities_data_matrix = genes_intensities_data_matrix.values\n",
    "    rgm = np.loadtxt(matrix_path)\n",
    "    \n",
    "    return np.asarray(replicates), rgm.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes, _ = read_filesV2(data_path_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolated_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(interpolated_genes[2]).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = []\n",
    "for k in range(len(interpolated_genes)):\n",
    "    #print(k)\n",
    "    if k == 2 or k == 3 or k == 4:\n",
    "        inter.append(np.reshape(interpolated_genes[k], (4,6,NUM_TARGETS)))\n",
    "    else: \n",
    "        inter.append(np.reshape(interpolated_genes[k], (5,6,NUM_TARGETS)))\n",
    "inter = np.vstack(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beanIntensities[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter = model(regulator_gene_matrix, NUM_TARGETS, 6, NUM_TARGETS) \n",
    "enc_dec_inter.compile(optimizer='adam', loss=ignore_noParent_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_inter.fit(inter, inter,epochs=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter = enc_dec_inter.predict(testCandidate)\n",
    "interpolationLoss = ignore_noParent_MSE(testCandidate, outInter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolationLoss #used to be 3.84 on broke ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outInter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = enc_dec_inter.history\n",
    "u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons between various outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = plt.imshow(np.reshape((np.abs(outSyn)), (24,NUM_TARGETS)), cmap = \"hot\", vmin=0,vmax=1.0 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(outSyn[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = pd.DataFrame(testCandidate[0])\n",
    "u.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape((np.abs(outSyn-outInter)), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 );\n",
    "plt.title(\"Difference Between the Outputs of Both Autoencoders\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"DiffBetweenOut.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outInter-experiment1), (24,NUM_TARGETS)) , cmap = \"hot\") #, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Interpolated Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30)\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"InterDiffImage.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50, 50))\n",
    "u = plt.imshow(np.reshape(np.abs(outSyn-experiment1), (24,NUM_TARGETS)), cmap = \"hot\")#, vmin=0,vmax=1.0 )\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data\", fontsize = 40);\n",
    "plt.xlabel(\"Phosphopeptide\", fontsize = 30);\n",
    "plt.ylabel(\"Times Concatenated\", fontsize = 30);\n",
    "plt.colorbar(u ,fraction=0.0046, pad=0.02);\n",
    "#plt.savefig(\"SynDiffImage.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_idx = parentIndex.numpy()\n",
    "#print(parent_idx)\n",
    "oSyn = (np.reshape((outSyn), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "oSyn = oSyn.T\n",
    "oSyn.shape\n",
    "\n",
    "exp1_col = (np.reshape((experiment1), (24,NUM_TARGETS)).T)[parent_idx]\n",
    "exp1_col = exp1_col.T\n",
    "print(exp1_col.shape)\n",
    "\n",
    "u = plt.imshow(np.abs(oSyn - exp1_col), cmap = 'hot') #TODO use TF loss function instead of difference.\n",
    "ddff = oSyn-exp1_col\n",
    "plt.colorbar(u)\n",
    "plt.title(\"Difference Between the Input and Output of the Autoencoder Trained on Synthetic Data. Only parents.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(oSyn).head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(exp1_col).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ddff).head(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"interpolated_v2.npy\", inter) #the interpolated dataset\n",
    "# np.save(\"synthetic_v2.npy\", beanIntensities[1:]) # the synthetic dataset\n",
    "# np.save(\"synOut_v2.npy\", outSyn) #the output of the encoder trained on synthetic data with the input being exp1\n",
    "# np.save(\"interOut_v2.npy\", outInter) #the output of the encoder trained on interpolated data with the input being exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6J5QSynTTgZ+lFOhgOtAG",
   "collapsed_sections": [],
   "name": "cnn-third.ipynb",
   "provenance": [
    {
     "file_id": "1mmRxO5jnBc5CdzxXj8sMtPpG0BQ0ulSs",
     "timestamp": 1648921397876
    },
    {
     "file_id": "10758zFj2UnTnwpQaSFIr5r9MqZWdiMsf",
     "timestamp": 1648674775255
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
